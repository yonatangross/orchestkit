// AUTO-GENERATED by scripts/generate-playground-data.js
// DO NOT EDIT MANUALLY ‚Äî your changes will be overwritten.
// Only import this module when skill content is actually needed (on-demand).

export const SKILL_CONTENT: Record<string, { content: string; contentTruncated: boolean }> = {
  "accessibility": {
    "content": "# Accessibility\n\nComprehensive patterns for building accessible web applications: WCAG 2.2 AA compliance, keyboard focus management, and React Aria component patterns. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [WCAG Compliance](#wcag-compliance) | 3 | CRITICAL | Color contrast, semantic HTML, automated testing |\n| [Focus Management](#focus-management) | 3 | HIGH | Focus traps, focus restoration, keyboard navigation |\n| [React Aria](#react-aria) | 3 | HIGH | Accessible components, form hooks, overlay patterns |\n\n**Total: 9 rules across 3 categories**\n\n## Quick Start\n\n```tsx\n// Semantic HTML with ARIA\n<main>\n  <article>\n    <header><h1>Page Title</h1></header>\n    <section aria-labelledby=\"features-heading\">\n      <h2 id=\"features-heading\">Features</h2>\n    </section>\n  </article>\n</main>\n```\n\n```tsx\n// Focus trap with React Aria\nimport { FocusScope } from 'react-aria';\n\n<FocusScope contain restoreFocus autoFocus>\n  <div role=\"dialog\" aria-modal=\"true\">\n    {children}\n  </div>\n</FocusScope>\n```\n\n## WCAG Compliance\n\nWCAG 2.2 AA implementation for inclusive, legally compliant web applications.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Color Contrast | `rules/wcag-color-contrast.md` | 4.5:1 text, 3:1 UI components, focus indicators |\n| Semantic HTML | `rules/wcag-semantic-html.md` | Landmarks, headings, ARIA labels, form structure |\n| Testing | `rules/wcag-testing.md` | axe-core, Playwright a11y, screen reader testing |\n\n## Focus Management\n\nKeyboard focus management patterns for accessible interactive widgets.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Focus Trap | `rules/focus-trap.md` | Modal focus trapping, FocusScope, Escape key |\n| Focus Restoration | `rules/focus-restoration.md` | Return focus to trigger, focus first error |\n| Keyboard Navigation | `rules/focus-keyboard-nav.md` | Roving tabindex, skip links, arrow keys |\n\n## React Aria\n\nAdobe React Aria hooks for building WCAG-compliant interactive UI.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Components | `rules/aria-components.md` | useButton, useDialog, useMenu, FocusScope |\n| Forms | `rules/aria-forms.md` | useComboBox, useTextField, useListBox |\n| Overlays | `rules/aria-overlays.md` | useModalOverlay, useTooltip, usePopover |\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Conformance level | WCAG 2.2 AA (legal standard: ADA, Section 508) |\n| Contrast ratio | 4.5:1 normal text, 3:1 large text and UI components |\n| Target size | 24px min (WCAG 2.5.8), 44px for touch |\n| Focus indicator | 3px solid outline, 3:1 contrast |\n| Component library | React Aria hooks for control, react-aria-components for speed |\n| State management | react-stately hooks (designed for a11y) |\n| Focus management | FocusScope for modals, roving tabindex for widgets |\n| Testing | jest-axe ",
    "contentTruncated": true
  },
  "add-golden": {
    "content": "# Add to Golden Dataset\n\nMulti-agent curation workflow with quality score explanations, bias detection, and version tracking.\n\n## Quick Start\n\n```bash\n/add-golden https://example.com/article\n/add-golden https://arxiv.org/abs/2312.xxxxx\n```\n\n---\n\n## Task Management (CC 2.1.16)\n\n```python\n# Create main curation task\nTaskCreate(\n  subject=\"Add to golden dataset: {url}\",\n  description=\"Multi-agent curation with quality explanation\",\n  activeForm=\"Curating document\"\n)\n\n# Create subtasks for 9-phase process\nphases = [\"Fetch content\", \"Run quality analysis\", \"Explain scores\",\n          \"Check bias\", \"Check diversity\", \"Validate\", \"Get approval\",\n          \"Write to dataset\", \"Update version\"]\nfor phase in phases:\n    TaskCreate(subject=phase, activeForm=f\"{phase}ing\")\n```\n\n---\n\n## Workflow Overview\n\n| Phase | Activities | Output |\n|-------|------------|--------|\n| **1. Input Collection** | Get URL, detect content type | Document metadata |\n| **2. Fetch and Extract** | Parse document structure | Structured content |\n| **3. Quality Analysis** | 4 parallel agents evaluate | Raw scores |\n| **4. Quality Explanation** | Explain WHY each score | Score rationale |\n| **5. Bias Detection** | Check for bias in content | Bias report |\n| **6. Diversity Check** | Assess dataset balance | Diversity metrics |\n| **7. Validation** | Schema, duplicates, gates | Validation status |\n| **8. Silver-to-Gold** | Promote or mark as silver | Classification |\n| **9. Version Tracking** | Track changes, rollback | Version entry |\n\n---\n\n## Phase 1-2: Input and Extraction\n\nDetect content type: article, tutorial, documentation, research_paper.\n\nExtract: title, sections, code blocks, key terms, metadata (author, date).\n\n---\n\n## Phase 3: Parallel Quality Analysis (4 Agents)\n\nLaunch ALL agents in ONE message with `run_in_background=True` and `max_turns=25`.\n\n| Agent | Focus | Output |\n|-------|-------|--------|\n| code-quality-reviewer | Accuracy, coherence, depth, relevance | Quality scores |\n| workflow-architect | Keyword directness, paraphrase, reasoning | Difficulty level |\n| data-pipeline-engineer | Primary/secondary domains, skill level | Tags |\n| test-generator | Direct, paraphrased, multi-hop queries | Test queries |\n\nSee [Quality Scoring](references/quality-scoring.md) for detailed criteria.\n\n---\n\n## Phase 4: Quality Explanation\n\nEach dimension gets WHY explanation:\n\n```markdown\n### Accuracy: [N.NN]/1.0\n**Why this score:**\n- [Specific reason with evidence]\n**What would improve it:**\n- [Specific improvement]\n```\n\n---\n\n## Phase 5: Bias Detection\n\nSee [Bias Detection Guide](references/bias-detection-guide.md) for patterns.\n\nCheck for:\n- Technology bias (favors specific tools)\n- Recency bias (ignores LTS versions)\n- Complexity bias (assumed knowledge)\n- Vendor bias (promotes products)\n- Geographic/cultural bias\n\n| Bias Score | Action |\n|------------|--------|\n| 0-2 | Proceed normally |\n| 3-5 | Add disclaimer |\n| 6-8 | Require user review |\n| 9-10 | Recommend against |\n\n---\n\n## Phase 6",
    "contentTruncated": true
  },
  "advanced-guardrails": {
    "content": "# Advanced Guardrails\n\nProduction LLM safety using NeMo Guardrails, Guardrails AI, and OpenAI moderation with red-teaming validation.\n\n> **NeMo Guardrails **: LangChain 1.x compatible, parallel rails execution, OpenTelemetry tracing. **DeepTeam**: 40+ vulnerabilities, OWASP Top 10 alignment.\n\n## Overview\n\n- Implementing input/output validation for LLM applications\n- Preventing hallucinations and enforcing factuality\n- Detecting and filtering toxic, harmful, or off-topic content\n- Restricting LLM responses to specific domains/topics\n- PII detection and redaction in LLM outputs\n- Red-teaming and adversarial testing of LLM systems\n- OWASP Top 10 for LLMs compliance\n\n## Framework Comparison\n\n| Framework | Best For | Key Features |\n|-----------|----------|--------------|\n| **NeMo Guardrails** | Programmable flows, Colang 2.0 | Input/output rails, fact-checking, dialog control |\n| **Guardrails AI** | Validator-based, modular | 100+ validators, PII, toxicity, structured output |\n| **OpenAI Guardrails** | Drop-in wrapper | Simple integration, moderation API |\n| **DeepTeam** | Red teaming, adversarial | GOAT attacks, multi-turn jailbreaking, vulnerability scanning |\n\n## Quick Reference\n\n### NeMo Guardrails with Guardrails AI Integration\n\n```yaml\n# config.yml\nmodels:\n  - type: main\n    engine: openai\n    model: gpt-5.2\n\nrails:\n  config:\n    guardrails_ai:\n      validators:\n        - name: toxic_language\n          parameters:\n            threshold: 0.5\n            validation_method: \"sentence\"\n        - name: guardrails_pii\n          parameters:\n            entities: [\"phone_number\", \"email\", \"ssn\", \"credit_card\"]\n        - name: restricttotopic\n          parameters:\n            valid_topics: [\"technology\", \"support\"]\n        - name: valid_length\n          parameters:\n            min: 10\n            max: 500\n\n  input:\n    flows:\n      - guardrailsai check input $validator=\"guardrails_pii\"\n      - guardrailsai check input $validator=\"competitor_check\"\n\n  output:\n    flows:\n      - guardrailsai check output $validator=\"toxic_language\"\n      - guardrailsai check output $validator=\"restricttotopic\"\n      - guardrailsai check output $validator=\"valid_length\"\n```\n\n### Colang 2.0 Fact-Checking Rails\n\n```colang\ndefine flow answer question with facts\n  \"\"\"Enable fact-checking for RAG responses.\"\"\"\n  user ...\n  $answer = execute rag()\n  $check_facts = True  # Enables fact-checking rail\n  bot $answer\n\ndefine flow check hallucination\n  \"\"\"Block responses about people without verification.\"\"\"\n  user ask about people\n  $check_hallucination = True  # Blocking mode\n  bot respond about people\n\ndefine flow restrict competitor mentions\n  \"\"\"Prevent discussing competitor products.\"\"\"\n  user ask about $competitor\n  if $competitor in [\"CompetitorA\", \"CompetitorB\"]\n    bot \"I can only discuss our products.\"\n  else\n    bot respond normally\n```\n\n### Guardrails AI Validators\n\n```python\nfrom guardrails import Guard\nfrom guardrails.hub import (\n    ToxicLanguage,\n    DetectPII,\n    Re",
    "contentTruncated": true
  },
  "agent-orchestration": {
    "content": "# Agent Orchestration\n\nComprehensive patterns for building and coordinating AI agents -- from single-agent reasoning loops to multi-agent systems and framework selection. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Agent Loops](#agent-loops) | 2 | HIGH | ReAct reasoning, plan-and-execute, self-correction |\n| [Multi-Agent Coordination](#multi-agent-coordination) | 3 | CRITICAL | Supervisor routing, agent debate, result synthesis |\n| [Alternative Frameworks](#alternative-frameworks) | 3 | HIGH | CrewAI crews, AutoGen teams, framework comparison |\n| [Multi-Scenario](#multi-scenario) | 2 | MEDIUM | Parallel scenario orchestration, difficulty routing |\n\n**Total: 10 rules across 4 categories**\n\n## Quick Start\n\n```python\n# ReAct agent loop\nasync def react_loop(question: str, tools: dict, max_steps: int = 10) -> str:\n    history = REACT_PROMPT.format(tools=list(tools.keys()), question=question)\n    for step in range(max_steps):\n        response = await llm.chat([{\"role\": \"user\", \"content\": history}])\n        if \"Final Answer:\" in response.content:\n            return response.content.split(\"Final Answer:\")[-1].strip()\n        if \"Action:\" in response.content:\n            action = parse_action(response.content)\n            result = await tools[action.name](*action.args)\n            history += f\"\\nObservation: {result}\\n\"\n    return \"Max steps reached without answer\"\n```\n\n```python\n# Supervisor with fan-out/fan-in\nasync def multi_agent_analysis(content: str) -> dict:\n    agents = [(\"security\", security_agent), (\"perf\", perf_agent)]\n    tasks = [agent(content) for _, agent in agents]\n    results = await asyncio.gather(*tasks, return_exceptions=True)\n    return await synthesize_findings(results)\n```\n\n## Agent Loops\n\nPatterns for autonomous LLM reasoning: ReAct (Reasoning + Acting), Plan-and-Execute with replanning, self-correction loops, and sliding-window memory management.\n\n**Key decisions:** Max steps 5-15, temperature 0.3-0.7, memory window 10-20 messages.\n\n## Multi-Agent Coordination\n\nFan-out/fan-in parallelism, supervisor routing with dependency ordering, conflict resolution (confidence-based or LLM arbitration), result synthesis, and CC Agent Teams (mesh topology for peer messaging in CC 2.1.33+).\n\n**Key decisions:** 3-8 specialists, parallelize independent agents, use Task tool (star) for simple work, Agent Teams (mesh) for cross-cutting concerns.\n\n## Alternative Frameworks\n\nCrewAI hierarchical crews with Flows (1.8+), OpenAI Agents SDK handoffs and guardrails (0.7.0), Microsoft Agent Framework (AutoGen + SK merger), GPT-5.2-Codex for long-horizon coding, and AG2 for open-source flexibility.\n\n**Key decisions:** Match framework to team expertise + use case. LangGraph for state machines, CrewAI for role-based teams, OpenAI SDK for handoff workflows, MS Agent for enterprise compliance.\n\n## Multi-Scenario\n\nOrchestrate a",
    "contentTruncated": true
  },
  "aggregate-patterns": {
    "content": "# Aggregate Design Patterns\n\nDesign aggregates with clear boundaries, invariants, and consistency guarantees.\n\n## Overview\n\n- Defining transactional consistency boundaries\n- Enforcing business invariants across related entities\n- Designing aggregate roots and their children\n- Handling references between aggregates\n- Optimizing aggregate size for performance\n\n## Core Concepts\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                 ORDER AGGREGATE                         ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ\n‚îÇ  ‚îÇ         Order (Aggregate Root)                   ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ id: UUID (UUIDv7)                            ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ customer_id: UUID (reference by ID!)         ‚îÇ   ‚îÇ\n‚îÇ  ‚îÇ  ‚Ä¢ status: OrderStatus                          ‚îÇ   ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ\n‚îÇ           ‚îÇ                      ‚îÇ                      ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ\n‚îÇ  ‚îÇ  OrderItem     ‚îÇ    ‚îÇ  OrderItem     ‚îÇ              ‚îÇ\n‚îÇ  ‚îÇ  (child)       ‚îÇ    ‚îÇ  (child)       ‚îÇ              ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ\n‚îÇ                                                         ‚îÇ\n‚îÇ  INVARIANTS enforced by root:                          ‚îÇ\n‚îÇ  ‚Ä¢ Total = sum of items                                ‚îÇ\n‚îÇ  ‚Ä¢ Max 100 items per order                             ‚îÇ\n‚îÇ  ‚Ä¢ Cannot modify after shipped                         ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Four Rules\n\n1. **Root controls access** - External code only references aggregate root\n2. **Transactional boundary** - One aggregate per transaction\n3. **Reference by ID** - Never hold references to other aggregates\n4. **Invariants enforced** - Root ensures all business rules\n\n## Quick Reference\n\n```python\nfrom dataclasses import dataclass, field\nfrom uuid import UUID\nfrom uuid_utils import uuid7\n\n@dataclass\nclass OrderAggregate:\n    \"\"\"Aggregate root with invariant enforcement.\"\"\"\n\n    id: UUID = field(default_factory=uuid7)\n    customer_id: UUID  # Reference by ID, not Customer object!\n    _items: list[\"OrderItem\"] = field(default_factory=list)\n    status: str = \"draft\"\n\n    MAX_ITEMS = 100\n\n    def add_item(self, product_id: UUID, quantity: int, price: Money) -> None:\n        \"\"\"Add item with invariant checks.\"\"\"\n        self._ensure_modifiable()\n        if len(self._items) >= self.MAX_ITEMS:\n            raise DomainError(\"Max items exceeded\")\n        self._items.append(OrderItem(product_id, quantity, price))\n\n    def _ensure_modifiable(self) -> None:\n        if self.status != \"draft\":\n            raise DomainError(f\"Cannot modify {self.status} order\")\n```\n\nSee [aggregate-root-template.py](scripts/aggregate-root-template.py) for complete implementation.\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Aggregate size | Small (< 20 children), split if larger |\n| Cross-aggregate refs | Always by ID, never by object |\n| Consis",
    "contentTruncated": true
  },
  "analytics": {
    "content": "# Cross-Project Analytics\n\nQuery local analytics data from `~/.claude/analytics/`. All data is local-only, privacy-safe (hashed project IDs, no PII).\n\n## Data Files\n\n| File | Contents | Key Fields |\n|------|----------|-----------|\n| `agent-usage.jsonl` | Agent spawn events | `ts, pid, agent, duration_ms, success, output_len, team?` |\n| `skill-usage.jsonl` | Skill invocations | `ts, pid, skill, team?` |\n| `hook-timing.jsonl` | Hook execution timing | `ts, hook, duration_ms, ok, pid, team?` |\n| `session-summary.jsonl` | Session end summaries | `ts, pid, total_tools, team?` |\n| `task-usage.jsonl` | Task completions | `ts, pid, task_status, duration_ms, team?` |\n| `team-activity.jsonl` | Team spawns and idle | `ts, pid, event, agent, member?, idle_ms?, model?, team` |\n\n## Subcommands\n\nParse the user's argument to determine which report to show. If no argument provided, use AskUserQuestion to let them pick.\n\n### `agents` ‚Äî Top agents by frequency and average duration\n\n```bash\njq -s 'group_by(.agent) | map({agent: .[0].agent, count: length, avg_ms: (map(.duration_ms // 0) | add / length | floor), success_rate: (map(select(.success)) | length) / length * 100 | floor}) | sort_by(-.count)' ~/.claude/analytics/agent-usage.jsonl\n```\n\n### `skills` ‚Äî Top skills by invocation count\n\n```bash\njq -s 'group_by(.skill) | map({skill: .[0].skill, count: length}) | sort_by(-.count)' ~/.claude/analytics/skill-usage.jsonl\n```\n\n### `hooks` ‚Äî Slowest hooks and failure rates\n\n```bash\njq -s 'group_by(.hook) | map({hook: .[0].hook, count: length, avg_ms: (map(.duration_ms) | add / length | floor), fail_rate: (map(select(.ok == false)) | length) / length * 100 | floor}) | sort_by(-.avg_ms) | .[0:15]' ~/.claude/analytics/hook-timing.jsonl\n```\n\n### `teams` ‚Äî Team spawn counts, idle time, task completions\n\n```bash\n# Team activity (spawns + idle)\njq -s 'group_by(.team) | map({team: .[0].team, spawns: [.[] | select(.event == \"spawn\")] | length, idles: [.[] | select(.event == \"idle\")] | length, agents: [.[].agent] | unique}) | sort_by(-.spawns)' ~/.claude/analytics/team-activity.jsonl\n\n# Task completions by team\njq -s '[.[] | select(.team != null)] | group_by(.team) | map({team: .[0].team, tasks: length, avg_ms: (map(.duration_ms // 0) | add / length | floor)})' ~/.claude/analytics/task-usage.jsonl\n```\n\n### `summary` ‚Äî Overall cross-project summary\n\nRun all of the above and present a unified view:\n- Total sessions, total tool invocations\n- Top 5 agents, top 5 skills\n- Team activity overview (if team data exists)\n- Unique project hashes count\n\n```bash\n# Quick counts\nwc -l ~/.claude/analytics/*.jsonl 2>/dev/null\n# Unique projects\njq -r .pid ~/.claude/analytics/agent-usage.jsonl 2>/dev/null | sort -u | wc -l\n```\n\n## Important Notes\n\n- All files are in JSONL (newline-delimited JSON) format\n- For large files (>50MB), use streaming `jq` without `-s` (slurp) flag\n- Rotated files follow pattern `<name>.<YYYY-MM>.jsonl` ‚Äî include them in queries if historical data is needed\n- The `team` fie",
    "contentTruncated": true
  },
  "api-design": {
    "content": "# API Design\n\nComprehensive API design patterns covering REST/GraphQL framework design, versioning strategies, and RFC 9457 error handling. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [API Framework](#api-framework) | 3 | HIGH | REST conventions, resource modeling, OpenAPI specifications |\n| [Versioning](#versioning) | 3 | HIGH | URL path versioning, header versioning, deprecation/sunset policies |\n| [Error Handling](#error-handling) | 3 | HIGH | RFC 9457 Problem Details, validation errors, error type registries |\n\n**Total: 9 rules across 3 categories**\n\n## API Framework\n\nREST and GraphQL API design conventions for consistent, developer-friendly APIs.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| REST Conventions | `rules/framework-rest-conventions.md` | Plural nouns, HTTP methods, status codes, pagination |\n| Resource Modeling | `rules/framework-resource-modeling.md` | Hierarchical URLs, filtering, sorting, field selection |\n| OpenAPI | `rules/framework-openapi.md` | OpenAPI 3.1 specs, documentation, schema definitions |\n\n## Versioning\n\nStrategies for API evolution without breaking clients.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| URL Path | `rules/versioning-url-path.md` | `/api/v1/` prefix routing, version-specific schemas |\n| Header | `rules/versioning-header.md` | `X-API-Version` header, content negotiation |\n| Deprecation | `rules/versioning-deprecation.md` | Sunset headers, lifecycle management, breaking change policy |\n\n## Error Handling\n\nRFC 9457 Problem Details for machine-readable, standardized error responses.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Problem Details | `rules/errors-problem-details.md` | RFC 9457 schema, `application/problem+json`, exception classes |\n| Validation | `rules/errors-validation.md` | Field-level errors, Pydantic integration, 422 responses |\n| Error Catalog | `rules/errors-error-catalog.md` | Problem type registry, error type URIs, client handling |\n\n## Quick Start Example\n\n```python\n# REST endpoint with versioning and RFC 9457 errors\nfrom fastapi import APIRouter, Depends, Request\nfrom fastapi.responses import JSONResponse\n\nrouter = APIRouter()\n\n@router.get(\"/api/v1/users/{user_id}\")\nasync def get_user(user_id: str, service: UserService = Depends()):\n    user = await service.get_user(user_id)\n    if not user:\n        raise NotFoundProblem(\n            resource=\"User\",\n            resource_id=user_id,\n        )\n    return UserResponseV1(id=user.id, name=user.full_name)\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Versioning strategy | URL path (`/api/v1/`) for public APIs |\n| Resource naming | Plural nouns, kebab-case |\n| Pagination | Cursor-based for large datasets |\n| Error format | RFC 9457 Problem Details with `application/problem+json` |\n| Error type URI | Your API dom",
    "contentTruncated": true
  },
  "architecture-decision-record": {
    "content": "# Architecture Decision Records\nArchitecture Decision Records (ADRs) are lightweight documents that capture important architectural decisions along with their context and consequences. This skill provides templates, examples, and best practices for creating and maintaining ADRs in your projects.\n\n## Overview\n- Making significant technology choices (databases, frameworks, cloud providers)\n- Designing system architecture or major components\n- Establishing patterns or conventions for the team\n- Evaluating trade-offs between multiple approaches\n- Documenting decisions that will impact future development\n\n## Why ADRs Matter\n\nADRs serve as architectural memory for your team:\n- **Context Preservation**: Capture why decisions were made, not just what was decided\n- **Onboarding**: Help new team members understand architectural rationale\n- **Prevent Revisiting**: Avoid endless debates about settled decisions\n- **Track Evolution**: See how architecture evolved over time\n- **Accountability**: Clear ownership and decision timeline\n\n## ADR Format (Nygard Template)\n\nEach ADR should follow this structure:\n\n### 1. Title\nFormat: `ADR-####: [Decision Title]`\nExample: `ADR-0001: Adopt Microservices Architecture`\n\n### 2. Status\nCurrent state of the decision:\n- **Proposed**: Under consideration\n- **Accepted**: Decision approved and being implemented\n- **Superseded**: Replaced by a later decision (reference ADR number)\n- **Deprecated**: No longer recommended but not yet replaced\n- **Rejected**: Considered but not adopted (document why)\n\n### 3. Context\n**What to include:**\n- Problem statement or opportunity\n- Business/technical constraints\n- Stakeholder requirements\n- Current state of the system\n- Forces at play (conflicting concerns)\n\n### 4. Decision\n**What to include:**\n- The choice being made\n- Key principles or patterns to follow\n- What will change as a result\n- Who is responsible for implementation\n\n**Be specific and actionable:**\n- ‚úÖ \"We will adopt microservices architecture using Node.js with Express\"\n- ‚ùå \"We will consider using microservices\"\n\n### 5. Consequences\n**What to include:**\n- Positive outcomes (benefits)\n- Negative outcomes (costs, risks, trade-offs)\n- Neutral outcomes (things that change but aren't clearly better/worse)\n\n### 6. Alternatives Considered\n**Document at least 2 alternatives:**\n\n**For each alternative, explain:**\n- What it was\n- Why it was considered\n- Why it was not chosen\n\n### 7. References (Optional)\nLinks to relevant resources:\n- Meeting notes or discussion threads\n- Related ADRs\n- External research or articles\n- Proof of concept implementations\n\n## ADR Lifecycle\n\n```\nProposed ‚Üí Accepted ‚Üí [Implemented] ‚Üí (Eventually) Superseded/Deprecated\n          ‚Üì\n      Rejected\n```\n\n## Best Practices\n\n### 1. **Keep ADRs Immutable**\nOnce accepted, don't edit ADRs. Create new ADRs that supersede old ones.\n- ‚úÖ Create ADR-0015 that supersedes ADR-0003\n- ‚ùå Update ADR-0003 with new decisions\n\n### 2. **Write in Present Tense**\nADRs are historical records ",
    "contentTruncated": true
  },
  "architecture-patterns": {
    "content": "# Architecture Patterns\n\nConsolidated architecture validation and enforcement patterns covering clean architecture, backend layer separation, project structure conventions, and test standards. Each category has individual rule files in `references/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Clean Architecture](#clean-architecture) | 3 | HIGH | SOLID principles, hexagonal architecture, ports & adapters, DDD |\n| [Project Structure](#project-structure) | 2 | HIGH | Folder conventions, nesting depth, import direction, barrel files |\n| [Backend Layers](#backend-layers) | 3 | HIGH | Router/service/repository separation, DI, file naming |\n| [Test Standards](#test-standards) | 3 | MEDIUM | AAA pattern, naming conventions, coverage thresholds |\n\n**Total: 11 rules across 4 categories**\n\n## Quick Start\n\n```python\n# Clean Architecture: Dependency Inversion via Protocol\nclass IUserRepository(Protocol):\n    async def get_by_id(self, id: str) -> User | None: ...\n\nclass UserService:\n    def __init__(self, repo: IUserRepository):\n        self._repo = repo  # Depends on abstraction, not concretion\n\n# FastAPI DI chain: DB -> Repository -> Service\ndef get_user_service(db: AsyncSession = Depends(get_db)) -> UserService:\n    return UserService(PostgresUserRepository(db))\n```\n\n```\n# Project Structure: Unidirectional Import Architecture\nshared/lib  ->  components  ->  features  ->  app\n(lowest)                                    (highest)\n\n# Backend Layers: Strict Separation\nRouters (HTTP) -> Services (Business Logic) -> Repositories (Data Access)\n```\n\n## Clean Architecture\n\nSOLID principles, hexagonal architecture, ports and adapters, and DDD tactical patterns for maintainable backends.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Hexagonal Architecture | `references/clean-hexagonal-ports-adapters.md` | Driving/driven ports, adapter implementations, layer structure |\n| SOLID & Dependency Rule | `references/clean-solid-dependency-rule.md` | Protocol-based interfaces, dependency inversion, FastAPI DI |\n| DDD Tactical Patterns | `references/clean-ddd-tactical-patterns.md` | Entities, value objects, aggregate roots, domain events |\n\n### Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Protocol vs ABC | Protocol (structural typing) |\n| Dataclass vs Pydantic | Dataclass for domain, Pydantic for API |\n| Repository granularity | One per aggregate root |\n| Transaction boundary | Service layer, not repository |\n| Event publishing | Collect in aggregate, publish after commit |\n\n## Project Structure\n\nFeature-based organization, max nesting depth, unidirectional imports, and barrel file prevention.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Folder Structure & Nesting | `references/structure-folder-conventions.md` | React/Next.js and FastAPI layouts, 4-level max nesting, barrel file rules |\n| Import Direction & Location | `referenc",
    "contentTruncated": true
  },
  "ascii-visualizer": {
    "content": "# ASCII Visualizer Skill\n\nCreate clear ASCII visualizations for explaining complex concepts.\n\n## Box-Drawing Characters\n\n**IMPORTANT:** Use a fixed-width (monospace) font for proper rendering.\n\n```\n‚îå‚îÄ‚îê‚îÇ‚îî‚îÄ‚îò  Standard weight\n‚îè‚îÅ‚îì‚îÉ‚îó‚îÅ‚îõ  Heavy weight\n‚îú‚îÄ‚î§‚î¨‚î¥    Connectors\n‚ïî‚ïê‚ïó‚ïë‚ïö‚ïê‚ïù  Double lines\n```\n\n## Quick Examples\n\n### Architecture\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ   Frontend   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Backend    ‚îÇ\n‚îÇ   React 19   ‚îÇ      ‚îÇ   FastAPI    ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                              ‚îÇ\n                              ‚ñº\n                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n                      ‚îÇ  PostgreSQL  ‚îÇ\n                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Progress\n```\n[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë] 80% Complete\n‚úÖ Design    (2 days)\n‚úÖ Backend   (5 days)\nüîÑ Frontend  (3 days)\n‚è≥ Testing   (pending)\n```\n\nSee `references/` for complete patterns.\n\n## Related Skills\n\n- `architecture-decision-record` - Document decisions that ASCII diagrams help visualize\n- `brainstorming` - Use visualizations to explore and communicate ideas\n- `explore` - Visualize codebase structure during exploration\n\n## Capability Details\n\n### architecture-diagrams\n**Keywords:** architecture, diagram, system design, components, flow\n**Solves:**\n- How do I visualize system architecture?\n- Show component relationships with ASCII\n- Explain system design visually\n- Create architecture diagrams in documentation\n\n### workflows\n**Keywords:** workflow, process, steps, pipeline, flowchart\n**Solves:**\n- How do I visualize process flow?\n- Show step-by-step workflow with ASCII\n- Explain pipeline stages visually\n- Document multi-agent workflows\n\n### comparisons\n**Keywords:** compare, vs, before after, metrics, changes\n**Solves:**\n- How do I compare two options visually?\n- Show before/after metrics\n- Display progress comparison\n- Visualize A/B testing results\n\n### file-trees\n**Keywords:** file tree, directory, structure, folder hierarchy\n**Solves:**\n- How do I show directory structure?\n- Visualize file hierarchy with ASCII\n- Explain codebase organization\n- Document project structure\n\n### progress-tracking\n**Keywords:** progress, status, completion, percentage, metrics\n**Solves:**\n- How do I show progress visually?\n- Create progress bars with ASCII\n- Display completion status\n- Track task completion metrics",
    "contentTruncated": false
  },
  "assess": {
    "content": "# Assess\n\nComprehensive assessment skill for answering \"is this good?\" with structured evaluation, scoring, and actionable recommendations.\n\n## Quick Start\n\n```bash\n/assess backend/app/services/auth.py\n/assess our caching strategy\n/assess the current database schema\n/assess frontend/src/components/Dashboard\n```\n\n---\n\n## STEP 0: Verify User Intent with AskUserQuestion\n\n**BEFORE creating tasks**, clarify assessment dimensions:\n\n```python\nAskUserQuestion(\n  questions=[{\n    \"question\": \"What dimensions to assess?\",\n    \"header\": \"Dimensions\",\n    \"options\": [\n      {\"label\": \"Full assessment (Recommended)\", \"description\": \"All dimensions: quality, maintainability, security, performance\"},\n      {\"label\": \"Code quality only\", \"description\": \"Readability, complexity, best practices\"},\n      {\"label\": \"Security focus\", \"description\": \"Vulnerabilities, attack surface, compliance\"},\n      {\"label\": \"Quick score\", \"description\": \"Just give me a 0-10 score with brief notes\"}\n    ],\n    \"multiSelect\": false\n  }]\n)\n```\n\n**Based on answer, adjust workflow:**\n- **Full assessment**: All 7 phases, parallel agents\n- **Code quality only**: Skip security and performance phases\n- **Security focus**: Prioritize security-auditor agent\n- **Quick score**: Single pass, brief output\n\n---\n\n## STEP 0b: Select Orchestration Mode\n\nChoose **Agent Teams** (mesh ‚Äî assessors cross-validate scores) or **Task tool** (star ‚Äî all report to lead):\n\n```python\nimport os\nteams_available = os.environ.get(\"CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS\") is not None\nforce_task_tool = os.environ.get(\"ORCHESTKIT_FORCE_TASK_TOOL\") == \"1\"\n\nif force_task_tool or not teams_available:\n    mode = \"task_tool\"\nelse:\n    # Teams available ‚Äî use for full multi-dimensional assessment\n    mode = \"agent_teams\" if dimensions == \"full\" else \"task_tool\"\n```\n\n1. `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS` set ‚Üí **Agent Teams mode** (for full assessment)\n2. Flag not set ‚Üí **Task tool mode** (default)\n3. Quick score or single-dimension ‚Üí **Task tool** (regardless of flag)\n\n| Aspect | Task Tool | Agent Teams |\n|--------|-----------|-------------|\n| Score calibration | Lead normalizes independently | Assessors discuss disagreements |\n| Cross-dimension findings | Lead correlates after completion | Security assessor alerts performance assessor of overlap |\n| Cost | ~200K tokens | ~500K tokens |\n| Best for | Quick scores, single dimension | Full multi-dimensional assessment |\n\n> **Context window:** For full codebase assessments (>20 files), use the 1M context window to avoid agent context exhaustion. On 200K context, the scope discovery in Phase 1.5 limits files to prevent overflow.\n\n> **Fallback:** If Agent Teams encounters issues, fall back to Task tool for remaining assessment.\n\n---\n\n## Task Management (CC 2.1.16)\n\n```python\n# Create main assessment task\nTaskCreate(\n  subject=\"Assess: {target}\",\n  description=\"Comprehensive evaluation with quality scores and recommendations\",\n  activeForm=\"Assessing {target}\"\n)\n\n# Create subta",
    "contentTruncated": true
  },
  "assess-complexity": {
    "content": "# Assess Complexity\n\nEvaluate task complexity using automated codebase analysis before starting implementation work.\n\n## Overview\n\n- Determining if a task is ready for implementation\n- Deciding whether to break down a large task\n- Estimating effort before committing to work\n- Identifying high-risk areas in the codebase\n- Planning sprint work with complexity scores\n\n## Usage\n\nAssess complexity for: **$ARGUMENTS**\n\n## Step 1: Gather Metrics\n\nRun the analysis script to collect codebase metrics:\n\n!`./scripts/analyze-codebase.sh \"$ARGUMENTS\"`\n\n## Step 2: Assess Each Criterion\n\nScore each criterion from 1-5 based on the metrics and your understanding:\n\n### 1. Lines of Code\n\n| Range | Score |\n|-------|-------|\n| < 50 lines | 1 |\n| 50-200 lines | 2 |\n| 200-500 lines | 3 |\n| 500-1500 lines | 4 |\n| 1500+ lines | 5 |\n\n### 2. Time Estimate\n\n| Duration | Score |\n|----------|-------|\n| < 30 minutes | 1 |\n| 30 min - 2 hours | 2 |\n| 2-8 hours | 3 |\n| 8-24 hours (1-3 days) | 4 |\n| 24+ hours (3+ days) | 5 |\n\n### 3. Number of Files\n\n| Count | Score |\n|-------|-------|\n| 1 file | 1 |\n| 2-3 files | 2 |\n| 4-10 files | 3 |\n| 11-25 files | 4 |\n| 26+ files | 5 |\n\n### 4. Dependencies Count\n\n| Unique Modules | Score |\n|----------------|-------|\n| 0 dependencies | 1 |\n| 1 dependency | 2 |\n| 2-3 dependencies | 3 |\n| 4-6 dependencies | 4 |\n| 7+ dependencies | 5 |\n\n### 5. Unknowns/Uncertainty\n\n| Level | Score |\n|-------|-------|\n| No unknowns - Everything clear | 1 |\n| Minimal - 1-2 minor questions | 2 |\n| Some - Several questions, researchable | 3 |\n| Significant - Many questions, requires exploration | 4 |\n| Many - Unclear scope, needs prototyping | 5 |\n\n### 6. Cross-Cutting Concerns\n\n| Scope | Score |\n|-------|-------|\n| Isolated change - Single module | 1 |\n| Minor integration - 2-3 modules | 2 |\n| Multiple integrations - 4-5 modules | 3 |\n| Cross-cutting - Affects many modules | 4 |\n| Architectural - System-wide impact | 5 |\n\n### 7. Risk Level\n\n| Risk | Score |\n|------|-------|\n| No risk - Trivial change | 1 |\n| Low risk - Well-understood pattern | 2 |\n| Medium risk - Some complexity, testable | 3 |\n| High risk - Complex logic, many edge cases | 4 |\n| Very high risk - Mission-critical, high stakes | 5 |\n\n## Step 3: Calculate Total Score\n\n**Sum all scores:** _____ / 35\n\n**Calculate average:** Total / 7 = _____\n\n### Complexity Level Assignment\n\n| Average Score | Level | Classification |\n|---------------|-------|----------------|\n| 1.0 - 1.4 | 1 | Trivial |\n| 1.5 - 2.4 | 2 | Simple |\n| 2.5 - 3.4 | 3 | Moderate |\n| 3.5 - 4.4 | 4 | Complex |\n| 4.5 - 5.0 | 5 | Very Complex |\n\n## Step 4: Decision\n\n### Level 1-3: Proceed\n\nTask is manageable. Continue with implementation.\n\n### Level 4-5: Break Down\n\nTask is too complex. Decompose into subtasks and reassess each part.\n\n## Output Format\n\nProvide assessment in this format:\n\n```\n## Complexity Assessment: [Target]\n\n**Date:** YYYY-MM-DD\n**Assessor:** [Agent Name]\n\n### Scores\n| Criterion | Score |\n|-----------|-------|\n| Lines of Code | ",
    "contentTruncated": true
  },
  "async-jobs": {
    "content": "# Async Jobs\n\nPatterns for background task processing with Celery, ARQ, and Redis. Covers task queues, canvas workflows, scheduling, retry strategies, rate limiting, and production monitoring. Each category has individual rule files in `references/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Configuration](#configuration) | celery-config | HIGH | Celery app setup, broker, serialization, worker tuning |\n| [Task Routing](#task-routing) | task-routing | HIGH | Priority queues, multi-queue workers, dynamic routing |\n| [Canvas Workflows](#canvas-workflows) | canvas-workflows | HIGH | Chain, group, chord, nested workflows |\n| [Retry Strategies](#retry-strategies) | retry-strategies | HIGH | Exponential backoff, idempotency, dead letter queues |\n| [Scheduling](#scheduling) | scheduled-tasks | MEDIUM | Celery Beat, crontab, database-backed schedules |\n| [Monitoring](#monitoring) | monitoring-health | MEDIUM | Flower, custom events, health checks, metrics |\n| [Result Backends](#result-backends) | result-backends | MEDIUM | Redis results, custom states, progress tracking |\n| [ARQ Patterns](#arq-patterns) | arq-patterns | MEDIUM | Async Redis Queue for FastAPI, lightweight jobs |\n\n**Total: 8 rules across 8 categories**\n\n## Quick Start\n\n```python\n# Celery task with retry\nfrom celery import shared_task\n\n@shared_task(\n    bind=True,\n    max_retries=3,\n    autoretry_for=(ConnectionError, TimeoutError),\n    retry_backoff=True,\n)\ndef process_order(self, order_id: str) -> dict:\n    result = do_processing(order_id)\n    return {\"order_id\": order_id, \"status\": \"completed\"}\n```\n\n```python\n# ARQ task with FastAPI\nfrom arq import create_pool\nfrom arq.connections import RedisSettings\n\nasync def generate_report(ctx: dict, report_id: str) -> dict:\n    data = await ctx[\"db\"].fetch_report_data(report_id)\n    pdf = await render_pdf(data)\n    return {\"report_id\": report_id, \"size\": len(pdf)}\n\n@router.post(\"/api/v1/reports\")\nasync def create_report(data: ReportRequest, arq: ArqRedis = Depends(get_arq_pool)):\n    job = await arq.enqueue_job(\"generate_report\", data.report_id)\n    return {\"job_id\": job.job_id}\n```\n\n## Configuration\n\nProduction Celery app configuration with secure defaults and worker tuning.\n\n### Key Patterns\n\n- **JSON serialization** with `task_serializer=\"json\"` for safety\n- **Late acknowledgment** with `task_acks_late=True` to prevent task loss on crash\n- **Time limits** with both `task_time_limit` (hard) and `task_soft_time_limit` (soft)\n- **Fair distribution** with `worker_prefetch_multiplier=1`\n- **Reject on lost** with `task_reject_on_worker_lost=True`\n\n### Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Serializer | JSON (never pickle) |\n| Ack mode | Late ack (`task_acks_late=True`) |\n| Prefetch | 1 for fair, 4-8 for throughput |\n| Time limit | soft < hard (e.g., 540/600) |\n| Timezone | UTC always |\n\n## Task Routing\n\nPriority queue configurati",
    "contentTruncated": true
  },
  "audio-language-models": {
    "content": "# Audio Language Models ()\n\nBuild real-time voice agents and audio processing using the latest native speech-to-speech models.\n\n## Overview\n\n- Real-time voice assistants and agents\n- Live conversational AI (phone agents, support bots)\n- Audio transcription with speaker diarization\n- Multilingual voice interactions\n- Text-to-speech generation\n- Voice-to-voice translation\n\n## Model Comparison (January )\n\n### Real-Time Voice (Speech-to-Speech)\n\n| Model | Latency | Languages | Price | Best For |\n|-------|---------|-----------|-------|----------|\n| **Grok Voice Agent** | <1s TTFA | 100+ | $0.05/min | Fastest, #1 Big Bench |\n| **Gemini Live API** | Low | 24 (30 voices) | Usage-based | Emotional awareness |\n| **OpenAI Realtime** | ~1s | 50+ | $0.10/min | Ecosystem integration |\n\n### Speech-to-Text Only\n\n| Model | WER | Latency | Best For |\n|-------|-----|---------|----------|\n| **Gemini 2.5 Pro** | ~5% | Medium | 9.5hr audio, diarization |\n| **GPT-4o-Transcribe** | ~7% | Medium | Accuracy + accents |\n| **AssemblyAI Universal-2** | 8.4% | 200ms | Best features |\n| **Deepgram Nova-3** | ~18% | <300ms | Lowest latency |\n| **Whisper Large V3** | 7.4% | Slow | Self-host, 99+ langs |\n\n## Grok Voice Agent API (xAI) - Fastest\n\n```python\nimport asyncio\nimport websockets\nimport json\n\nasync def grok_voice_agent():\n    \"\"\"Real-time voice agent with Grok - #1 on Big Bench Audio.\n\n    Features:\n    - <1 second time-to-first-audio (5x faster than competitors)\n    - Native speech-to-speech (no transcription intermediary)\n    - 100+ languages, $0.05/min\n    - OpenAI Realtime API compatible\n    \"\"\"\n    uri = \"wss://api.x.ai/v1/realtime\"\n    headers = {\"Authorization\": f\"Bearer {XAI_API_KEY}\"}\n\n    async with websockets.connect(uri, extra_headers=headers) as ws:\n        # Configure session\n        await ws.send(json.dumps({\n            \"type\": \"session.update\",\n            \"session\": {\n                \"model\": \"grok-4-voice\",\n                \"voice\": \"Aria\",  # or \"Eve\", \"Leo\"\n                \"instructions\": \"You are a helpful voice assistant.\",\n                \"input_audio_format\": \"pcm16\",\n                \"output_audio_format\": \"pcm16\",\n                \"turn_detection\": {\"type\": \"server_vad\"}\n            }\n        }))\n\n        # Stream audio in/out\n        async def send_audio(audio_stream):\n            async for chunk in audio_stream:\n                await ws.send(json.dumps({\n                    \"type\": \"input_audio_buffer.append\",\n                    \"audio\": base64.b64encode(chunk).decode()\n                }))\n\n        async def receive_audio():\n            async for message in ws:\n                data = json.loads(message)\n                if data[\"type\"] == \"response.audio.delta\":\n                    yield base64.b64decode(data[\"delta\"])\n\n        return send_audio, receive_audio\n\n# Expressive voice with auditory cues\nasync def expressive_response(ws, text: str):\n    \"\"\"Use auditory cues for natural speech.\"\"\"\n    # Supports: [whisper], [sigh], [laugh], [pause]\n    ",
    "contentTruncated": true
  },
  "audit-full": {
    "content": "# Full-Codebase Audit\n\nSingle-pass whole-project analysis leveraging Opus 4.6's extended context window. Loads entire codebases (~50K LOC) into context for cross-file vulnerability detection, architecture review, and dependency analysis.\n\n## Quick Start\n\n```bash\n/audit-full                          # Full audit (all modes)\n/audit-full security                 # Security-focused audit\n/audit-full architecture             # Architecture review\n/audit-full dependencies             # Dependency audit\n```\n\n> **Opus 4.6**: Uses `complexity: max` for extended thinking across entire codebases. 1M context (beta, Tier 4+) enables cross-file reasoning that chunked approaches miss.\n\n---\n\n## STEP 0: Verify User Intent with AskUserQuestion\n\n**BEFORE creating tasks**, clarify audit scope:\n\n```python\nAskUserQuestion(\n  questions=[\n    {\n      \"question\": \"What type of audit do you want to run?\",\n      \"header\": \"Audit mode\",\n      \"options\": [\n        {\"label\": \"Full audit (Recommended)\", \"description\": \"Security + architecture + dependencies in one pass\"},\n        {\"label\": \"Security audit\", \"description\": \"Cross-file vulnerability analysis, data flow tracing, OWASP mapping\"},\n        {\"label\": \"Architecture review\", \"description\": \"Pattern consistency, coupling analysis, dependency violations\"},\n        {\"label\": \"Dependency audit\", \"description\": \"License compliance, CVE checking, version currency\"}\n      ],\n      \"multiSelect\": false\n    },\n    {\n      \"question\": \"What should be audited?\",\n      \"header\": \"Scope\",\n      \"options\": [\n        {\"label\": \"Entire codebase\", \"description\": \"Load all source files into context\"},\n        {\"label\": \"Specific directory\", \"description\": \"Focus on a subdirectory (e.g., src/api/)\"},\n        {\"label\": \"Changed files only\", \"description\": \"Audit only files changed vs main branch\"}\n      ],\n      \"multiSelect\": false\n    }\n  ]\n)\n```\n\n**Based on answers, adjust workflow:**\n- **Full audit**: All 3 domains, maximum context usage\n- **Security only**: Focus token budget on source + config files\n- **Architecture only**: Focus on module boundaries, imports, interfaces\n- **Dependency only**: Focus on lock files, manifests, import maps\n- **Changed files only**: Use `git diff --name-only main...HEAD` to scope\n\n---\n\n## CRITICAL: Task Management is MANDATORY\n\n```python\nTaskCreate(\n  subject=\"Full-codebase audit\",\n  description=\"Single-pass audit using extended context\",\n  activeForm=\"Running full-codebase audit\"\n)\n\n# Phase subtasks\nTaskCreate(subject=\"Estimate token budget and plan loading\", activeForm=\"Estimating token budget\")\nTaskCreate(subject=\"Load codebase into context\", activeForm=\"Loading codebase\")\nTaskCreate(subject=\"Run audit analysis\", activeForm=\"Analyzing codebase\")\nTaskCreate(subject=\"Generate audit report\", activeForm=\"Generating report\")\n```\n\n---\n\n## STEP 1: Estimate Token Budget\n\nBefore loading files, estimate whether the codebase fits in context.\n\n### Run Token Estimation\n\n```bash\n# Use the estimation script\nbash ${",
    "contentTruncated": true
  },
  "best-practices": {
    "content": "# Best Practices - View Your Pattern Library\n\nDisplay your aggregated best practices library, showing successful patterns and anti-patterns across all projects.\n\n## Usage\n\n```\n/best-practices                     # Show full library\n/best-practices <category>          # Filter by category\n/best-practices --warnings          # Show only anti-patterns\n/best-practices --successes         # Show only successes\n/best-practices --stats             # Show statistics only\n```\n\n## Options\n\n- `<category>` - Filter by specific category (pagination, database, authentication, etc.)\n- `--warnings` - Show only anti-patterns (failed patterns)\n- `--successes` - Show only successful patterns\n- `--stats` - Show statistics summary without individual patterns\n\n## Workflow\n\n### 1. Query Memory for Best Practices\n\nUse the knowledge graph to search for stored patterns:\n\n```javascript\nmcp__memory__search_nodes({ query: \"patterns outcomes\" })\n```\n\n### 2. Aggregate Results\n\nGroup patterns by category, then by outcome:\n\n```json\n{\n  \"pagination\": {\n    \"successes\": [...],\n    \"failures\": [...]\n  },\n  \"authentication\": {\n    \"successes\": [...],\n    \"failures\": [...]\n  }\n}\n```\n\n### 3. Calculate Statistics\n\nFor each pattern:\n- Count occurrences across projects\n- Calculate success rate: successes / (successes + failures)\n- Note which projects contributed\n\n### 4. Display Output\n\n**Full Library View:**\n```\nüìö Your Best Practices Library\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nPAGINATION\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  ‚úÖ Cursor-based pagination (3 projects, always worked)\n     \"Scales well for large datasets\"\n\n  ‚ùå Offset pagination (failed in 2 projects)\n     \"Caused timeouts on tables with 1M+ rows\"\n     üí° Lesson: Use cursor-based for large datasets\n\nAUTHENTICATION\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n  ‚úÖ JWT + httpOnly refresh tokens (4 projects)\n     \"Secure and scalable for web apps\"\n\n  ‚ö†Ô∏è Session-based auth (mixed: 1 success, 1 failure)\n     \"Works but scaling issues in high-traffic scenarios\"\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nüìä Summary: 8 patterns | 5 ‚úÖ successes | 3 ‚ùå anti-patterns\nüí° Use `/remember --success` or `/remember --failed` to add more\n```\n\n**Stats Only View (`--stats`):**\n```\nüìä Best Practices Statistics\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\nTotal Patterns: 15\n‚îú‚îÄ‚îÄ ‚úÖ Successful: 10 (67%)\n‚îú‚îÄ‚îÄ ‚ùå Anti-patterns: 5 (33%)\n‚îî‚îÄ‚îÄ ‚ö†Ô∏è Mixed: 2\n\nCategories:\n‚îú‚îÄ‚îÄ pagination: 3 patterns (2 ‚úÖ, 1 ‚ùå)\n‚îú‚îÄ‚îÄ authentication: 4 patterns (3 ‚úÖ, 1 ‚ö†Ô∏è)\n‚îú‚îÄ‚îÄ database: 5 patterns (4 ‚úÖ, 1 ‚ùå)\n‚îî‚îÄ‚îÄ api: 3 patterns (1 ‚úÖ, 2 ‚ùå)\n\nProjects Contributing: 7\nLast Updated: 2 days ago\n```\n\n**Filtered View (by category):**\n```\nüìö Best Practices: PAGINATION\n‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n\n  ‚úÖ Cursor-based pagination (3 projects, always worked)\n     \"Scales well for large datasets\"\n     Projects: project-a, project-b, project-c\n\n  ‚ùå Offset pa",
    "contentTruncated": true
  },
  "biome-linting": {
    "content": "# Biome Linting\n\nFast, unified linting and formatting (10-25x faster than ESLint + Prettier).\n\n## Why Biome in 2026\n\n| Aspect | Biome | ESLint + Prettier |\n|--------|-------|-------------------|\n| Speed | ~200ms for 10k lines | 3-5s |\n| Config files | 1 (biome.json) | 4+ |\n| npm packages | 1 binary | 127+ |\n| Rules | 421 | Varies by plugins |\n| Type inference | Yes (v2.0+) | Requires tsconfig |\n\n## Quick Start\n\n```bash\n# Install\nnpm install --save-dev --save-exact @biomejs/biome\n\n# Initialize\nnpx @biomejs/biome init\n\n# Check (lint + format)\nnpx @biomejs/biome check .\n\n# Fix\nnpx @biomejs/biome check --write .\n\n# CI mode (fails on errors)\nnpx @biomejs/biome ci .\n```\n\n## Biome 2.0 Features\n\n**Type Inference**: Reads `.d.ts` from node_modules for type-aware rules:\n\n```json\n{\n  \"linter\": {\n    \"rules\": {\n      \"nursery\": {\n        \"noFloatingPromises\": \"error\"  // Catches unhandled promises\n      }\n    }\n  }\n}\n```\n\n**Multi-file Analysis**: Cross-module analysis for better diagnostics.\n\n## Basic Configuration\n\n```json\n{\n  \"$schema\": \"https://biomejs.dev/schemas/2.0.0/schema.json\",\n  \"formatter\": {\n    \"enabled\": true,\n    \"indentStyle\": \"space\",\n    \"indentWidth\": 2,\n    \"lineWidth\": 100\n  },\n  \"linter\": {\n    \"enabled\": true,\n    \"rules\": {\n      \"recommended\": true,\n      \"correctness\": {\n        \"noUnusedVariables\": \"error\",\n        \"noUnusedImports\": \"error\"\n      },\n      \"suspicious\": {\n        \"noExplicitAny\": \"warn\"\n      }\n    }\n  },\n  \"javascript\": {\n    \"formatter\": {\n      \"quoteStyle\": \"single\",\n      \"trailingCommas\": \"all\"\n    }\n  }\n}\n```\n\n## ESLint Migration\n\n```bash\n# Auto-migrate configuration\nnpx @biomejs/biome migrate eslint --write\n```\n\n**Common Rule Mappings:**\n| ESLint | Biome |\n|--------|-------|\n| no-unused-vars | correctness/noUnusedVariables |\n| no-console | suspicious/noConsole |\n| @typescript-eslint/* | Most supported |\n| eslint-plugin-react | Most supported |\n| eslint-plugin-jsx-a11y | Most supported |\n\n## CI Integration\n\n```yaml\n# .github/workflows/lint.yml\n- uses: biomejs/setup-biome@v2\n- run: biome ci .\n```\n\n## Overrides for Gradual Adoption\n\n```json\n{\n  \"overrides\": [\n    {\n      \"include\": [\"*.test.ts\", \"*.spec.ts\"],\n      \"linter\": {\n        \"rules\": {\n          \"suspicious\": { \"noExplicitAny\": \"off\" }\n        }\n      }\n    },\n    {\n      \"include\": [\"legacy/**\"],\n      \"linter\": { \"enabled\": false }\n    }\n  ]\n}\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| New vs migration | Biome first for new projects; migrate existing gradually |\n| Config strictness | Start with recommended, tighten over time |\n| CI strategy | Use `biome ci` for strict mode, `biome check` for local |\n| Type inference | Enable for TypeScript projects (v2.0+) |\n\n## Related Skills\n\n- `vite-advanced` - Build tooling integration\n- `react-server-components-framework` - React linting rules\n- `ci-cd-engineer` - CI pipeline setup\n\n## References\n\n- [ESLint Migration](references/eslint-migration.md) - Step-by-step migr",
    "contentTruncated": true
  },
  "brainstorming": {
    "content": "# Brainstorming Ideas Into Designs\n\nTransform rough ideas into fully-formed designs through intelligent agent selection and structured exploration.\n\n**Core principle:** Analyze the topic, select relevant agents dynamically, explore alternatives in parallel, present design incrementally.\n\n---\n\n## STEP 0: Verify User Intent with AskUserQuestion\n\n**BEFORE creating tasks**, clarify brainstorming constraints:\n\n```python\nAskUserQuestion(\n  questions=[\n    {\n      \"question\": \"What type of design exploration?\",\n      \"header\": \"Type\",\n      \"options\": [\n        {\"label\": \"Open exploration (Recommended)\", \"description\": \"Generate 10+ ideas, evaluate all, synthesize top 3\"},\n        {\"label\": \"Constrained design\", \"description\": \"I have specific requirements to work within\"},\n        {\"label\": \"Comparison\", \"description\": \"Compare 2-3 specific approaches I have in mind\"},\n        {\"label\": \"Quick ideation\", \"description\": \"Generate ideas fast, skip deep evaluation\"}\n      ],\n      \"multiSelect\": false\n    },\n    {\n      \"question\": \"Any preferences or constraints?\",\n      \"header\": \"Constraints\",\n      \"options\": [\n        {\"label\": \"None\", \"description\": \"Explore all possibilities\"},\n        {\"label\": \"Use existing patterns\", \"description\": \"Prefer patterns already in codebase\"},\n        {\"label\": \"Minimize complexity\", \"description\": \"Favor simpler solutions\"},\n        {\"label\": \"I'll specify\", \"description\": \"Let me provide specific constraints\"}\n      ],\n      \"multiSelect\": false\n    }\n  ]\n)\n```\n\n**Based on answers, adjust workflow:**\n- **Open exploration**: Full 7-phase process with all agents\n- **Constrained design**: Skip divergent phase, focus on feasibility\n- **Comparison**: Skip ideation, jump to evaluation phase\n- **Quick ideation**: Generate ideas, skip deep evaluation\n\n---\n\n## STEP 0b: Select Orchestration Mode\n\nChoose **Agent Teams** (mesh ‚Äî agents debate and challenge ideas) or **Task tool** (star ‚Äî all report to lead):\n\n1. `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1` ‚Üí **Agent Teams mode**\n2. Agent Teams unavailable ‚Üí **Task tool mode** (default)\n3. Otherwise: Open exploration with 3+ agents ‚Üí recommend **Agent Teams** (real-time debate produces better ideas); Quick ideation ‚Üí **Task tool**\n\n| Aspect | Task Tool | Agent Teams |\n|--------|-----------|-------------|\n| Idea generation | Each agent generates independently | Agents riff on each other's ideas |\n| Devil's advocate | Lead challenges after all complete | Agents challenge each other in real-time |\n| Cost | ~150K tokens | ~400K tokens |\n| Best for | Quick ideation, constrained design | Open exploration, deep evaluation |\n\n> **Fallback:** If Agent Teams encounters issues, fall back to Task tool for remaining phases.\n\n---\n\n## CRITICAL: Task Management is MANDATORY (CC 2.1.16)\n\n```python\n# Create main task IMMEDIATELY\nTaskCreate(\n  subject=\"Brainstorm: {topic}\",\n  description=\"Design exploration with parallel agent research\",\n  activeForm=\"Brainstorming {topic}\"\n)\n\n# Create subtasks for ea",
    "contentTruncated": true
  },
  "browser-tools": {
    "content": "# Browser Tools\n\nBrowser automation and content capture patterns using agent-browser CLI, Playwright, and Puppeteer. Each category has individual rule files in `references/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | When to Use |\n|----------|-------|-------------|\n| [Playwright Setup](#playwright-setup) | 1 | Installing and configuring Playwright for automation |\n| [Page Interaction](#page-interaction) | 1 | Clicking, filling forms, navigating with snapshot + refs |\n| [Content Extraction](#content-extraction) | 1 | Extracting text, HTML, structured data from pages |\n| [SPA Extraction](#spa-extraction) | 1 | React, Vue, Angular apps with client-side rendering |\n| [Scraping Strategies](#scraping-strategies) | 1 | Multi-page crawls, pagination, recursive crawling |\n| [Anti-Bot Handling](#anti-bot-handling) | 1 | Rate limiting, CAPTCHA, session management |\n| [Authentication Flows](#authentication-flows) | 1 | Login forms, OAuth, SSO, session persistence |\n| [Structured Output](#structured-output) | 1 | Converting scraped content to clean markdown/JSON |\n\n**Total: 8 rules across 8 categories**\n\n## Quick Start\n\n```bash\n# Install agent-browser\nnpm install -g agent-browser\nagent-browser install                # Download Chromium\n\n# Basic capture workflow\nagent-browser open https://example.com\nagent-browser wait --load networkidle\nagent-browser snapshot -i            # Get interactive elements with @refs\nagent-browser get text @e5           # Extract content by ref\nagent-browser screenshot /tmp/page.png\nagent-browser close\n```\n\n```bash\n# Fallback decision tree\n# 1. Try WebFetch first (fast, no browser overhead)\n# 2. If empty/partial -> use agent-browser\n# 3. If SPA -> wait --load networkidle\n# 4. If login required -> authentication flow + state save\n# 5. If dynamic -> wait @element or wait --text\n```\n\n```bash\n# Authentication with state persistence\nagent-browser open https://app.example.com/login\nagent-browser snapshot -i\nagent-browser fill @e1 \"$EMAIL\"\nagent-browser fill @e2 \"$PASSWORD\"\nagent-browser click @e3\nagent-browser wait --url \"**/dashboard\"\nagent-browser state save /tmp/auth-state.json\n```\n\n```bash\n# SPA extraction (React/Vue/Angular)\nagent-browser open https://react-app.example.com\nagent-browser wait --load networkidle\nagent-browser eval \"document.querySelector('article').innerText\"\n```\n\n## Playwright Setup\n\nBrowser automation setup using agent-browser CLI (93% less context than full Playwright MCP) or Playwright directly.\n\n| Rule | Description |\n|------|-------------|\n| `playwright-setup.md` | Installation, configuration, environment variables, cloud providers |\n\n**Key Decisions:** agent-browser CLI preferred | `snapshot -i` for element discovery | `--session` for parallel isolation\n\n## Page Interaction\n\nInteract with page elements using snapshot refs for clicking, filling, and navigating.\n\n| Rule | Description |\n|------|-------------|\n| `page-interaction.md` | Click, fill, navigate, wait patterns using snapshot refs |\n\n**Ke",
    "contentTruncated": true
  },
  "caching": {
    "content": "# Caching\n\nComprehensive caching patterns for backend services and LLM applications. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Backend Caching](#backend-caching) | 3 | HIGH | Cache-aside, write-through, invalidation, stampede prevention |\n| [Prompt Caching](#prompt-caching) | 3 | HIGH | Claude cache_control, OpenAI prefix caching, breakpoint strategies |\n| [Semantic Caching](#semantic-caching) | 3 | MEDIUM | Vector similarity cache, multi-level hierarchy, Redis hybrid search |\n| [Cost Tracking](#cost-tracking) | 3 | MEDIUM | Langfuse cost tracking, per-agent attribution, cache ROI metrics |\n\n**Total: 12 rules across 4 categories**\n\n## Backend Caching\n\nRedis-based caching patterns for high-performance backends.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Cache-Aside | `rules/backend-cache-aside.md` | Lazy loading with get_or_set and Redis |\n| Write-Through | `rules/backend-write-through.md` | Synchronous cache + DB writes, write-behind batching |\n| Invalidation | `rules/backend-invalidation.md` | TTL, event-based, version-based + stampede prevention |\n\n## Prompt Caching\n\nProvider-native prompt caching for 90% token savings.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Claude Caching | `rules/prompt-claude.md` | cache_control with ephemeral TTL (5m/1h) |\n| OpenAI Caching | `rules/prompt-openai.md` | Automatic prefix caching, no markers needed |\n| Breakpoints | `rules/prompt-breakpoints.md` | Processing order, pricing, best practices |\n\n## Semantic Caching\n\nCache LLM responses by semantic similarity with Redis vector search.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Vector Cache | `rules/semantic-vector.md` | Redis vector similarity with configurable thresholds |\n| Multi-Level | `rules/semantic-multi-level.md` | L1 exact -> L2 semantic -> L3 prompt -> L4 LLM hierarchy |\n| Redis Hybrid | `rules/semantic-redis.md` | Redis 8.4 FT.HYBRID for metadata + vector queries |\n\n## Cost Tracking\n\nMonitor cache effectiveness and LLM costs with Langfuse.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Langfuse Tracking | `rules/cost-langfuse.md` | Automatic cost tracking with @observe decorator |\n| Cost Attribution | `rules/cost-attribution.md` | Per-agent hierarchical cost rollup |\n| Effectiveness | `rules/cost-effectiveness.md` | Cache hit rate calculation and ROI metrics |\n\n## Quick Start Example\n\n```python\nimport redis.asyncio as redis\nimport json\n\nclass CacheAside:\n    def __init__(self, redis_client: redis.Redis, default_ttl: int = 3600):\n        self.redis = redis_client\n        self.ttl = default_ttl\n\n    async def get_or_set(self, key: str, fetch_fn, ttl: int | None = None):\n        \"\"\"Get from cache, or fetch and cache.\"\"\"\n        cached = await self.redis.get(key)\n        if cached:\n            return json.loads(cached)\n\n        value = await f",
    "contentTruncated": true
  },
  "code-review-playbook": {
    "content": "# Code Review Playbook\nThis skill provides a comprehensive framework for effective code reviews that improve code quality, share knowledge, and foster collaboration. Whether you're a reviewer giving feedback or an author preparing code for review, this playbook ensures reviews are thorough, consistent, and constructive.\n\n## Overview\n- Reviewing pull requests or merge requests\n- Preparing code for review (self-review)\n- Establishing code review standards for teams\n- Training new developers on review best practices\n- Resolving disagreements about code quality\n- Improving review processes and efficiency\n\n## Code Review Philosophy\n\n### Purpose of Code Reviews\n\nCode reviews serve multiple purposes:\n\n1. **Quality Assurance**: Catch bugs, logic errors, and edge cases\n2. **Knowledge Sharing**: Spread domain knowledge across the team\n3. **Consistency**: Ensure codebase follows conventions and patterns\n4. **Mentorship**: Help developers improve their skills\n5. **Collective Ownership**: Build shared responsibility for code\n6. **Documentation**: Create discussion history for future reference\n\n### Principles\n\n**Be Kind and Respectful:**\n- Review the code, not the person\n- Assume positive intent\n- Praise good solutions\n- Frame feedback constructively\n\n**Be Specific and Actionable:**\n- Point to specific lines of code\n- Explain *why* something should change\n- Suggest concrete improvements\n- Provide examples when helpful\n\n**Balance Speed with Thoroughness:**\n- Aim for timely feedback (< 24 hours)\n- Don't rush critical reviews\n- Use automation for routine checks\n- Focus human review on logic and design\n\n**Distinguish Must-Fix from Nice-to-Have:**\n- Use conventional comments to indicate severity\n- Block merges only for critical issues\n- Allow authors to defer minor improvements\n- Capture deferred work in follow-up tickets\n\n---\n\n## Conventional Comments\n\nA standardized format for review comments that makes intent clear.\n\n### Format\n\n```\n<label> [decorations]: <subject>\n\n[discussion]\n```\n\n### Labels\n\n| Label | Meaning | Blocks Merge? |\n|-------|---------|---------------|\n| **praise** | Highlight something positive | No |\n| **nitpick** | Minor, optional suggestion | No |\n| **suggestion** | Propose an improvement | No |\n| **issue** | Problem that should be addressed | Usually |\n| **question** | Request clarification | No |\n| **thought** | Idea to consider | No |\n| **chore** | Routine task (formatting, deps) | No |\n| **note** | Informational comment | No |\n| **todo** | Follow-up work needed | Maybe |\n| **security** | Security concern | **Yes** |\n| **bug** | Potential bug | **Yes** |\n| **breaking** | Breaking change | **Yes** |\n\n### Decorations\n\nOptional modifiers in square brackets:\n\n| Decoration | Meaning |\n|------------|---------|\n| **[blocking]** | Must be addressed before merge |\n| **[non-blocking]** | Optional, can be deferred |\n| **[if-minor]** | Only if it's a quick fix |\n\n### Examples\n\n```typescript\n// ‚úÖ Good: Clear, specific, actionable\n\npraise: Excellent use o",
    "contentTruncated": true
  },
  "commit": {
    "content": "# Smart Commit\n\nSimple, validated commit creation. Run checks locally, no agents needed for standard commits.\n\n## Quick Start\n\n```bash\n/commit\n```\n\n## Workflow\n\n### Phase 1: Pre-Commit Safety Check\n\n```bash\n# CRITICAL: Verify we're not on dev/main\nBRANCH=$(git branch --show-current)\nif [[ \"$BRANCH\" == \"dev\" || \"$BRANCH\" == \"main\" || \"$BRANCH\" == \"master\" ]]; then\n  echo \"STOP! Cannot commit directly to $BRANCH\"\n  echo \"Create a feature branch: git checkout -b issue/<number>-<description>\"\n  exit 1\nfi\n```\n\n### Phase 2: Run Validation Locally\n\nRun every check that CI runs:\n\n```bash\n# Backend (Python)\npoetry run ruff format --check app/\npoetry run ruff check app/\npoetry run mypy app/\n\n# Frontend (Node.js)\nnpm run format:check\nnpm run lint\nnpm run typecheck\n```\n\nFix any failures before proceeding.\n\n### Phase 3: Review Changes\n\n```bash\ngit status\ngit diff --staged   # What will be committed\ngit diff            # Unstaged changes\n```\n\n### Phase 4: Stage and Commit\n\n```bash\n# Stage files\ngit add <files>\n# Or all: git add .\n\n# Commit with conventional format\ngit commit -m \"<type>(#<issue>): <brief description>\n\n- [Change 1]\n- [Change 2]\n\nCo-Authored-By: Claude <noreply@anthropic.com>\"\n\n# Verify\ngit log -1 --stat\n```\n\n## Commit Types\n\n| Type | Use For |\n|------|---------|\n| `feat` | New feature |\n| `fix` | Bug fix |\n| `refactor` | Code improvement |\n| `docs` | Documentation |\n| `test` | Tests only |\n| `chore` | Build/deps/CI |\n\n## Rules\n\n1. **Run validation locally** - Don't spawn agents to run lint/test\n2. **NO file creation** - Don't create MD files or documentation\n3. **One logical change per commit** - Keep commits focused\n4. **Reference issues** - Use `#123` format in commit message\n5. **Subject line < 72 chars** - Keep it concise\n\n## Quick Commit\n\nFor trivial changes (typos, single-line fixes):\n\n```bash\ngit add . && git commit -m \"fix(#123): Fix typo in error message\n\nCo-Authored-By: Claude <noreply@anthropic.com>\"\n```\n\n## Related Skills\n- create-pr: Create pull requests from commits\n- review-pr: Review changes before committing\n- fix-issue: Fix issues and commit the fixes\n- issue-progress-tracking: Auto-updates GitHub issues with commit progress\n\n## Rules\n\nEach category has individual rule files in `rules/` loaded on-demand:\n\n| Category | Rule | Impact | Key Pattern |\n|----------|------|--------|-------------|\n| Atomic Commits | `rules/atomic-commit.md` | CRITICAL | One logical change per commit, atomicity test |\n| Commit Splitting | `rules/commit-splitting.md` | HIGH | `git add -p`, interactive staging, separation strategies |\n| Conventional Format | `rules/conventional-format.md` | HIGH | type(scope): description, breaking changes |\n\n**Total: 3 rules across 3 categories**\n\n## References\n\n- [Conventional Commits](references/conventional-commits.md)\n- [Recovery](references/recovery.md)",
    "contentTruncated": false
  },
  "competitive-monitoring": {
    "content": "# Competitive Monitoring\n\nTrack competitor websites for changes in pricing, features, positioning, and content.\n\n## Quick Start\n\n```bash\n# Capture initial snapshot\n/ork:competitive-monitoring capture https://competitor.com/pricing\n\n# Check for changes (compares to last snapshot)\n/ork:competitive-monitoring diff https://competitor.com/pricing\n\n# View change history\n/ork:competitive-monitoring history competitor.com\n```\n\n## Core Concepts\n\n### Snapshot\nA point-in-time capture of a webpage including:\n- **Text content** - Main body text\n- **Structured data** - Pricing tiers, feature lists, etc.\n- **Screenshot** - Visual state\n- **Metadata** - Timestamp, URL, capture method\n\n### Diff\nComparison between two snapshots showing:\n- **Added content** - New text, features, prices\n- **Removed content** - Deleted sections\n- **Changed content** - Modified values\n- **Visual changes** - Layout/design shifts\n\n### Change Classification\n\n| Severity | Examples | Action |\n|----------|----------|--------|\n| **Critical** | Price increase/decrease, major feature change | Immediate alert |\n| **High** | New feature added, feature removed | Review required |\n| **Medium** | Copy changes, positioning shift | Note for analysis |\n| **Low** | Typos, minor styling | Log only |\n\n## Site Discovery with Tavily (Optional Pre-Step)\n\nWhen `TAVILY_API_KEY` is available, use Tavily's crawl API to discover and extract all key pages on a competitor's site in one call. This replaces the manual map‚Üíextract two-step workflow.\n\n### Option A: Crawl (Recommended ‚Äî Single Call)\n\n```bash\n# Crawl competitor site ‚Äî discovers URLs and extracts content in one step\ncurl -s -X POST 'https://api.tavily.com/crawl' \\\n  -H 'Content-Type: application/json' \\\n  -H \"Authorization: Bearer $TAVILY_API_KEY\" \\\n  -d '{\n    \"url\": \"https://competitor.com\",\n    \"max_depth\": 2,\n    \"limit\": 50,\n    \"include_raw_content\": \"markdown\",\n    \"include_paths\": [\"/pricing*\", \"/features*\", \"/changelog*\", \"/blog*\"]\n  }' | python3 -c \"\nimport json, sys\ndata = json.load(sys.stdin)\nfor page in data.get('results', []):\n    fname = page['url'].replace('https://', '').replace('/', '_')\n    with open(f'.competitive-intel/snapshots/{fname}.md', 'w') as f:\n        f.write(page.get('raw_content', page.get('content', '')))\n    print(f'Captured: {page[\\\"url\\\"]}')\n\"\n```\n\n### Option B: Map + Extract (Granular Control)\n\nUse when you need to filter URLs before extracting:\n\n```bash\n# Step 1: Discover URLs\ncurl -s -X POST 'https://api.tavily.com/map' \\\n  -H 'Content-Type: application/json' \\\n  -H \"Authorization: Bearer $TAVILY_API_KEY\" \\\n  -d '{\"url\": \"https://competitor.com\", \"max_depth\": 2, \"limit\": 50}' \\\n  | python3 -c \"\nimport json, sys\nfor url in json.load(sys.stdin).get('urls', []): print(url)\n\" > .competitive-intel/discovered-urls.txt\n\n# Step 2: Filter and batch extract\nURLS=$(grep -E '(pricing|features|changelog)' .competitive-intel/discovered-urls.txt \\\n  | head -20 | python3 -c \"import json,sys; print(json.dumps([l.strip() for l in sys",
    "contentTruncated": true
  },
  "configure": {
    "content": "# OrchestKit Configuration\n\nInteractive setup for customizing your OrchestKit installation.\n\n## Quick Start\n\n```bash\n/configure\n```\n\n## Step 1: Choose Preset\n\nUse AskUserQuestion:\n\n| Preset | Skills | Agents | Hooks | Description |\n|--------|--------|--------|-------|-------------|\n| **Complete** | 78 | 20 | 92 | Everything |\n| **Standard** | 78 | 0 | 92 | Skills, no agents |\n| **Lite** | 10 | 0 | 92 | Essential only |\n| **Hooks-only** | 0 | 0 | 92 | Just safety |\n| **Monorepo** | 78 | 20 | 92 | Complete + monorepo detection |\n\n## Step 2: Customize Skill Categories\n\nCategories available:\n- AI/ML (26 skills)\n- Backend (15 skills)\n- Frontend (8 skills)\n- Testing (13 skills)\n- Security (7 skills)\n- DevOps (4 skills)\n- Planning (6 skills)\n\n## Step 3: Customize Agents\n\n**Product Agents (6):**\n- market-intelligence\n- product-strategist\n- requirements-translator\n- ux-researcher\n- prioritization-analyst\n- business-case-builder\n\n**Technical Agents (14):**\n- backend-system-architect\n- frontend-ui-developer\n- database-engineer\n- llm-integrator\n- workflow-architect\n- data-pipeline-engineer\n- test-generator\n- code-quality-reviewer\n- security-auditor\n- security-layer-auditor\n- debug-investigator\n- metrics-architect\n- rapid-ui-designer\n- system-design-reviewer\n\n## Step 4: Configure Hooks\n\n**Safety Hooks (Always On):**\n- git-branch-protection\n- file-guard\n- redact-secrets\n\n**Toggleable Hooks:**\n- Productivity (auto-approve, logging)\n- Quality Gates (coverage, patterns)\n- Team Coordination (locks, conflicts)\n- Notifications (desktop, sound)\n\n## Step 5: Configure MCPs (Optional)\n\nAll MCPs disabled by default. Enable selectively:\n\n| MCP | Purpose |\n|-----|---------|\n| context7 | Library documentation |\n| sequential-thinking | Complex reasoning |\n| memory | Cross-session persistence |\n| playwright | Browser automation |\n\n## Step 6: CC 2.1.7 Settings (New)\n\nConfigure CC 2.1.7-specific features:\n\n### Turn Duration Display\n\n```\nEnable turn duration in statusline? [y/N]: y\n```\n\nAdds to settings.json:\n```json\n{\n  \"statusline\": {\n    \"showTurnDuration\": true\n  }\n}\n```\n\n### MCP Auto-Deferral Threshold\n\n```\nMCP deferral threshold (default 10%): 10\n```\n\nAdds to config.json:\n```json\n{\n  \"cc217\": {\n    \"mcp_defer_threshold\": 0.10,\n    \"use_effective_window\": true\n  }\n}\n```\n\n### Effective Context Window Mode\n\n```\nUse effective context window for calculations? [Y/n]: y\n```\n\nWhen enabled:\n- Statusline shows `context_window.effective_percentage`\n- Compression triggers use effective window\n- MCP deferral more accurate\n\n## Step 7: CC 2.1.20 Settings\n\nConfigure CC 2.1.20-specific features:\n\n### Task Deletion Support\n\n```\nEnable task deletion (status: \"deleted\")? [Y/n]: y\n```\n\nEnables orphan detection and automatic cleanup of blocked tasks.\n\n### PR Status Enrichment\n\n```\nEnable PR status enrichment at session start? [Y/n]: y\n```\n\nDetects open PRs on current branch and sets `ORCHESTKIT_PR_URL` / `ORCHESTKIT_PR_STATE` env vars.\n\n### Background Agent Permission Pre-Mapping\n\n```\nEnable pe",
    "contentTruncated": true
  },
  "context-optimization": {
    "content": "# Context Optimization\n\n**The discipline of managing, compressing, and engineering LLM context windows for maximum effectiveness.**\n\n## Overview\n\nContext optimization unifies two complementary disciplines: **context compression** (reducing context size while preserving critical information) and **context engineering** (curating the smallest high-signal token set that achieves desired outcomes). Together, they ensure agents operate efficiently within token budgets without sacrificing task completion quality.\n\n**Key Metric:** Tokens-per-task (total tokens to complete a task), NOT tokens-per-request.\n\n**Key Insight:** 80% of agent performance variance is explained by token usage. Optimize context efficiency BEFORE switching models.\n\n## When to Use\n\n- Long-running conversations approaching context limits\n- Designing agent system prompts and tool definitions\n- Optimizing RAG retrieval pipelines\n- Multi-step agent workflows with accumulating history\n- Sessions with large tool outputs (can reach 83.9% of context)\n- Reducing token costs while maintaining quality\n- Building multi-agent architectures with context isolation\n\n---\n\n## The \"Lost in the Middle\" Phenomenon\n\nModels pay unequal attention across the context window:\n\n```\nAttention\nStrength   HIGH        LOW (10-40% recall)                      HIGH\n           START              MIDDLE                             END\n```\n\n| Position | Recall Rate | Best For |\n|----------|-------------|----------|\n| First 10% | 85-95% | Identity, constraints, critical rules |\n| Middle 50% | 10-40% | Background context, optional details |\n| Last 20% | 80-95% | Current task, recent messages, output format |\n\n---\n\n## The Five Context Layers\n\n```\nLayer 1: System Prompt        (~5-10% of budget)   Identity, constraints\nLayer 2: Tool Definitions     (~5-15% of budget)   Capabilities, trigger conditions\nLayer 3: Retrieved Documents  (~20-30% of budget)  Just-in-time knowledge\nLayer 4: Message History      (~30-50% of budget)  Conversation continuity\nLayer 5: Tool Outputs         (~10-30% of budget)  Observations (VARIABLE!)\n```\n\n**Critical Finding:** Tool outputs can consume 83.9% of total context. Always truncate and structure tool outputs.\n\n---\n\n## Compression Strategy Quick Reference\n\n| Strategy | Compression | Interpretable | Verifiable | Best For |\n|----------|-------------|---------------|------------|----------|\n| Anchored Iterative | 60-80% | Yes | Yes | Long sessions |\n| Opaque | 95-99% | No | No | Storage-critical |\n| Regenerative Full | 70-85% | Yes | Partial | Simple tasks |\n| Sliding Window | 50-70% | Yes | Yes | Real-time chat |\n\n**Recommended:** Anchored Iterative Summarization with probe-based evaluation.\n\n---\n\n## Anchored Summarization (RECOMMENDED)\n\nMaintains structured, persistent summaries with forced sections:\n\n```markdown\n## Session Intent\n[What we're trying to accomplish - NEVER lose this]\n\n## Files Modified\n- path/to/file.ts: Added function X, modified class Y\n\n## Decisions Made\n- Decision 1: Chose X ",
    "contentTruncated": true
  },
  "create-pr": {
    "content": "# Create Pull Request\n\nComprehensive PR creation with validation. All output goes directly to GitHub PR.\n\n## Quick Start\n\n```bash\n/create-pr\n```\n\n---\n\n## STEP 0: Verify User Intent with AskUserQuestion\n\n**BEFORE creating tasks**, clarify PR type:\n\n```python\nAskUserQuestion(\n  questions=[{\n    \"question\": \"What type of PR is this?\",\n    \"header\": \"Type\",\n    \"options\": [\n      {\"label\": \"Feature (Recommended)\", \"description\": \"New functionality with full validation\"},\n      {\"label\": \"Bug fix\", \"description\": \"Fix for existing issue\"},\n      {\"label\": \"Refactor\", \"description\": \"Code improvement, no behavior change\"},\n      {\"label\": \"Quick\", \"description\": \"Skip validation, just create PR\"}\n    ],\n    \"multiSelect\": false\n  }]\n)\n```\n\n**Based on answer, adjust workflow:**\n- **Feature**: Full validation with all agents\n- **Bug fix**: Focus on test verification\n- **Refactor**: Skip new feature validation\n- **Quick**: Skip all validation, just create PR\n\n---\n\n## ‚ö†Ô∏è CRITICAL: Task Management is MANDATORY (CC 2.1.16)\n\n**BEFORE doing ANYTHING else, create tasks to show progress:**\n\n```python\n# 1. Create main PR task IMMEDIATELY\nTaskCreate(\n  subject=\"Create PR for {branch}\",\n  description=\"PR creation with parallel validation agents\",\n  activeForm=\"Creating pull request\"\n)\n\n# 2. Create subtasks for phases\nTaskCreate(subject=\"Pre-flight checks\", activeForm=\"Running pre-flight checks\")\nTaskCreate(subject=\"Run parallel validation agents\", activeForm=\"Validating with agents\")\nTaskCreate(subject=\"Run local tests\", activeForm=\"Running local tests\")\nTaskCreate(subject=\"Create PR on GitHub\", activeForm=\"Creating GitHub PR\")\n\n# 3. Update status as you progress\nTaskUpdate(taskId=\"2\", status=\"in_progress\")  # When starting phase\nTaskUpdate(taskId=\"2\", status=\"completed\")    # When phase done\n```\n\n---\n\n## Workflow\n\n### Phase 1: Pre-Flight Checks\n\n```bash\n# Verify branch\nBRANCH=$(git branch --show-current)\nif [[ \"$BRANCH\" == \"dev\" || \"$BRANCH\" == \"main\" ]]; then\n  echo \"Cannot create PR from dev/main. Create a feature branch first.\"\n  exit 1\nfi\n\n# Check for uncommitted changes\nif [[ -n $(git status --porcelain) ]]; then\n  echo \"Uncommitted changes detected. Commit or stash first.\"\n  exit 1\nfi\n\n# Push branch if needed\ngit fetch origin\nif ! git rev-parse --verify origin/$BRANCH &>/dev/null; then\n  git push -u origin $BRANCH\nfi\n```\n\n### Phase 2: Parallel Pre-PR Validation (3 Agents)\n\nLaunch validation agents in ONE message BEFORE creating PR:\n\n```python\n# PARALLEL - All 3 in ONE message\nTask(\n  subagent_type=\"security-auditor\",\n  prompt=\"\"\"Security audit for PR changes:\n  1. Check for secrets/credentials in diff\n  2. Dependency vulnerabilities (npm audit/pip-audit)\n  3. OWASP Top 10 quick scan\n  Return: {status: PASS/BLOCK, issues: [...]}\n\n  Scope: ONLY read files directly relevant to the PR diff. Do NOT explore the entire codebase.\n\n  SUMMARY: End with: \"RESULT: [PASS|WARN|BLOCK] - [N] issues: [brief list or 'clean']\"\n  \"\"\",\n  run_in_background=True,\n  max_turns=25\n)\n",
    "contentTruncated": true
  },
  "data-visualization": {
    "content": "# Data Visualization\n\nPatterns for building dashboards, widget grids, and Recharts 3.x chart components in React. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Dashboard](#dashboard) | 3 | HIGH | Grid layouts, widget composition, filter/search patterns |\n| [Charts](#charts) | 3 | HIGH | Line/bar/pie charts, responsive containers, custom tooltips |\n\n**Total: 6 rules across 2 categories**\n\n## Quick Start\n\n```tsx\n// Dashboard grid with stat cards\nfunction DashboardGrid() {\n  return (\n    <div className=\"grid gap-4 grid-cols-1 sm:grid-cols-2 lg:grid-cols-4\">\n      <StatCard title=\"Revenue\" value=\"$45,231\" change=\"+12%\" />\n      <StatCard title=\"Users\" value=\"2,350\" change=\"+5.2%\" />\n      <div className=\"col-span-full\"><RevenueChart /></div>\n    </div>\n  );\n}\n```\n\n```tsx\n// Recharts responsive line chart\nfunction RevenueChart({ data }: { data: ChartData[] }) {\n  return (\n    <ResponsiveContainer width=\"100%\" height={400}>\n      <LineChart data={data}>\n        <CartesianGrid strokeDasharray=\"3 3\" />\n        <XAxis dataKey=\"date\" />\n        <YAxis />\n        <Tooltip content={<CustomTooltip />} />\n        <Line type=\"monotone\" dataKey=\"revenue\" stroke=\"#8884d8\" strokeWidth={2} />\n      </LineChart>\n    </ResponsiveContainer>\n  );\n}\n```\n\n## Dashboard\n\nResponsive grid layouts, widget composition, and real-time data patterns for admin panels and analytics dashboards.\n\n### Key Patterns\n\n- **CSS Grid layouts** with responsive breakpoints (`grid-cols-1 sm:grid-cols-2 lg:grid-cols-4`)\n- **Widget registry** pattern for dynamic dashboard composition\n- **TanStack Query + SSE** for real-time metric updates\n- **TanStack Table** for sortable, filterable data tables with pagination\n- **Skeleton loading** states for content areas\n\n### Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Layout | CSS Grid for 2D dashboard layouts |\n| Real-time | SSE for server->client, WebSocket for bidirectional |\n| Data table | TanStack Table for features |\n| State | TanStack Query with granular keys |\n| Loading | Skeleton for content areas |\n\n## Charts\n\nRecharts 3.x data visualization with responsive containers, custom tooltips, accessibility, and performance optimization.\n\n### Key Patterns\n\n- **ResponsiveContainer** wrapping all charts (always set parent height)\n- **Custom tooltips** for branded UX\n- **Accessibility** with `figure` role and `aria-label`\n- **Real-time charts** with `isAnimationActive={false}` and `dot={false}`\n- **Performance** limiting data points and memoizing calculations\n\n### Chart Types\n\n| Chart | Component | Best For |\n|-------|-----------|----------|\n| Line | `LineChart` | Trends over time |\n| Bar | `BarChart` | Comparisons |\n| Pie/Donut | `PieChart` with `innerRadius` | Proportions |\n| Area | `AreaChart` with gradient | Volume over time |\n\n### Key Decisions\n\n| Decision | Recommendation |\n|----------|",
    "contentTruncated": true
  },
  "database-patterns": {
    "content": "# Database Patterns\n\nComprehensive patterns for database migrations, schema design, and version management. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Alembic Migrations](#alembic-migrations) | 3 | CRITICAL | Autogenerate, data migrations, branch management |\n| [Schema Design](#schema-design) | 3 | HIGH | Normalization, indexing strategies, NoSQL patterns |\n| [Versioning](#versioning) | 3 | HIGH | Changelogs, rollback plans, schema drift detection |\n\n**Total: 9 rules across 3 categories**\n\n## Quick Start\n\n```python\n# Alembic: Auto-generate migration from model changes\n# alembic revision --autogenerate -m \"add user preferences\"\n\ndef upgrade() -> None:\n    op.add_column('users', sa.Column('org_id', UUID(as_uuid=True), nullable=True))\n    op.execute(\"UPDATE users SET org_id = 'default-org-uuid' WHERE org_id IS NULL\")\n\ndef downgrade() -> None:\n    op.drop_column('users', 'org_id')\n```\n\n```sql\n-- Schema: Normalization to 3NF with proper indexing\nCREATE TABLE orders (\n    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    customer_id UUID NOT NULL REFERENCES customers(id),\n    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()\n);\nCREATE INDEX idx_orders_customer_id ON orders(customer_id);\n```\n\n## Alembic Migrations\n\nMigration management with Alembic for SQLAlchemy 2.0 async applications.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Autogenerate | `rules/alembic-autogenerate.md` | Auto-generate from models, async env.py, review workflow |\n| Data Migration | `rules/alembic-data-migration.md` | Batch backfill, two-phase NOT NULL, zero-downtime |\n| Branching | `rules/alembic-branching.md` | Feature branches, merge migrations, conflict resolution |\n\n## Schema Design\n\nSQL and NoSQL schema design with normalization, indexing, and constraint patterns.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Normalization | `rules/schema-normalization.md` | 1NF-3NF, when to denormalize, JSON vs normalized |\n| Indexing | `rules/schema-indexing.md` | B-tree, GIN, HNSW, partial/covering indexes |\n| NoSQL Patterns | `rules/schema-nosql.md` | Embed vs reference, document design, sharding |\n\n## Versioning\n\nDatabase version control and change management across environments.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Changelog | `rules/versioning-changelog.md` | Schema version table, semantic versioning, audit trails |\n| Rollback | `rules/versioning-rollback.md` | Rollback testing, destructive rollback docs, CI verification |\n| Drift Detection | `rules/versioning-drift.md` | Environment sync, checksum verification, migration locks |\n\n## Key Decisions\n\n| Decision | Recommendation | Rationale |\n|----------|----------------|-----------|\n| Async dialect | `postgresql+asyncpg` | Native async support for SQLAlchemy 2.0 |\n| NOT NULL column | Two-phase: nullable first, then alter | Avoids locking, back",
    "contentTruncated": true
  },
  "demo-producer": {
    "content": "# Demo Producer\n\nUniversal demo video creation for any content type.\n\n## Quick Start\n\n```bash\n/demo-producer                    # Interactive mode - asks what to create\n/demo-producer skill explore      # Create demo for a skill\n/demo-producer plugin ork     # Create demo for a plugin\n/demo-producer tutorial \"Building a REST API\"  # Custom tutorial\n```\n\n## Supported Content Types\n\n| Type | Source | Example |\n|------|--------|---------|\n| `skill` | skills/{name}/SKILL.md | `/demo-producer skill commit` |\n| `agent` | agents/{name}.md | `/demo-producer agent debug-investigator` |\n| `plugin` | plugins/{name}/plugin.json | `/demo-producer plugin ork` |\n| `marketplace` | Marketplace install flow | `/demo-producer marketplace ork` |\n| `tutorial` | Custom description | `/demo-producer tutorial \"Git workflow\"` |\n| `cli` | Any CLI tool | `/demo-producer cli \"npm create vite\"` |\n| `code` | Code walkthrough | `/demo-producer code src/api/auth.ts` |\n\n## Interactive Flow\n\nWhen invoked without arguments, asks:\n\n### Question 1: Content Type\n```\nWhat type of demo do you want to create?\n\n‚óã Skill - OrchestKit skill showcase\n‚óã Agent - AI agent demonstration\n‚óã Plugin - Plugin installation/features\n‚óã Tutorial - Custom coding tutorial\n‚óã CLI Tool - Command-line tool demo\n‚óã Code Walkthrough - Explain existing code\n```\n\n### Question 2: Format\n```\nWhat format(s) do you need?\n\n‚òë Horizontal (16:9) - YouTube, Twitter\n‚òë Vertical (9:16) - TikTok, Reels, Shorts\n‚òê Square (1:1) - Instagram, LinkedIn\n```\n\n### Question 3: Style\n```\nWhat style fits your content?\n\n‚óã Quick Demo (6-10s) - Fast showcase, single feature\n‚óã Standard Demo (15-25s) - Full workflow, multiple steps\n‚óã Tutorial (30-60s) - Detailed explanation, code examples\n‚óã Cinematic (60s+) - Story-driven, high polish\n‚óã Scrapbook (15-35s) - Warm paper, fast cuts, social proof collage (Anthropic style)\n```\n\n### Question 4: Audio\n```\nAudio preferences?\n\n‚óã Music Only - Subtle ambient background\n‚óã Music + SFX - Background + success sounds\n‚óã Silent - No audio\n```\n\n## Pipeline Architecture\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                     Demo Producer Pipeline                        ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                                                   ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ   Content   ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Content    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Script Generator  ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ   Detector  ‚îÇ    ‚îÇ   Analyzer   ‚îÇ    ‚îÇ   (per type)        ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ\n‚îÇ                                                     ‚îÇ             ‚îÇ\n‚îÇ                                                     ‚ñº             ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ\n‚îÇ  ‚îÇ  Remotion   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ    VHS       ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ   Terminal Script   ‚îÇ  ‚îÇ\n‚îÇ  ‚îÇ  Composer   ‚îÇ    ‚îÇ   Recorder   ‚îÇ    ‚îÇ   (.sh + .tape)     ‚îÇ  ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ",
    "contentTruncated": true
  },
  "design-system-starter": {
    "content": "# Design System Starter\n\n## Overview\n\nThis skill provides comprehensive guidance for building robust, scalable design systems that ensure visual consistency, improve development velocity, and create exceptional user experiences.\n\n**When to use this skill:**\n- Creating a new design system from scratch\n- Evolving or refactoring existing design systems\n- Establishing design token standards\n- Defining component architecture\n- Creating design documentation\n- Ensuring accessibility compliance (WCAG 2.1)\n- Implementing theming and dark mode\n\n**Bundled Resources:**\n- `references/design-tokens.md` - Complete token definitions\n- `references/component-patterns.md` - Architecture patterns\n- `references/component-examples.md` - Full component implementations\n- `references/theming.md` - Theme and dark mode patterns\n- `assets/design-tokens-template.json` - W3C design token format\n- `assets/component-template.tsx` - React component template\n\n```typescript\n// Example: Design token structure\nconst tokens = {\n  colors: {\n    primary: { base: \"#0066cc\", hover: \"#0052a3\" },\n    semantic: { success: \"#28a745\", error: \"#dc3545\" }\n  },\n  spacing: { xs: \"4px\", sm: \"8px\", md: \"16px\", lg: \"24px\" }\n};\n```\n- `checklists/design-system-checklist.md` - Design system audit checklist\n\n---\n\n## Design System Philosophy\n\nA design system is more than a component library. It includes:\n\n| Layer | Description | Examples |\n|-------|-------------|----------|\n| **Design Tokens** | Foundational design decisions | Colors, spacing, typography |\n| **Components** | Reusable UI building blocks | Button, Input, Card, Modal |\n| **Patterns** | Common UX solutions | Forms, Navigation, Layouts |\n| **Guidelines** | Rules and best practices | Accessibility, naming, APIs |\n| **Documentation** | How to use everything | Storybook, usage examples |\n\n### Core Principles\n\n1. **Consistency Over Creativity** - Predictable patterns reduce cognitive load\n2. **Accessible by Default** - WCAG 2.1 Level AA compliance minimum\n3. **Scalable and Maintainable** - Design tokens enable global changes\n4. **Developer-Friendly** - Clear API contracts and documentation\n\n---\n\n## References\n\n### Design Tokens\n**See: `references/design-tokens.md`**\n\nKey topics covered:\n- Color scales (primitive 50-950, semantic tokens)\n- Typography system (font families, sizes, weights, line heights)\n- Spacing scale (4px base system)\n- Border radius and shadow tokens\n- W3C design token format\n- Tailwind `@theme` integration\n\n**Quick Reference - Token Categories:**\n\n| Category | Examples | Scale |\n|----------|----------|-------|\n| Colors | `blue.500`, `text.primary`, `feedback.error` | 50-950 |\n| Typography | `fontSize.base`, `fontWeight.semibold` | xs-5xl |\n| Spacing | `spacing.4`, `spacing.8` | 0-24 (4px base) |\n| Border Radius | `borderRadius.md`, `borderRadius.full` | none-full |\n| Shadows | `shadow.sm`, `shadow.lg` | xs-xl |\n\n---\n\n### Component Patterns\n**See: `references/component-patterns.md`**\n\nKey topics covered:\n- Atomic Design methodol",
    "contentTruncated": true
  },
  "devops-deployment": {
    "content": "# DevOps & Deployment Skill\n\nComprehensive frameworks for CI/CD pipelines, containerization, deployment strategies, and infrastructure automation.\n\n## Overview\n\n- Setting up CI/CD pipelines\n- Containerizing applications\n- Deploying to Kubernetes or cloud platforms\n- Implementing GitOps workflows\n- Managing infrastructure as code\n- Planning release strategies\n\n## Pipeline Architecture\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ    Code     ‚îÇ‚îÄ‚îÄ>‚îÇ    Build    ‚îÇ‚îÄ‚îÄ>‚îÇ    Test     ‚îÇ‚îÄ‚îÄ>‚îÇ   Deploy    ‚îÇ\n‚îÇ   Commit    ‚îÇ   ‚îÇ   & Lint    ‚îÇ   ‚îÇ   & Scan    ‚îÇ   ‚îÇ  & Release  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n       ‚îÇ                 ‚îÇ                 ‚îÇ                 ‚îÇ\n       v                 v                 v                 v\n   Triggers         Artifacts          Reports          Monitoring\n```\n\n## Key Concepts\n\n### CI/CD Pipeline Stages\n\n1. **Lint & Type Check** - Code quality gates\n2. **Unit Tests** - Test coverage with reporting\n3. **Security Scan** - npm audit + Trivy vulnerability scanner\n4. **Build & Push** - Docker image to container registry\n5. **Deploy Staging** - Environment-gated deployment\n6. **Deploy Production** - Manual approval or automated\n\n### Container Best Practices\n\n**Multi-stage builds** minimize image size:\n- Stage 1: Install production dependencies only\n- Stage 2: Build application with dev dependencies\n- Stage 3: Production runtime with minimal footprint\n\n**Security hardening**:\n- Non-root user (uid 1001)\n- Read-only filesystem where possible\n- Health checks for orchestrator integration\n\n### Kubernetes Deployment\n\n**Essential manifests**:\n- Deployment with rolling update strategy\n- Service for internal routing\n- Ingress for external access with TLS\n- HorizontalPodAutoscaler for scaling\n\n**Security context**:\n- `runAsNonRoot: true`\n- `allowPrivilegeEscalation: false`\n- `readOnlyRootFilesystem: true`\n- Drop all capabilities\n\n### Deployment Strategies\n\n| Strategy | Use Case | Risk |\n|----------|----------|------|\n| **Rolling** | Default, gradual replacement | Low - automatic rollback |\n| **Blue-Green** | Instant switch, easy rollback | Medium - double resources |\n| **Canary** | Progressive traffic shift | Low - gradual exposure |\n\n**Rolling Update** (Kubernetes default):\n```yaml\nstrategy:\n  type: RollingUpdate\n  rollingUpdate:\n    maxSurge: 25%\n    maxUnavailable: 0  # Zero downtime\n```\n\n### Secrets Management\n\nUse External Secrets Operator to sync from cloud providers:\n- AWS Secrets Manager\n- HashiCorp Vault\n- Azure Key Vault\n- GCP Secret Manager\n\n---\n\n## References\n\n### Docker Patterns\n**See: `references/docker-patterns.md`**\n\nKey topics covered:\n- Multi-stage build examples with 78% size reduction\n- Layer caching optimization\n- Security hardening (non-root, health checks)\n- Trivy vulnerability scanning\n- Docker Compose development setup\n\n### CI/CD Pipelines\n**See: `references/ci-cd-pipelines.md`**\n\nKey topics covered:\n- Branch strategy (Git Flow)\n- GitHub Acti",
    "contentTruncated": true
  },
  "distributed-systems": {
    "content": "# Distributed Systems Patterns\n\nComprehensive patterns for building reliable distributed systems. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Distributed Locks](#distributed-locks) | 3 | CRITICAL | Redis/Redlock locks, PostgreSQL advisory locks, fencing tokens |\n| [Resilience](#resilience) | 3 | CRITICAL | Circuit breakers, retry with backoff, bulkhead isolation |\n| [Idempotency](#idempotency) | 3 | HIGH | Idempotency keys, request dedup, database-backed idempotency |\n| [Rate Limiting](#rate-limiting) | 3 | HIGH | Token bucket, sliding window, distributed rate limits |\n\n**Total: 12 rules across 4 categories**\n\n## Quick Start\n\n```python\n# Redis distributed lock with Lua scripts\nasync with RedisLock(redis_client, \"payment:order-123\"):\n    await process_payment(order_id)\n\n# Circuit breaker for external APIs\n@circuit_breaker(failure_threshold=5, recovery_timeout=30)\n@retry(max_attempts=3, base_delay=1.0)\nasync def call_external_api():\n    ...\n\n# Idempotent API endpoint\n@router.post(\"/payments\")\nasync def create_payment(\n    data: PaymentCreate,\n    idempotency_key: str = Header(..., alias=\"Idempotency-Key\"),\n):\n    return await idempotent_execute(db, idempotency_key, \"/payments\", process)\n\n# Token bucket rate limiting\nlimiter = TokenBucketLimiter(redis_client, capacity=100, refill_rate=10)\nif await limiter.is_allowed(f\"user:{user_id}\"):\n    await handle_request()\n```\n\n## Distributed Locks\n\nCoordinate exclusive access to resources across multiple service instances.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Redis & Redlock | `rules/locks-redis-redlock.md` | Lua scripts, SET NX, multi-node quorum |\n| PostgreSQL Advisory | `rules/locks-postgres-advisory.md` | Session/transaction locks, lock ID strategies |\n| Fencing Tokens | `rules/locks-fencing-tokens.md` | Owner validation, TTL, heartbeat extension |\n\n## Resilience\n\nProduction-grade fault tolerance for distributed systems.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Circuit Breaker | `rules/resilience-circuit-breaker.md` | CLOSED/OPEN/HALF_OPEN states, sliding window |\n| Retry & Backoff | `rules/resilience-retry-backoff.md` | Exponential backoff, jitter, error classification |\n| Bulkhead Isolation | `rules/resilience-bulkhead.md` | Semaphore tiers, rejection policies, queue depth |\n\n## Idempotency\n\nEnsure operations can be safely retried without unintended side effects.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Idempotency Keys | `rules/idempotency-keys.md` | Deterministic hashing, Stripe-style headers |\n| Request Dedup | `rules/idempotency-dedup.md` | Event consumer dedup, Redis + DB dual layer |\n| Database-Backed | `rules/idempotency-database.md` | Unique constraints, upsert, TTL cleanup |\n\n## Rate Limiting\n\nProtect APIs with distributed rate limiting using Redis.\n\n| Rule | File | Key Pattern |\n|------|--",
    "contentTruncated": true
  },
  "doctor": {
    "content": "# OrchestKit Health Diagnostics\n\n## Overview\n\nThe `/ork:doctor` command performs comprehensive health checks on your OrchestKit installation. It auto-detects installed plugins and validates 10 categories:\n\n1. **Installed Plugins** - Detects orkl or ork\n2. **Skills Validation** - Frontmatter, references, token budget (dynamic count)\n3. **Agents Validation** - Frontmatter, tool refs, skill refs (dynamic count)\n4. **Hook Health** - Registration, bundles, async patterns\n5. **Permission Rules** - Detects unreachable rules (CC 2.1.3 feature)\n6. **Schema Compliance** - Validates JSON files against schemas\n7. **Coordination System** - Checks lock health and registry integrity\n8. **Context Budget** - Monitors token usage against budget\n9. **Memory System** - Graph memory health\n10. **Claude Code Version** - Validates CC >= 2.1.16\n\n## When to Use\n\n- After installing or updating OrchestKit\n- When hooks aren't firing as expected\n- Before deploying to a team environment\n- When debugging coordination issues\n- After running `npm run build`\n\n## Quick Start\n\n```bash\n/ork:doctor           # Standard health check\n/ork:doctor -v        # Verbose output\n/ork:doctor --json    # Machine-readable for CI\n```\n\n## CLI Options\n\n| Flag | Description |\n|------|-------------|\n| `-v`, `--verbose` | Detailed output per check |\n| `--json` | JSON output for CI integration |\n| `--category=X` | Run only specific category |\n\n## Health Check Categories\n\n### 0. Installed Plugins Detection\n\nAuto-detects which OrchestKit plugins are installed:\n\n```bash\n# Detection logic:\n# - Scans for .claude-plugin/plugin.json in plugin paths\n# - Identifies orkl or ork\n# - Counts skills/agents per installed plugin\n```\n\n**Output (orkl):**\n```\nInstalled Plugins: 1\n- orkl: 88 skills, 36 agents, 119 hook entries\n```\n\n**Output (ork full):**\n```\nInstalled Plugins: 1\n- ork: 200 skills, 36 agents, 98 hook entries\n```\n\n### 1. Skills Validation\n\nValidates skills in installed plugins (count varies by installation):\n\n```bash\n# Checks performed:\n# - SKILL.md frontmatter (name, description, user-invocable)\n# - context: fork field (required for CC 2.1.0+)\n# - Token budget compliance (300-5000 tokens)\n# - Internal link validation (references/ paths)\n# - Related Skills references exist\n```\n\n**Output (full ork):**\n```\nSkills: 186/186 valid\n- User-invocable: 24 commands\n- Reference skills: 163\n```\n\n**Output (orkl only):**\n```\nSkills: 18/18 valid\n- User-invocable: 0 commands\n- Reference skills: 18\n```\n\n### 2. Agents Validation\n\nValidates agents in installed plugins:\n\n```bash\n# Checks performed:\n# - Frontmatter fields (name, description, model, tools, skills)\n# - Model validation (opus, sonnet, haiku only)\n# - Skills references exist in src/skills/\n# - Tools are valid CC tools\n```\n\n**Output:**\n```\nAgents: 35/35 valid\n- Models: 12 sonnet, 15 haiku, 8 opus\n- All skill references valid\n```\n\n### 3. Hook Health\n\nVerifies hooks are properly configured:\n\n```bash\n# Checks performed:\n# - hooks.json schema valid\n# - Bundle files exis",
    "contentTruncated": true
  },
  "domain-driven-design": {
    "content": "# Domain-Driven Design Tactical Patterns\n\nModel complex business domains with entities, value objects, and bounded contexts.\n\n## Overview\n\n- Modeling complex business logic\n- Separating domain from infrastructure\n- Establishing clear boundaries between subdomains\n- Building rich domain models with behavior\n- Implementing ubiquitous language in code\n\n## Building Blocks Overview\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                    DDD Building Blocks                       ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  ENTITIES           VALUE OBJECTS        AGGREGATES         ‚îÇ\n‚îÇ  Order (has ID)     Money (no ID)        [Order]‚ÜíItems      ‚îÇ\n‚îÇ                                                              ‚îÇ\n‚îÇ  DOMAIN SERVICES    REPOSITORIES         DOMAIN EVENTS      ‚îÇ\n‚îÇ  PricingService     IOrderRepository     OrderSubmitted     ‚îÇ\n‚îÇ                                                              ‚îÇ\n‚îÇ  FACTORIES          SPECIFICATIONS       MODULES            ‚îÇ\n‚îÇ  OrderFactory       OverdueOrderSpec     orders/, payments/ ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Quick Reference\n\n### Entity (Has Identity)\n\n```python\nfrom dataclasses import dataclass, field\nfrom uuid import UUID\nfrom uuid_utils import uuid7\n\n@dataclass\nclass Order:\n    \"\"\"Entity: Has identity, mutable state, lifecycle.\"\"\"\n    id: UUID = field(default_factory=uuid7)\n    customer_id: UUID = field(default=None)\n    status: str = \"draft\"\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Order):\n            return NotImplemented\n        return self.id == other.id  # Identity equality\n\n    def __hash__(self) -> int:\n        return hash(self.id)\n```\n\nSee [entities-value-objects.md](references/entities-value-objects.md) for complete patterns.\n\n### Value Object (Immutable)\n\n```python\nfrom dataclasses import dataclass\nfrom decimal import Decimal\n\n@dataclass(frozen=True)  # MUST be frozen!\nclass Money:\n    \"\"\"Value Object: Defined by attributes, not identity.\"\"\"\n    amount: Decimal\n    currency: str\n\n    def __add__(self, other: \"Money\") -> \"Money\":\n        if self.currency != other.currency:\n            raise ValueError(\"Cannot add different currencies\")\n        return Money(self.amount + other.amount, self.currency)\n```\n\nSee [entities-value-objects.md](references/entities-value-objects.md) for Address, DateRange examples.\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Entity vs VO | Has unique ID + lifecycle? Entity. Otherwise VO |\n| Entity equality | By ID, not attributes |\n| Value object mutability | Always immutable (`frozen=True`) |\n| Repository scope | One per aggregate root |\n| Domain events | Collect in entity, publish after persist |\n| Context boundaries | By business capability, not technical |\n\n## Anti-Patterns (FORBIDDEN)\n\n```python\n# NEVER have anemic domain models (data-only classes)\n@dataclass\nclass Order:\n    id: UUID\n    items: l",
    "contentTruncated": true
  },
  "edge-computing-patterns": {
    "content": "# Edge Computing Patterns\n\n## Overview\n\nEdge computing runs code closer to users worldwide, reducing latency from seconds to milliseconds. This skill covers Cloudflare Workers, Vercel Edge Functions, and Deno Deploy patterns for building globally distributed applications.\n\n**When to use this skill:**\n- Global applications requiring <50ms latency\n- Authentication/authorization at the edge\n- A/B testing and feature flags\n- Geo-routing and localization\n- API rate limiting and DDoS protection\n- Transforming responses (image optimization, HTML rewriting)\n\n## Platform Comparison\n\n| Feature | Cloudflare Workers | Vercel Edge | Deno Deploy |\n|---------|-------------------|-------------|-------------|\n| Cold Start | <1ms | <10ms | <10ms |\n| Locations | 300+ | 100+ | 35+ |\n| Runtime | V8 Isolates | V8 Isolates | Deno |\n| Max Duration | 30s (paid: unlimited) | 25s | 50ms-5min |\n| Free Tier | 100k req/day | 100k req/month | 100k req/month |\n\n## Platform-Specific Implementation\n\nFor detailed code examples and patterns, load the appropriate reference file:\n\n### Cloudflare Workers\n**Reference:** `references/cloudflare-workers.md`\n- Worker fetch handlers and routing\n- KV storage patterns (eventually consistent)\n- Durable Objects for stateful edge\n- Wrangler CLI and wrangler.toml configuration\n- Caching strategies with Cache API\n\n### Vercel Edge Functions\n**Reference:** `references/vercel-edge.md`\n- Edge Middleware for Next.js (auth, A/B testing, geo-routing)\n- Edge API routes with streaming\n- Edge Config for feature flags\n- Geolocation-based routing patterns\n\n### Runtime Differences\n**Reference:** `references/runtime-differences.md`\n- Node.js APIs NOT available at edge\n- Web API compatibility matrix\n- Polyfill strategies for crypto, Buffer, streams\n\n## Edge Runtime Constraints\n\n**Available APIs:**\n- fetch, Request, Response, Headers\n- URL, URLSearchParams\n- TextEncoder, TextDecoder\n- ReadableStream, WritableStream\n- crypto, SubtleCrypto (Web Crypto API)\n- Web APIs (atob, btoa, setTimeout, etc.)\n\n**NOT Available:**\n- Node.js APIs (fs, path, child_process)\n- Native modules and binary dependencies\n- File system access\n- Some npm packages with Node.js dependencies\n\n## Common Patterns Summary\n\n### Authentication at Edge\nVerify JWT tokens at edge for sub-millisecond auth checks. See `references/cloudflare-workers.md` for implementation.\n\n### Rate Limiting\nUse KV (Cloudflare) or Edge Config (Vercel) for distributed rate limiting. Pattern: IP-based key with TTL expiration.\n\n### Edge Caching\nCache API with cache-aside pattern. Check cache first, fetch origin on miss, store with TTL.\n\n### A/B Testing\nAssign users to buckets via cookie, rewrite URLs to variant pages. See `references/vercel-edge.md` for middleware pattern.\n\n### Geo-Routing\nAccess request.cf.country (Cloudflare) or request.geo (Vercel) for location-based routing.\n\n## Best Practices\n\n- Keep bundles small (<1MB compressed)\n- Use streaming for large responses to avoid timeouts\n- Leverage platform caching (KV, D",
    "contentTruncated": true
  },
  "errors": {
    "content": "# Error Pattern Analysis\n\nAnalyze errors captured from Claude Code sessions to identify patterns and get actionable insights.\n\n## Quick Start\n\n```bash\n/errors              # Batch analysis of historical error patterns\n/debug               # CC 2.1.30 real-time debug for current session\n```\n\n### When to Use Which\n\n| Command | Purpose | Scope |\n|---------|---------|-------|\n| `/errors` | Batch analysis of error patterns (last 24h/7d) | Historical patterns |\n| `/debug` | Real-time debug of current session state | Current session |\n| `/ork:fix-issue` | Full RCA workflow for specific bug | Single issue |\n\n## Quick Analysis\n\n```bash\n# Run batch analysis on last 24h of errors\npython .claude/scripts/analyze_errors.py\n\n# Analyze last 7 days\npython .claude/scripts/analyze_errors.py --days 7\n\n# Generate markdown report\npython .claude/scripts/analyze_errors.py --report\n```\n\n## What Gets Captured\n\nThe error collector hook captures:\n- Tool name (Bash, mcp__memory__search_nodes, etc.)\n- Error message (first 500 chars)\n- Tool input (command/query that failed)\n- Timestamp and session ID\n\n**Location:** `.claude/logs/errors.jsonl`\n\n## Current Error Rules\n\nCheck learned patterns that trigger warnings:\n\n```bash\ncat .claude/rules/error_rules.json | jq '.rules[] | {id, signature, count: .occurrence_count}'\n```\n\n## Files\n\n| File | Purpose |\n|------|---------|\n| `.claude/hooks/posttool/error-collector.sh` | Captures errors to JSONL |\n| `.claude/hooks/pretool/bash/error-pattern-warner.sh` | Warns before risky commands |\n| `.claude/scripts/analyze_errors.py` | Batch pattern analysis |\n| `.claude/rules/error_rules.json` | Learned error patterns |\n| `.claude/logs/errors.jsonl` | Raw error log |\n\n## Common Patterns\n\n### PostgreSQL Connection Errors\n\n```\npattern: role \"X\" does not exist\nfix: Use Docker connection: docker exec -it orchestkit-postgres-dev psql -U orchestkit_user -d orchestkit_dev\n\npattern: relation \"X\" does not exist\nfix: Check MCP postgres server connection string - may be connected to wrong database\n```\n\n## Related Skills\n- fix-issue: Fix identified errors\n- debug-investigator: Debug error root causes\n## Adding New Rules\n\nRules are auto-generated by `analyze_errors.py` when patterns repeat 2+ times.\nFor manual rules, edit `.claude/rules/error_rules.json`:\n\n```json\n{\n  \"id\": \"custom-001\",\n  \"pattern\": \"your regex pattern\",\n  \"signature\": \"human readable signature\",\n  \"tool\": \"Bash\",\n  \"occurrence_count\": 1,\n  \"fix_suggestion\": \"How to fix this\"\n}\n```",
    "contentTruncated": false
  },
  "event-driven": {
    "content": "# Event-Driven Architecture\n\nComprehensive event-driven patterns. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Event Sourcing](#event-sourcing) | 3 | HIGH | Event stores, append-only logs, projections, snapshots |\n| [CQRS](#cqrs) | 3 | HIGH | Command/query separation, read models, materialized views |\n| [Saga Patterns](#saga-patterns) | 3 | HIGH | Distributed transactions, orchestration vs choreography, compensation |\n| [Outbox Pattern](#outbox-pattern) | 2 | HIGH | Reliable publishing, CDC, Debezium, dual-write prevention |\n| [Message Queues](#message-queues) | 3 | MEDIUM | Kafka, RabbitMQ, Redis Streams, topic design, DLQ |\n\n**Total: 14 rules across 5 categories**\n\n## Quick Start\n\n```python\n# Event-sourced aggregate with optimistic concurrency\nclass Account:\n    def __init__(self):\n        self._changes, self._version, self.balance = [], 0, 0.0\n\n    def deposit(self, amount: float):\n        self._raise_event(MoneyDeposited(aggregate_id=self.id, amount=amount, version=self._version + 1))\n\n    def _apply(self, event):\n        match event:\n            case MoneyDeposited(): self.balance += event.amount\n            case MoneyWithdrawn(): self.balance -= event.amount\n```\n\n```python\n# CQRS command dispatch with event publishing\nasync def dispatch(self, command: Command) -> list[DomainEvent]:\n    handler = self._handlers.get(type(command))\n    events = await handler.handle(command)\n    for event in events:\n        await self.event_publisher.publish(event)\n    return events\n```\n\n## Event Sourcing\n\nStore application state as immutable events rather than current state snapshots.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Event Store | `rules/sourcing-event-store.md` | Append-only store, optimistic concurrency, versioning |\n| Aggregate Pattern | `rules/sourcing-aggregates.md` | Event-sourced aggregates, apply/raise, history loading |\n| Projections & Snapshots | `rules/sourcing-projections.md` | Read model projections, snapshot strategies, upcasting |\n\n## CQRS\n\nSeparate read and write concerns for optimized data access.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Command Side | `rules/cqrs-commands.md` | Command bus, handlers, write model aggregates |\n| Read Models | `rules/cqrs-read-models.md` | Query handlers, denormalized views, projections |\n| API Integration | `rules/cqrs-api-integration.md` | FastAPI endpoints, command/query dispatch, consistency |\n\n## Saga Patterns\n\nMaintain consistency across microservices without distributed locks.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Orchestration | `rules/saga-orchestration.md` | Central orchestrator, step execution, compensation |\n| Choreography | `rules/saga-choreography.md` | Event-driven flows, distributed handlers |\n| Recovery & Idempotency | `rules/saga-recovery.md` | Timeout handling, retry, idempotent step",
    "contentTruncated": true
  },
  "evidence-verification": {
    "content": "# Evidence Verification\n\nEnsures all claims are backed by executable proof: test results, coverage metrics, build success, and deployment verification.\n\n**Key Principle:** Show, don't tell. No task is complete without verifiable evidence.\n\n## Overview\n\n### Auto-Activate Triggers\n- Completing code implementation\n- Finishing code review\n- Marking tasks complete in Squad mode\n- Before agent handoff\n- Production deployment verification\n\n### Manual Activation\n- When user requests \"verify this works\"\n- Before creating pull requests\n- During quality assurance reviews\n- When troubleshooting failures\n\n---\n\n# Evidence Verification\n\n## Core Concepts\n\n### 1. Evidence Types\n\n**Test Evidence**\n- Exit code (must be 0 for success)\n- Test suite results (passed/failed/skipped)\n- Coverage percentage (if available)\n- Test duration\n\n**Build Evidence**\n- Build exit code (0 = success)\n- Compilation errors/warnings\n- Build artifacts created\n- Build duration\n\n**Deployment Evidence**\n- Deployment status (success/failed)\n- Environment deployed to\n- Health check results\n- Rollback capability verified\n\n**Code Quality Evidence**\n- Linter results (errors/warnings)\n- Type checker results\n- Security scan results\n- Accessibility audit results\n\n### 2. Evidence Collection Protocol\n\n```markdown\n## Evidence Collection Steps\n\n1. **Identify Verification Points**\n   - What needs to be proven?\n   - What could go wrong?\n   - What does \"complete\" mean?\n\n2. **Execute Verification**\n   - Run tests\n   - Run build\n   - Run linters\n   - Check deployments\n\n3. **Capture Results**\n   - Record exit codes\n   - Save output snippets\n   - Note timestamps\n   - Document environment\n\n4. **Store Evidence**\n   - Add to shared context\n   - Reference in task completion\n   - Link to artifacts\n```\n\n### 3. Verification Standards\n\n**Minimum Evidence Requirements:**\n- ‚úÖ At least ONE verification type executed\n- ‚úÖ Exit code captured (0 = pass, non-zero = fail)\n- ‚úÖ Timestamp recorded\n- ‚úÖ Evidence stored in context\n\n**Production-Grade Requirements:**\n- ‚úÖ Tests run with exit code 0\n- ‚úÖ Coverage >70% (or project standard)\n- ‚úÖ Build succeeds with exit code 0\n- ‚úÖ No critical linter errors\n- ‚úÖ Security scan passes\n\n---\n\n# Evidence Verification\n\n## Evidence Collection Templates\n\n### Template 1: Test Evidence\n\nUse this template when running tests:\n\n```markdown\n## Test Evidence\n\n**Command:** `npm test` (or equivalent)\n**Exit Code:** 0 ‚úÖ / non-zero ‚ùå\n**Duration:** X seconds\n**Results:**\n- Tests passed: X\n- Tests failed: X\n- Tests skipped: X\n- Coverage: X%\n\n**Output Snippet:**\n```\n[First 10 lines of test output]\n```\n\n**Timestamp:** YYYY-MM-DD HH:MM:SS\n**Environment:** Node vX.X.X, OS, etc.\n```\n\n### Template 2: Build Evidence\n\nUse this template when building:\n\n```markdown\n## Build Evidence\n\n**Command:** `npm run build` (or equivalent)\n**Exit Code:** 0 ‚úÖ / non-zero ‚ùå\n**Duration:** X seconds\n**Artifacts Created:**\n- dist/bundle.js (245 KB)\n- dist/styles.css (18 KB)\n\n**Errors:** X\n**Warnings:** X\n\n**Output Snippet:**\n```\n[First 10",
    "contentTruncated": true
  },
  "explore": {
    "content": "# Codebase Exploration\n\nMulti-angle codebase exploration using 3-5 parallel agents.\n\n## Quick Start\n\n```bash\n/explore authentication\n```\n\n> **Opus 4.6**: Exploration agents use native adaptive thinking for deeper pattern recognition across large codebases.\n\n---\n\n## STEP 0: Verify User Intent with AskUserQuestion\n\n**BEFORE creating tasks**, clarify what the user wants to explore:\n\n```python\nAskUserQuestion(\n  questions=[{\n    \"question\": \"What aspect do you want to explore?\",\n    \"header\": \"Focus\",\n    \"options\": [\n      {\"label\": \"Full exploration (Recommended)\", \"description\": \"Code structure + data flow + architecture + health assessment\"},\n      {\"label\": \"Code structure only\", \"description\": \"Find files, classes, functions related to topic\"},\n      {\"label\": \"Data flow\", \"description\": \"Trace how data moves through the system\"},\n      {\"label\": \"Architecture patterns\", \"description\": \"Identify design patterns and integrations\"},\n      {\"label\": \"Quick search\", \"description\": \"Just find relevant files, skip deep analysis\"}\n    ],\n    \"multiSelect\": false\n  }]\n)\n```\n\n**Based on answer, adjust workflow:**\n- **Full exploration**: All 8 phases, all parallel agents\n- **Code structure only**: Skip phases 4-6 (health, dependencies, product)\n- **Data flow**: Focus phase 3 agents on data tracing\n- **Architecture patterns**: Focus on backend-system-architect agent\n- **Quick search**: Skip to phase 1-2 only, return file list\n\n---\n\n## STEP 0b: Select Orchestration Mode\n\nChoose **Agent Teams** (mesh ‚Äî explorers share discoveries) or **Task tool** (star ‚Äî all report to lead):\n\n1. `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1` ‚Üí **Agent Teams mode**\n2. Agent Teams unavailable ‚Üí **Task tool mode** (default)\n3. Otherwise: Full exploration with 4+ agents ‚Üí recommend **Agent Teams**; Quick search or single-focus ‚Üí **Task tool**\n\n| Aspect | Task Tool | Agent Teams |\n|--------|-----------|-------------|\n| Discovery sharing | Lead synthesizes after all complete | Explorers share discoveries as they go |\n| Cross-referencing | Lead connects dots | Data flow explorer alerts architecture explorer |\n| Cost | ~150K tokens | ~400K tokens |\n| Best for | Quick/focused searches | Deep full-codebase exploration |\n\n> **Fallback:** If Agent Teams encounters issues, fall back to Task tool for remaining exploration.\n\n---\n\n## ‚ö†Ô∏è CRITICAL: Task Management is MANDATORY (CC 2.1.16)\n\n**BEFORE doing ANYTHING else, create tasks to show progress:**\n\n```python\n# 1. Create main exploration task IMMEDIATELY\nTaskCreate(\n  subject=\"Explore: {topic}\",\n  description=\"Deep codebase exploration for {topic}\",\n  activeForm=\"Exploring {topic}\"\n)\n\n# 2. Create subtasks for phases (8-phase process)\nTaskCreate(subject=\"Initial file search\", activeForm=\"Searching files\")\nTaskCreate(subject=\"Check knowledge graph\", activeForm=\"Checking memory\")\nTaskCreate(subject=\"Launch exploration agents\", activeForm=\"Dispatching explorers\")\nTaskCreate(subject=\"Assess code health (0-10)\", activeForm=\"Assessing code health\")\nT",
    "contentTruncated": true
  },
  "feedback": {
    "content": "# Feedback - Manage Learning System\n\nView and manage the OrchestKit feedback system that learns from your usage.\n\n## Overview\n\n- Checking feedback system status\n- Pausing/resuming learning\n- Resetting learned patterns\n- Exporting feedback data\n- Managing privacy settings\n- Enabling/disabling anonymous analytics sharing\n- Viewing privacy policy\n\n## Usage\n\n```\n/feedback                    # Same as status\n/feedback status             # Show current state\n/feedback pause              # Pause learning\n/feedback resume             # Resume learning\n/feedback reset              # Clear learned patterns\n/feedback export             # Export feedback data\n/feedback settings           # Show/edit settings\n/feedback opt-in             # Enable anonymous sharing\n/feedback opt-out            # Disable anonymous sharing\n/feedback privacy            # View privacy policy\n/feedback export-analytics   # Export anonymous analytics for review\n```\n\n## Subcommands\n\n### status (default)\n\nShow the current feedback system state.\n\n**Output:**\n```\nFeedback System Status\n-----------------------------\nLearning: Enabled\nAnonymous sharing: Disabled\nData retention: 90 days\n\nLearned Patterns:\n- Auto-approves: npm install, npm test, git push (3 commands)\n- Code style: async/await preferred, TypeScript strict mode\n\nAgent Performance:\n- backend-architect: 94% success (28 spawns) [improving]\n- test-generator: 72% success (18 spawns) [declining]\n\nContext Savings: ~8k tokens/session (estimated)\n\nStorage: .claude/feedback/ (45 KB)\n```\n\n### pause\n\nTemporarily pause all learning without clearing data.\n\n**Action:**\n1. Set `pausedUntil` in preferences to a far future date\n2. Confirm to user\n\n**Output:**\n```\nFeedback learning paused\n\nYour existing patterns are preserved.\nResume with: /feedback resume\n```\n\n### resume\n\nResume paused learning.\n\n**Action:**\n1. Clear `pausedUntil` in preferences\n2. Confirm to user\n\n**Output:**\n```\nFeedback learning resumed\n\nThe system will continue learning from your usage.\n```\n\n### reset\n\nClear all learned patterns (requires confirmation).\n\n**Action:**\n1. Show what will be deleted\n2. Ask for confirmation (user must type \"RESET\")\n3. If confirmed, clear patterns file but keep preferences\n\n**Output (before confirmation):**\n```\nWARNING: This will clear all learned patterns:\n\n- 5 auto-approve permission rules\n- 3 code style preferences\n- Agent performance history\n\nYour preferences (enabled, sharing, retention) will be kept.\n\nTo confirm, respond with exactly: RESET\nTo cancel, respond with anything else.\n```\n\n**Output (after confirmation):**\n```\nFeedback data reset\n\n- Cleared 5 permission patterns\n- Cleared 3 style preferences\n- Cleared agent metrics\n\nLearning will start fresh from now.\n```\n\n### export\n\nExport all feedback data to a JSON file.\n\n**Action:**\n1. Read all feedback files\n2. Combine into single export\n3. Write to `.claude/feedback/export-{date}.json`\n\n**Output:**\n```\nExported feedback data to:\n   .claude/feedback/export-2026-01-14.json\n\nContains:\n- 5 lear",
    "contentTruncated": true
  },
  "fix-issue": {
    "content": "# Fix Issue\n\nSystematic issue resolution with hypothesis-based root cause analysis, similar issue detection, and prevention recommendations.\n\n## Quick Start\n\n```bash\n/fix-issue 123\n/fix-issue 456\n```\n\n> **Opus 4.6**: Root cause analysis uses native adaptive thinking. Dynamic token budgets scale with context window for thorough investigation.\n\n---\n\n## STEP 0: Verify User Intent with AskUserQuestion\n\n**BEFORE creating tasks**, clarify fix approach:\n\n```python\nAskUserQuestion(\n  questions=[{\n    \"question\": \"What approach for this fix?\",\n    \"header\": \"Approach\",\n    \"options\": [\n      {\"label\": \"Proper fix (Recommended)\", \"description\": \"Full RCA, tests, prevention recommendations\"},\n      {\"label\": \"Quick fix\", \"description\": \"Minimal fix to resolve the immediate issue\"},\n      {\"label\": \"Investigate first\", \"description\": \"Understand the issue before deciding on approach\"},\n      {\"label\": \"Hotfix\", \"description\": \"Emergency patch, minimal testing\"}\n    ],\n    \"multiSelect\": false\n  }]\n)\n```\n\n**Based on answer, adjust workflow:**\n- **Proper fix**: All 11 phases, parallel agents for RCA\n- **Quick fix**: Skip phases 8-10 (prevention, runbook, lessons)\n- **Investigate first**: Only phases 1-4 (understand, search, hypotheses, analyze)\n- **Hotfix**: Minimal phases, skip similar issue search\n\n---\n\n## STEP 0b: Select Orchestration Mode\n\nChoose **Agent Teams** (mesh ‚Äî RCA agents share hypotheses) or **Task tool** (star ‚Äî all report to lead):\n\n1. `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1` ‚Üí **Agent Teams mode**\n2. Agent Teams unavailable ‚Üí **Task tool mode** (default)\n3. Otherwise: Complex cross-cutting bugs (backend + frontend + tests involved) ‚Üí recommend **Agent Teams**; Focused bugs (single domain) ‚Üí **Task tool**\n\n| Aspect | Task Tool | Agent Teams |\n|--------|-----------|-------------|\n| Hypothesis sharing | Lead relays between agents | Investigators share hypotheses in real-time |\n| Conflicting evidence | Lead resolves | Investigators debate directly |\n| Cost | ~250K tokens | ~600K tokens |\n| Best for | Single-domain bugs | Cross-cutting bugs with multiple hypotheses |\n\n> **Fallback:** If Agent Teams encounters issues, fall back to Task tool for remaining investigation.\n\n---\n\n## Task Management (CC 2.1.16)\n\n```python\n# Create main fix task\nTaskCreate(\n  subject=\"Fix issue #{number}\",\n  description=\"Systematic issue resolution with hypothesis-based RCA\",\n  activeForm=\"Fixing issue #{number}\"\n)\n\n# Create subtasks for 11-phase process\nphases = [\"Understand issue\", \"Search similar issues\", \"Form hypotheses\",\n          \"Analyze root cause\", \"Design fix\", \"Implement fix\", \"Validate fix\",\n          \"Generate prevention\", \"Create runbook\", \"Capture lessons\", \"Commit and PR\"]\nfor phase in phases:\n    TaskCreate(subject=phase, activeForm=f\"{phase}ing\")\n```\n\n---\n\n## Workflow Overview\n\n| Phase | Activities | Output |\n|-------|------------|--------|\n| **1. Understand Issue** | Read GitHub issue details | Problem statement |\n| **2. Similar Issue Detection** | Sear",
    "contentTruncated": true
  },
  "form-state-patterns": {
    "content": "# Form State Patterns\n\nProduction form patterns with React Hook Form v7 + Zod - type-safe, performant, accessible.\n\n## Overview\n\n- Complex forms with validation\n- Multi-step wizards\n- Dynamic field arrays\n- Server-side validation\n- Async field validation\n- Forms with file uploads\n\n## Core Patterns\n\n### 1. Basic Form with Zod Schema\n\n```typescript\nimport { useForm } from 'react-hook-form';\nimport { zodResolver } from '@hookform/resolvers/zod';\nimport { z } from 'zod';\n\nconst userSchema = z.object({\n  email: z.string().email('Invalid email'),\n  password: z.string().min(8, 'Min 8 characters'),\n  confirmPassword: z.string(),\n}).refine((data) => data.password === data.confirmPassword, {\n  message: \"Passwords don't match\",\n  path: ['confirmPassword'],\n});\n\ntype UserForm = z.infer<typeof userSchema>;\n\nfunction SignupForm() {\n  const {\n    register,\n    handleSubmit,\n    formState: { errors, isSubmitting },\n  } = useForm<UserForm>({\n    resolver: zodResolver(userSchema),\n    defaultValues: { email: '', password: '', confirmPassword: '' },\n  });\n\n  const onSubmit = async (data: UserForm) => {\n    await api.signup(data);\n  };\n\n  return (\n    <form onSubmit={handleSubmit(onSubmit)}>\n      <input {...register('email')} aria-invalid={!!errors.email} />\n      {errors.email && <span role=\"alert\">{errors.email.message}</span>}\n\n      <input type=\"password\" {...register('password')} />\n      {errors.password && <span role=\"alert\">{errors.password.message}</span>}\n\n      <input type=\"password\" {...register('confirmPassword')} />\n      {errors.confirmPassword && <span role=\"alert\">{errors.confirmPassword.message}</span>}\n\n      <button type=\"submit\" disabled={isSubmitting}>\n        {isSubmitting ? 'Submitting...' : 'Sign Up'}\n      </button>\n    </form>\n  );\n}\n```\n\n### 2. Field Arrays (Dynamic Fields)\n\n```typescript\nimport { useFieldArray, useForm } from 'react-hook-form';\n\nconst orderSchema = z.object({\n  items: z.array(z.object({\n    productId: z.string().min(1),\n    quantity: z.number().min(1).max(100),\n  })).min(1, 'At least one item required'),\n});\n\nfunction OrderForm() {\n  const { control, register, handleSubmit } = useForm({\n    resolver: zodResolver(orderSchema),\n    defaultValues: { items: [{ productId: '', quantity: 1 }] },\n  });\n\n  const { fields, append, remove } = useFieldArray({\n    control,\n    name: 'items',\n  });\n\n  return (\n    <form onSubmit={handleSubmit(onSubmit)}>\n      {fields.map((field, index) => (\n        <div key={field.id}>\n          <input {...register(`items.${index}.productId`)} />\n          <input\n            type=\"number\"\n            {...register(`items.${index}.quantity`, { valueAsNumber: true })}\n          />\n          <button type=\"button\" onClick={() => remove(index)}>Remove</button>\n        </div>\n      ))}\n      <button type=\"button\" onClick={() => append({ productId: '', quantity: 1 })}>\n        Add Item\n      </button>\n      <button type=\"submit\">Submit Order</button>\n    </form>\n  );\n}\n```\n\n### 3. Async Field Validation\n\n``",
    "contentTruncated": true
  },
  "frontend-animation": {
    "content": "# Frontend Animation\n\nComprehensive patterns for building performant, accessible animations in frontend applications. Covers three complementary technologies: Motion (Framer Motion) for React component animations, CSS Scroll-Driven Animations for scroll-linked effects, and the View Transitions API for page navigation transitions. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Motion Library](#motion-library) | 3 | HIGH | Layout animations, gesture interactions, exit animations |\n| [Scroll-Driven Animations](#scroll-driven-animations) | 3 | MEDIUM | Scroll progress, parallax, scroll-triggered reveals |\n| [View Transitions](#view-transitions) | 3 | MEDIUM | Page transitions, shared element morphs, cross-document navigation |\n\n**Total: 9 rules across 3 categories**\n\n## Quick Start\n\n```tsx\n// Motion: Staggered list with exit animations\nimport { motion, AnimatePresence } from 'motion/react';\nimport { staggerContainer, staggerItem } from '@/lib/animations';\n\n<AnimatePresence mode=\"wait\">\n  <motion.ul variants={staggerContainer} initial=\"initial\" animate=\"animate\">\n    {items.map(item => (\n      <motion.li key={item.id} variants={staggerItem}>{item.name}</motion.li>\n    ))}\n  </motion.ul>\n</AnimatePresence>\n```\n\n```css\n/* Scroll-Driven: Reading progress bar */\n.progress-bar {\n  position: fixed; top: 0; left: 0; height: 4px;\n  background: var(--color-primary);\n  transform-origin: left;\n  animation: grow-progress linear;\n  animation-timeline: scroll(root block);\n}\n@keyframes grow-progress {\n  from { transform: scaleX(0); }\n  to { transform: scaleX(1); }\n}\n```\n\n```tsx\n// View Transitions: React Router shared element\nimport { Link, useViewTransitionState } from 'react-router';\n\nfunction ProductCard({ product }) {\n  const isTransitioning = useViewTransitionState(`/products/${product.id}`);\n  return (\n    <Link to={`/products/${product.id}`} viewTransition>\n      <img src={product.image} style={{\n        viewTransitionName: isTransitioning ? 'product-image' : undefined,\n      }} />\n    </Link>\n  );\n}\n```\n\n## Motion Library\n\nReact component animations using Motion (Framer Motion) with centralized presets, AnimatePresence for exit animations, and spring physics.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Layout Animations | `rules/motion-layout.md` | AnimatePresence, page transitions, modal enter/exit, stagger lists |\n| Gesture Interactions | `rules/motion-gestures.md` | Hover lift, tap scale, card hover, micro-interactions |\n| Exit Animations | `rules/motion-exit.md` | AnimatePresence modes, collapse/expand, toast slide-in |\n\n## Scroll-Driven Animations\n\nCSS Scroll-Driven Animations API for performant, declarative scroll-linked effects without JavaScript. Chrome 115+, Edge 115+, Safari 18.4+.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Scroll Timeline | `rules/scroll-timeline.md` | scroll(",
    "contentTruncated": true
  },
  "git-recovery": {
    "content": "# Git Recovery\n\nInteractive recovery from common git mistakes. Safe operations with verification steps.\n\n## Quick Start\n\n```bash\n/git-recovery\n```\n\n## Recovery Scenarios\n\nWhen invoked, present these options to the user:\n\n### Option 1: Undo Last Commit (Keep Changes)\n\n**Scenario**: You committed but want to modify the changes before recommitting.\n\n```bash\n# Check current state first\ngit log --oneline -3\ngit status\n\n# Undo commit, keep changes staged\ngit reset --soft HEAD~1\n\n# Verify\ngit status  # Changes should be staged\ngit log --oneline -1  # Previous commit is now HEAD\n```\n\n**Safety**: Non-destructive. All changes remain staged.\n\n---\n\n### Option 2: Undo Last Commit (Discard Changes)\n\n**Scenario**: You committed something completely wrong and want to throw it away.\n\n**WARNING: DESTRUCTIVE - Changes will be lost!**\n\n```bash\n# CRITICAL: First, save a backup reference\nBACKUP_REF=$(git rev-parse HEAD)\necho \"Backup ref: $BACKUP_REF (save this to recover if needed)\"\n\n# Show what will be lost\ngit show HEAD --stat\n\n# Confirm with user before proceeding\n# Then execute:\ngit reset --hard HEAD~1\n\n# Verify\ngit log --oneline -3\ngit status  # Should be clean\n```\n\n**Recovery**: If you made a mistake, run `git reset --hard $BACKUP_REF`\n\n---\n\n### Option 3: Recover Deleted Branch\n\n**Scenario**: You deleted a branch and need it back.\n\n```bash\n# Find the branch's last commit in reflog\ngit reflog | grep -i \"branch-name\"\n# Or search all recent activity:\ngit reflog --all | head -30\n\n# Once you find the commit hash (e.g., abc1234):\ngit checkout -b recovered-branch abc1234\n\n# Verify\ngit log --oneline -5\ngit branch -v | grep recovered\n```\n\n**Note**: Reflog keeps entries for ~90 days by default.\n\n---\n\n### Option 4: Reset File to Last Commit\n\n**Scenario**: You modified a file and want to discard local changes.\n\n**WARNING: DESTRUCTIVE - Uncommitted changes to file will be lost!**\n\n```bash\n# Show current changes to file\ngit diff path/to/file\n\n# Confirm with user before proceeding\n# Then restore:\ngit checkout HEAD -- path/to/file\n\n# Or using newer git restore (Git 2.23+):\ngit restore path/to/file\n\n# Verify\ngit status path/to/file  # Should show no changes\ngit diff path/to/file    # Should be empty\n```\n\n---\n\n### Option 5: Undo a Rebase\n\n**Scenario**: A rebase went wrong and you want to return to pre-rebase state.\n\n```bash\n# Find the pre-rebase state\ngit reflog | head -20\n# Look for entry like: \"rebase (start): checkout main\"\n# The entry BEFORE that is your pre-rebase state\n\n# Alternative: ORIG_HEAD is set automatically before rebase\ngit log --oneline ORIG_HEAD -3\n\n# Reset to pre-rebase state\ngit reset --hard ORIG_HEAD\n\n# Verify\ngit log --oneline -5\ngit status\n```\n\n**Safety**: ORIG_HEAD is overwritten by other operations, use reflog if ORIG_HEAD is stale.\n\n---\n\n### Option 6: Undo a Merge\n\n**Scenario**: You merged a branch and want to undo it.\n\n```bash\n# If merge commit exists and NOT pushed:\ngit log --oneline -5  # Find the merge commit\n\n# Reset to before merge\ngit reset --hard ",
    "contentTruncated": true
  },
  "git-workflow": {
    "content": "# Git Workflow\n\nComplete git workflow patterns: GitHub Flow branching, atomic commits, and recovery operations. Essential for maintaining clean, reviewable history.\n\n## Branch Naming Convention\n\n```bash\n# Feature branches (link to issue)\nissue/<number>-<brief-description>\nissue/123-add-user-auth\n\n# When no issue exists\nfeature/<description>\nfix/<description>\nhotfix/<description>\n```\n\n**Branch Rules:**\n1. `main` is always deployable\n2. Branch from `main`, PR back to `main`\n3. Branches live < 1-3 days\n4. Delete branch after merge\n\n---\n\n## Atomic Commit Checklist\n\n```\n[ ] Does ONE logical thing\n[ ] Leaves codebase working (tests pass)\n[ ] Message doesn't need \"and\" in title\n[ ] Can be reverted independently\n[ ] Title < 50 chars, body wraps at 72\n```\n\n### Interactive Staging\n\n```bash\n# Stage changes hunk-by-hunk\ngit add -p\n\n# Options:\n# y - stage this hunk\n# n - skip this hunk\n# s - split into smaller hunks\n# e - manually edit the hunk\n# q - quit\n\n# Review what's staged\ngit diff --staged    # What will be committed\ngit diff             # What won't be committed\n```\n\n### Commit Patterns\n\n```bash\n# Separate concerns\ngit add -p && git commit -m \"refactor: Extract database pool\"\ngit add -p && git commit -m \"feat(#456): Add query caching\"\n\n# Never combine unrelated changes\n# BAD:  \"feat: Add auth and fix formatting\"\n# GOOD: Two separate commits\n```\n\n---\n\n## Recovery Quick Reference\n\n### The Safety Net\n\n```bash\n# ALWAYS check reflog first - it has everything\ngit reflog\n\n# Shows ALL recent HEAD movements\n# Even \"deleted\" commits live here for 90 days\n```\n\n### Common Recovery Scenarios\n\n| Scenario | Not Pushed | Already Pushed |\n|----------|------------|----------------|\n| Undo commit | `git reset --soft HEAD~1` | `git revert HEAD` |\n| Wrong branch | cherry-pick + reset | cherry-pick + revert |\n| Lost commits | `git reset --hard HEAD@{N}` | N/A |\n| Bad rebase | `git rebase --abort` or reflog | reflog + force-with-lease |\n\n### Quick Recovery Commands\n\n```bash\n# Undo last commit, keep changes staged\ngit reset --soft HEAD~1\n\n# Find lost commits\ngit reflog | grep \"your message\"\n\n# Recover to previous state\ngit reset --hard HEAD@{1}\n\n# Safe force push (feature branches only)\ngit push --force-with-lease\n```\n\n---\n\n## Standard Workflow\n\n```bash\n# 1. Start fresh\ngit checkout main && git pull origin main\ngit checkout -b issue/123-my-feature\n\n# 2. Work with atomic commits\ngit add -p\ngit commit -m \"feat(#123): Add User model\"\n\n# 3. Stay updated\ngit fetch origin && git rebase origin/main\n\n# 4. Push and PR\ngit push -u origin issue/123-my-feature\ngh pr create --fill\n\n# 5. Cleanup after merge\ngit checkout main && git pull\ngit branch -d issue/123-my-feature\n```\n\n---\n\n## Anti-Patterns\n\n```\nAvoid:\n- Long-lived branches (> 1 week)\n- Merging main into feature (use rebase)\n- Direct commits to main\n- Force push to shared branches\n- Commits that need \"and\" in message\n- Committing broken code\n```\n\n---\n\n## Best Practices Summary\n\n1. **Branch from main** - Always start fresh\n2. **Stag",
    "contentTruncated": true
  },
  "github-operations": {
    "content": "# GitHub Operations\n\nComprehensive GitHub CLI (`gh`) operations for project management, from basic issue creation to advanced Projects v2 integration and milestone tracking via REST API.\n\n## Overview\n\n- Creating and managing GitHub issues and PRs\n- Working with GitHub Projects v2 custom fields\n- Managing milestones (sprints, releases) via REST API\n- Automating bulk operations with `gh`\n- Running GraphQL queries for complex operations\n\n---\n\n## Quick Reference\n\n### Issue Operations\n\n```bash\n# Create issue with labels and milestone\ngh issue create --title \"Bug: API returns 500\" --body \"...\" --label \"bug\" --milestone \"Sprint 5\"\n\n# List and filter issues\ngh issue list --state open --label \"backend\" --assignee @me\n\n# Edit issue metadata\ngh issue edit 123 --add-label \"high\" --milestone \"v2.0\"\n```\n\n### PR Operations\n\n```bash\n# Create PR with reviewers\ngh pr create --title \"feat: Add search\" --body \"...\" --base dev --reviewer @teammate\n\n# Watch CI status and auto-merge\ngh pr checks 456 --watch\ngh pr merge 456 --auto --squash --delete-branch\n\n# Resume a session linked to a PR (CC 2.1.27)\nclaude --from-pr 456           # Resume session with PR context (diff, comments, review status)\nclaude --from-pr https://github.com/org/repo/pull/456\n```\n\n> **Tip (CC 2.1.27):** Sessions created via `gh pr create` are automatically linked to the PR. Use `--from-pr` to resume with full PR context.\n\n### Milestone Operations (REST API)\n\n```bash\n# List milestones with progress\ngh api repos/:owner/:repo/milestones --jq '.[] | \"\\(.title): \\(.closed_issues)/\\(.open_issues + .closed_issues)\"'\n\n# Create milestone with due date\ngh api -X POST repos/:owner/:repo/milestones \\\n  -f title=\"Sprint 8\" -f due_on=\"2026-02-15T00:00:00Z\"\n\n# Close milestone\ngh api -X PATCH repos/:owner/:repo/milestones/5 -f state=closed\n```\n\n### Projects v2 Operations\n\n```bash\n# Add issue to project\ngh project item-add 1 --owner @me --url https://github.com/org/repo/issues/123\n\n# Set custom field (requires GraphQL)\ngh api graphql -f query='mutation {...}' -f projectId=\"...\" -f itemId=\"...\"\n```\n\n---\n\n## JSON Output Patterns\n\n```bash\n# Get issue numbers matching criteria\ngh issue list --json number,labels --jq '[.[] | select(.labels[].name == \"bug\")] | .[].number'\n\n# PR summary with author\ngh pr list --json number,title,author --jq '.[] | \"\\(.number): \\(.title) by \\(.author.login)\"'\n\n# Find ready-to-merge PRs\ngh pr list --json number,reviewDecision,statusCheckRollupState \\\n  --jq '[.[] | select(.reviewDecision == \"APPROVED\" and .statusCheckRollupState == \"SUCCESS\")]'\n```\n\n---\n\n## Key Concepts\n\n### Milestone vs Epic\n\n| Milestones | Epics |\n|------------|-------|\n| Time-based (sprints, releases) | Topic-based (features) |\n| Has due date | No due date |\n| Progress bar | Task list checkbox |\n| Native REST API | Needs workarounds |\n\n**Rule**: Use milestones for \"when\", use parent issues for \"what\".\n\n### Projects v2 Custom Fields\n\nProjects v2 uses GraphQL for setting custom fields (Status, Priority, Domain). Basic `gh",
    "contentTruncated": true
  },
  "golden-dataset": {
    "content": "# Golden Dataset\n\nComprehensive patterns for building, managing, and validating golden datasets for AI/ML evaluation. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Curation](#curation) | 3 | HIGH | Content collection, annotation pipelines, diversity analysis |\n| [Management](#management) | 3 | HIGH | Versioning, backup/restore, CI/CD automation |\n| [Validation](#validation) | 3 | CRITICAL | Quality scoring, drift detection, regression testing |\n\n**Total: 9 rules across 3 categories**\n\n## Curation\n\nContent collection, multi-agent annotation, and diversity analysis for golden datasets.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Collection | `rules/curation-collection.md` | Content type classification, quality thresholds, duplicate prevention |\n| Annotation | `rules/curation-annotation.md` | Multi-agent pipeline, consensus aggregation, Langfuse tracing |\n| Diversity | `rules/curation-diversity.md` | Difficulty stratification, domain coverage, balance guidelines |\n\n## Management\n\nVersioning, storage, and CI/CD automation for golden datasets.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Versioning | `rules/management-versioning.md` | JSON backup format, embedding regeneration, disaster recovery |\n| Storage | `rules/management-storage.md` | Backup strategies, URL contract, data integrity checks |\n| CI Integration | `rules/management-ci.md` | GitHub Actions automation, pre-deployment validation, weekly backups |\n\n## Validation\n\nQuality scoring, drift detection, and regression testing for golden datasets.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Quality | `rules/validation-quality.md` | Schema validation, content quality, referential integrity |\n| Drift | `rules/validation-drift.md` | Duplicate detection, semantic similarity, coverage gap analysis |\n| Regression | `rules/validation-regression.md` | Difficulty distribution, pre-commit hooks, full dataset validation |\n\n## Quick Start Example\n\n```python\nfrom app.shared.services.embeddings import embed_text\n\nasync def validate_before_add(document: dict, source_url_map: dict) -> dict:\n    \"\"\"Pre-addition validation for golden dataset entries.\"\"\"\n    errors = []\n\n    # 1. URL contract check\n    if \"placeholder\" in document.get(\"source_url\", \"\"):\n        errors.append(\"URL must be canonical, not a placeholder\")\n\n    # 2. Content quality\n    if len(document.get(\"title\", \"\")) < 10:\n        errors.append(\"Title too short (min 10 chars)\")\n\n    # 3. Tag requirements\n    if len(document.get(\"tags\", [])) < 2:\n        errors.append(\"At least 2 domain tags required\")\n\n    return {\"valid\": len(errors) == 0, \"errors\": errors}\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Backup format | JSON (version controlled, portable) |\n| Embedding storage | Exclude from backup (regenerate on restore) |\n",
    "contentTruncated": true
  },
  "grpc-python": {
    "content": "# gRPC Python Patterns\n\nHigh-performance RPC framework for microservice communication.\n\n## Overview\n\n- Internal microservice communication (lower latency than REST)\n- Streaming data (real-time updates, file transfers)\n- Polyglot environments (shared proto definitions)\n- Strong typing between services (compile-time validation)\n- Bidirectional streaming (chat, gaming, real-time sync)\n\n## When NOT to Use\n\n- Public APIs (prefer REST/GraphQL for browser compatibility)\n- Simple CRUD with few services (REST is simpler)\n- When HTTP/2 is not available\n\n## Proto Definition\n\n```protobuf\n// protos/user_service.proto\nsyntax = \"proto3\";\npackage user.v1;\n\nimport \"google/protobuf/timestamp.proto\";\nimport \"google/protobuf/empty.proto\";\n\nservice UserService {\n  rpc GetUser(GetUserRequest) returns (User);\n  rpc CreateUser(CreateUserRequest) returns (User);\n  rpc ListUsers(ListUsersRequest) returns (stream User);  // Server streaming\n  rpc BulkCreateUsers(stream CreateUserRequest) returns (BulkCreateResponse);  // Client streaming\n  rpc UserUpdates(stream UserUpdateRequest) returns (stream User);  // Bidirectional\n}\n\nmessage User {\n  string id = 1;\n  string email = 2;\n  string name = 3;\n  UserStatus status = 4;\n  google.protobuf.Timestamp created_at = 5;\n}\n\nenum UserStatus {\n  USER_STATUS_UNSPECIFIED = 0;\n  USER_STATUS_ACTIVE = 1;\n  USER_STATUS_INACTIVE = 2;\n}\n\nmessage GetUserRequest { string user_id = 1; }\nmessage CreateUserRequest { string email = 1; string name = 2; string password = 3; }\nmessage ListUsersRequest { int32 page_size = 1; string page_token = 2; }\nmessage BulkCreateResponse { int32 created_count = 1; repeated string user_ids = 2; }\n```\n\n### Code Generation\n\n```bash\npip install grpcio grpcio-tools\npython -m grpc_tools.protoc -I./protos --python_out=./app/protos --pyi_out=./app/protos --grpc_python_out=./app/protos ./protos/user_service.proto\n```\n\n## Server Implementation\n\n```python\nimport grpc\nfrom concurrent import futures\nfrom google.protobuf.timestamp_pb2 import Timestamp\nfrom app.protos import user_service_pb2 as pb2\nfrom app.protos import user_service_pb2_grpc as pb2_grpc\n\nclass UserServiceServicer(pb2_grpc.UserServiceServicer):\n    def __init__(self, user_repo):\n        self.user_repo = user_repo\n\n    def GetUser(self, request, context):\n        user = self.user_repo.get(request.user_id)\n        if not user:\n            context.abort(grpc.StatusCode.NOT_FOUND, f\"User {request.user_id} not found\")\n        return self._to_proto(user)\n\n    def CreateUser(self, request, context):\n        if not request.email or \"@\" not in request.email:\n            context.abort(grpc.StatusCode.INVALID_ARGUMENT, \"Invalid email\")\n        if self.user_repo.get_by_email(request.email):\n            context.abort(grpc.StatusCode.ALREADY_EXISTS, \"Email already registered\")\n        user = self.user_repo.create(email=request.email, name=request.name)\n        return self._to_proto(user)\n\n    def ListUsers(self, request, context):\n        \"\"\"Server streaming: yield users one ",
    "contentTruncated": true
  },
  "help": {
    "content": "# OrchestKit Skill Directory\n\nInteractive guide to all user-invocable skills organized by category.\n\n## Quick Start\n\n```bash\n/ork:help           # Show all categories\n/ork:help build     # Show BUILD skills only\n/ork:help git       # Show GIT skills only\n```\n\n---\n\n## CRITICAL: Use AskUserQuestion for Category Selection\n\nWhen invoked without arguments, present categories interactively:\n\n```python\nAskUserQuestion(\n  questions=[{\n    \"question\": \"What type of task are you working on?\",\n    \"header\": \"Category\",\n    \"options\": [\n      {\"label\": \"BUILD\", \"description\": \"Implement features, brainstorm, verify\"},\n      {\"label\": \"GIT\", \"description\": \"Commits, PRs, issues, recovery\"},\n      {\"label\": \"MEMORY\", \"description\": \"Store decisions, search, sync context\"},\n      {\"label\": \"QUALITY\", \"description\": \"Assess code, health checks, golden datasets\"},\n      {\"label\": \"CONFIG\", \"description\": \"Configure OrchestKit, feedback, skill evolution\"},\n      {\"label\": \"EXPLORE\", \"description\": \"Explore codebase, coordinate worktrees\"},\n      {\"label\": \"MEDIA\", \"description\": \"Create demo videos\"},\n      {\"label\": \"Show all\", \"description\": \"List all 21 skills\"}\n    ],\n    \"multiSelect\": false\n  }]\n)\n```\n\n---\n\n## Skill Categories\n\n### BUILD (3 skills)\n*Implement features and verify changes*\n\n| Skill | Description | Example |\n|-------|-------------|---------|\n| `/ork:implement` | Full-power feature implementation with parallel subagents | `/ork:implement user authentication` |\n| `/ork:brainstorming` | Design exploration with parallel agents | `/ork:brainstorming API design for payments` |\n| `/ork:verify` | Comprehensive verification with parallel test agents | `/ork:verify authentication flow` |\n\n---\n\n### GIT (5 skills)\n*Version control and GitHub operations*\n\n| Skill | Description | Example |\n|-------|-------------|---------|\n| `/ork:commit` | Creates commits with conventional format | `/ork:commit` |\n| `/ork:create-pr` | Create GitHub pull requests with validation | `/ork:create-pr` |\n| `/ork:review-pr` | PR review with parallel specialized agents | `/ork:review-pr 123` |\n| `/ork:fix-issue` | Fix GitHub issues with parallel analysis | `/ork:fix-issue 456` |\n| `/ork:git-recovery` | Recovery from git mistakes | `/ork:git-recovery` |\n\n---\n\n### MEMORY (2 skills)\n*Knowledge persistence and retrieval*\n\n| Skill | Description | Example |\n|-------|-------------|---------|\n| `/ork:remember` | Store decisions and patterns | `/ork:remember We use cursor pagination` |\n| `/ork:memory` | Search, load, sync, history, viz | `/ork:memory search pagination` |\n\n**Subcommands for `/ork:memory`:**\n- `search` - Search decisions and patterns\n- `load` - Load session context\n- `history` - View decision timeline\n- `viz` - Visualize knowledge graph\n\n---\n\n### QUALITY (4 skills)\n*Assessment and diagnostics*\n\n| Skill | Description | Example |\n|-------|-------------|---------|\n| `/ork:assess` | Rate quality 0-10 with pros/cons | `/ork:assess src/api/` |\n| `/ork:assess-complexity` | Assess ta",
    "contentTruncated": true
  },
  "i18n-date-patterns": {
    "content": "# i18n and Localization Patterns\n\n## Overview\n\nThis skill provides comprehensive guidance for implementing internationalization in React applications. It ensures ALL user-facing strings, date displays, currency, lists, and time calculations are locale-aware.\n\n**When to use this skill:**\n- Adding ANY user-facing text to components\n- Formatting dates, times, currency, lists, or ordinals\n- Implementing complex pluralization\n- Embedding React components in translated text\n- Supporting RTL languages (Hebrew, Arabic)\n\n**Bundled Resources:**\n- `references/formatting-utilities.md` - useFormatting hook API reference\n- `references/icu-messageformat.md` - ICU plural/select syntax\n- `references/trans-component.md` - Trans component for rich text\n- `checklists/i18n-checklist.md` - Implementation and review checklist\n- `examples/component-i18n-example.md` - Complete component example\n\n**Canonical Reference:** See `docs/i18n-standards.md` for the full i18n standards document.\n\n---\n\n## Core Patterns\n\n### 1. useTranslation Hook (All UI Strings)\n\nEvery visible string MUST use the translation function:\n\n```tsx\nimport { useTranslation } from 'react-i18next';\n\nfunction MyComponent() {\n  const { t } = useTranslation(['patients', 'common']);\n  \n  return (\n    <div>\n      <h1>{t('patients:title')}</h1>\n      <button>{t('common:actions.save')}</button>\n    </div>\n  );\n}\n```\n\n### 2. useFormatting Hook (Locale-Aware Data)\n\nAll locale-sensitive formatting MUST use the centralized hook:\n\n```tsx\nimport { useFormatting } from '@/hooks';\n\nfunction PriceDisplay({ amount, items }) {\n  const { formatILS, formatList, formatOrdinal } = useFormatting();\n  \n  return (\n    <div>\n      <p>Price: {formatILS(amount)}</p>        {/* ‚Ç™1,500.00 */}\n      <p>Items: {formatList(items)}</p>        {/* \"a, b, and c\" */}\n      <p>Position: {formatOrdinal(3)}</p>      {/* \"3rd\" */}\n    </div>\n  );\n}\n```\n\nSee `references/formatting-utilities.md` for the complete API.\n\n### 3. Date Formatting\n\nAll dates MUST use the centralized `@/lib/dates` library:\n\n```tsx\nimport { formatDate, formatDateShort, calculateWaitTime } from '@/lib/dates';\n\nconst date = formatDate(appointment.date);    // \"Jan 6, 2026\"\nconst waitTime = calculateWaitTime('09:30');  // \"15 min\"\n```\n\n### 4. ICU MessageFormat (Complex Plurals)\n\nUse ICU syntax in translation files for pluralization:\n\n```json\n{\n  \"patients\": \"{count, plural, =0 {No patients} one {# patient} other {# patients}}\"\n}\n```\n\n```tsx\nt('patients', { count: 5 })  // ‚Üí \"5 patients\"\n```\n\nSee `references/icu-messageformat.md` for full syntax.\n\n### 5. Trans Component (Rich Text)\n\nFor embedded React components in translated text:\n\n```tsx\nimport { Trans } from 'react-i18next';\n\n<Trans\n  i18nKey=\"richText.welcome\"\n  values={{ name: userName }}\n  components={{ strong: <strong /> }}\n/>\n```\n\nSee `references/trans-component.md` for patterns.\n\n---\n\n## Translation File Structure\n\n```\nfrontend/src/i18n/locales/\n‚îú‚îÄ‚îÄ en/\n‚îÇ   ‚îú‚îÄ‚îÄ common.json      # Shared: actions, status, time\n‚îÇ   ‚îú‚îÄ‚îÄ p",
    "contentTruncated": true
  },
  "implement": {
    "content": "# Implement Feature\n\nMaximum utilization of parallel subagent execution for feature implementation with built-in scope control and reflection.\n\n## Quick Start\n\n```bash\n/implement user authentication\n/implement real-time notifications\n/implement dashboard analytics\n```\n\n> **Opus 4.6**: Parallel agents leverage native adaptive thinking and 128K output for comprehensive implementations. Token budgets scale dynamically with context window.\n\n---\n\n## STEP 0: Verify User Intent with AskUserQuestion\n\n**BEFORE creating tasks or doing ANY work**, ask the user to clarify scope:\n\n```python\nAskUserQuestion(\n  questions=[\n    {\n      \"question\": \"What scope for this implementation?\",\n      \"header\": \"Scope\",\n      \"options\": [\n        {\"label\": \"Full-stack (Recommended)\", \"description\": \"Backend + frontend + tests + docs\"},\n        {\"label\": \"Backend only\", \"description\": \"API + database + backend tests\"},\n        {\"label\": \"Frontend only\", \"description\": \"UI components + state + frontend tests\"},\n        {\"label\": \"Quick prototype\", \"description\": \"Minimal working version, skip tests\"}\n      ],\n      \"multiSelect\": false\n    },\n    {\n      \"question\": \"Any constraints I should know about?\",\n      \"header\": \"Constraints\",\n      \"options\": [\n        {\"label\": \"None (Recommended)\", \"description\": \"Use best practices and modern patterns\"},\n        {\"label\": \"Match existing patterns\", \"description\": \"Follow existing codebase conventions exactly\"},\n        {\"label\": \"Minimal dependencies\", \"description\": \"Avoid adding new packages\"},\n        {\"label\": \"Specific tech stack\", \"description\": \"I'll specify the technologies to use\"}\n      ],\n      \"multiSelect\": false\n    }\n  ]\n)\n```\n\n**Based on user's answers, adjust the workflow:**\n- **Full-stack**: All 10 phases, all parallel agents\n- **Backend only**: Skip frontend agents (phases 5b, 6b)\n- **Frontend only**: Skip backend agents (phases 5a, 6a)\n- **Quick prototype**: Skip phases 7-10 (scope check, verification, docs, reflection)\n\n---\n\n## STEP 0b: Select Orchestration Mode\n\nChoose **Agent Teams** (mesh, default when available) or **Task tool** (star, fallback):\n- `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1` ‚Üí Agent Teams (default); not set ‚Üí Task tool\n- When Teams available: complexity < 2.5 ‚Üí Task tool; >= 2.5 ‚Üí Agent Teams\n- Override: `ORCHESTKIT_FORCE_TASK_TOOL=1` ‚Üí always Task tool\n\n> See [Orchestration Modes](references/orchestration-modes.md) for decision logic, comparison table, and fallback strategy.\n\n---\n\n## Opus 4.6: 128K Output Token Advantage\n\nWith 128K output tokens (2x previous 64K), agents can generate **complete artifacts in fewer passes**:\n\n| Artifact | Before (64K) | After (128K) |\n|----------|-------------|--------------|\n| Full API + models | 2 passes | 1 pass |\n| Component + tests | 2 passes | 1 pass |\n| Complete feature (API + UI + tests) | 4-6 passes | 2-3 passes |\n\n**Guidance for agents:** Generate complete, working code in a single pass whenever possible. Don't split implementations across multipl",
    "contentTruncated": true
  },
  "issue-progress-tracking": {
    "content": "# Issue Progress Tracking\n\nAutomatic GitHub issue progress tracking that updates issues based on commits and marks sub-tasks as complete.\n\n## Overview\n\n- Working on GitHub issues with checkbox sub-tasks\n- Making commits that reference issue numbers\n- Using issue-prefixed branches (e.g., `issue/123-feature`, `fix/456-bug`)\n- Wanting automatic progress visibility without manual updates\n\n## How It Works\n\n### Automatic Progress Tracking\n\nThe plugin automatically tracks your work through three coordinated hooks:\n\n1. **Commit Detection** (`issue-progress-commenter.sh`)\n   - Extracts issue number from branch name or commit message\n   - Queues commit info for batch commenting\n   - Supports patterns: `issue/123-*`, `fix/123-*`, `feature/123-*`, `#123`\n\n2. **Sub-task Updates** (`issue-subtask-updater.sh`)\n   - Parses commit messages for task completion keywords\n   - Matches against unchecked `- [ ]` items in issue body\n   - Automatically checks off matching tasks via GitHub API\n\n3. **Session Summary** (`issue-work-summary.sh`)\n   - Posts consolidated progress comment when session ends\n   - Includes: commits, files changed, sub-tasks completed, PR link\n\n### Issue Number Extraction\n\n```bash\n# From branch name (priority)\nissue/123-implement-feature  # Extracts: 123\nfix/456-resolve-bug          # Extracts: 456\nfeature/789-add-tests        # Extracts: 789\n123-some-description         # Extracts: 123\n\n# From commit message (fallback)\n\"feat(#123): Add user validation\"     # Extracts: 123\n\"fix: Resolve bug (closes #456)\"      # Extracts: 456\n```\n\n### Sub-task Matching\n\nCommit messages are matched against issue checkboxes using:\n- Normalized text comparison (case-insensitive)\n- Partial matching for task descriptions\n- Keyword detection (Add, Implement, Fix, Test, etc.)\n\n**Example:**\n```markdown\n# Issue body\n- [ ] Add input validation\n- [ ] Write unit tests\n- [ ] Update documentation\n\n# Commit message\n\"feat(#123): Add input validation\"\n\n# Result: First checkbox auto-checked\n- [x] Add input validation\n- [ ] Write unit tests\n- [ ] Update documentation\n```\n\n## Progress Comment Format\n\n```markdown\n## Claude Code Progress Update\n\n**Session**: `abc12345...`\n**Branch**: `issue/123-implement-feature`\n\n### Commits (3)\n- `abc1234`: feat(#123): Add input validation\n- `def5678`: test(#123): Add unit tests\n- `ghi9012`: docs(#123): Update README\n\n### Files Changed\n- `src/validation.ts` (+45, -12)\n- `tests/validation.test.ts` (+89, -0)\n- `README.md` (+5, -2)\n\n### Sub-tasks Completed\n- [x] Add input validation\n- [x] Write unit tests\n\n### Pull Request\nhttps://github.com/owner/repo/pull/42\n\n---\n*Automated by OrchestKit*\n```\n\n## Requirements\n\n- `gh` CLI installed and authenticated\n- Repository with GitHub remote\n- Issue must exist and be accessible\n\n## Configuration\n\nDisable individual hooks in `.claude/config.json`:\n\n```json\n{\n  \"hooks\": {\n    \"issue-progress-commenter.sh\": false,\n    \"issue-subtask-updater.sh\": false,\n    \"issue-work-summary.sh\": false\n  }\n}\n```\n\n## PR-Aware Session",
    "contentTruncated": true
  },
  "langgraph": {
    "content": "# LangGraph Workflow Patterns\n\nComprehensive patterns for building production LangGraph workflows. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [State Management](#state-management) | 4 | CRITICAL | Designing workflow state schemas, accumulators, reducers |\n| [Routing & Branching](#routing--branching) | 3 | HIGH | Dynamic routing, retry loops, semantic routing |\n| [Parallel Execution](#parallel-execution) | 3 | HIGH | Fan-out/fan-in, map-reduce, concurrent agents |\n| [Supervisor Patterns](#supervisor-patterns) | 3 | HIGH | Central coordinators, round-robin, priority dispatch |\n| [Tool Calling](#tool-calling) | 4 | CRITICAL | Binding tools, ToolNode, dynamic selection, approvals |\n| [Checkpointing](#checkpointing) | 3 | HIGH | Persistence, recovery, cross-thread Store memory |\n| [Human-in-Loop](#human-in-loop) | 3 | MEDIUM | Approval gates, feedback loops, interrupt/resume |\n| [Streaming](#streaming) | 3 | MEDIUM | Real-time updates, token streaming, custom events |\n| [Subgraphs](#subgraphs) | 3 | MEDIUM | Modular composition, nested graphs, state mapping |\n| [Functional API](#functional-api) | 3 | MEDIUM | @entrypoint/@task decorators, migration from StateGraph |\n\n**Total: 32 rules across 10 categories**\n\n## State Management\n\nState schemas determine how data flows between nodes. Wrong schemas cause silent data loss.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| TypedDict State | `rules/state-typeddict.md` | `TypedDict` + `Annotated[list, add]` for accumulators |\n| Pydantic Validation | `rules/state-pydantic.md` | `BaseModel` at boundaries, TypedDict internally |\n| MessagesState | `rules/state-messages.md` | `MessagesState` or `add_messages` reducer |\n| Custom Reducers | `rules/state-reducers.md` | `Annotated[T, reducer_fn]` for merge/overwrite |\n\n## Routing & Branching\n\nControl flow between nodes. Always include END fallback to prevent hangs.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Conditional Edges | `rules/routing-conditional.md` | `add_conditional_edges` with explicit mapping |\n| Retry Loops | `rules/routing-retry-loops.md` | Loop-back edges with max retry counter |\n| Semantic Routing | `rules/routing-semantic.md` | Embedding similarity or `Command` API routing |\n\n## Parallel Execution\n\nRun independent nodes concurrently. Use `Annotated[list, add]` to accumulate results.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Fan-Out/Fan-In | `rules/parallel-fanout-fanin.md` | `Send` API for dynamic parallel branches |\n| Map-Reduce | `rules/parallel-map-reduce.md` | `asyncio.gather` + result aggregation |\n| Error Isolation | `rules/parallel-error-isolation.md` | `return_exceptions=True` + per-branch timeout |\n\n## Supervisor Patterns\n\nCentral coordinator routes to specialized workers. Workers return to supervisor.\n\n| Rule | File | Key Pattern |\n|------|------|-----",
    "contentTruncated": true
  },
  "llm-evaluation": {
    "content": "# LLM Evaluation\n\nEvaluate and validate LLM outputs for quality assurance using RAGAS and LLM-as-judge patterns.\n\n## Quick Reference\n\n### LLM-as-Judge Pattern\n\n```python\nasync def evaluate_quality(input_text: str, output_text: str, dimension: str) -> float:\n    response = await llm.chat([{\n        \"role\": \"user\",\n        \"content\": f\"\"\"Evaluate for {dimension}. Score 1-10.\nInput: {input_text[:500]}\nOutput: {output_text[:1000]}\nRespond with just the number.\"\"\"\n    }])\n    return int(response.content.strip()) / 10\n```\n\n### Quality Gate\n\n```python\nQUALITY_THRESHOLD = 0.7\n\nasync def quality_gate(state: dict) -> dict:\n    scores = await full_quality_assessment(state[\"input\"], state[\"output\"])\n    passed = scores[\"average\"] >= QUALITY_THRESHOLD\n    return {**state, \"quality_passed\": passed}\n```\n\n### Hallucination Detection\n\n```python\nasync def detect_hallucination(context: str, output: str) -> dict:\n    # Check if output contains claims not in context\n    return {\"has_hallucinations\": bool, \"unsupported_claims\": []}\n```\n\n## RAGAS Metrics ()\n\n| Metric | Use Case | Threshold |\n|--------|----------|-----------|\n| Faithfulness | RAG grounding | ‚â• 0.8 |\n| Answer Relevancy | Q&A systems | ‚â• 0.7 |\n| Context Precision | Retrieval quality | ‚â• 0.7 |\n| Context Recall | Retrieval completeness | ‚â• 0.7 |\n\n## Anti-Patterns (FORBIDDEN)\n\n```python\n# ‚ùå NEVER use same model as judge and evaluated\noutput = await gpt4.complete(prompt)\nscore = await gpt4.evaluate(output)  # Same model!\n\n# ‚ùå NEVER use single dimension\nif relevance_score > 0.7:  # Only checking one thing\n    return \"pass\"\n\n# ‚ùå NEVER set threshold too high\nTHRESHOLD = 0.95  # Blocks most content\n\n# ‚úÖ ALWAYS use different judge model\nscore = await gpt4_mini.evaluate(claude_output)\n\n# ‚úÖ ALWAYS use multiple dimensions\nscores = await evaluate_all_dimensions(output)\nif scores[\"average\"] > 0.7:\n    return \"pass\"\n```\n\n## Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Judge model | GPT-5.2-mini or Claude Haiku 4.5 |\n| Threshold | 0.7 for production, 0.6 for drafts |\n| Dimensions | 3-5 most relevant to use case |\n| Sample size | 50+ for reliable metrics |\n\n## Detailed Documentation\n\n| Resource | Description |\n|----------|-------------|\n| [references/evaluation-metrics.md](references/evaluation-metrics.md) | RAGAS & LLM-as-judge metrics |\n| [examples/evaluation-patterns.md](examples/evaluation-patterns.md) | Complete evaluation examples |\n| [checklists/evaluation-checklist.md](checklists/evaluation-checklist.md) | Setup and review checklists |\n| [scripts/evaluator-template.py](scripts/evaluator-template.py) | Starter evaluation template |\n\n## Related Skills\n\n- `quality-gates` - Workflow quality control\n- `monitoring-observability` - Tracking evaluation scores\n- `agent-loops` - Self-correcting with evaluation\n\n## Capability Details\n\n### llm-as-judge\n**Keywords:** LLM judge, judge model, evaluation model, grader LLM\n**Solves:**\n- Use LLM to evaluate other LLM outputs\n- Implement judge prompts",
    "contentTruncated": true
  },
  "llm-integration": {
    "content": "# LLM Integration\n\nPatterns for integrating LLMs into production applications: tool use, streaming, local inference, and fine-tuning. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Function Calling](#function-calling) | 3 | CRITICAL | Tool definitions, parallel execution, input validation |\n| [Streaming](#streaming) | 3 | HIGH | SSE endpoints, structured streaming, backpressure handling |\n| [Local Inference](#local-inference) | 3 | HIGH | Ollama setup, model selection, GPU optimization |\n| [Fine-Tuning](#fine-tuning) | 3 | HIGH | LoRA/QLoRA training, dataset preparation, evaluation |\n\n**Total: 12 rules across 4 categories**\n\n## Quick Start\n\n```python\n# Function calling: strict mode tool definition\ntools = [{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"search_documents\",\n        \"description\": \"Search knowledge base\",\n        \"strict\": True,\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n                \"limit\": {\"type\": \"integer\", \"description\": \"Max results\"}\n            },\n            \"required\": [\"query\", \"limit\"],\n            \"additionalProperties\": False\n        }\n    }\n}]\n```\n\n```python\n# Streaming: SSE endpoint with FastAPI\n@app.get(\"/chat/stream\")\nasync def stream_chat(prompt: str):\n    async def generate():\n        async for token in async_stream(prompt):\n            yield {\"event\": \"token\", \"data\": token}\n        yield {\"event\": \"done\", \"data\": \"\"}\n    return EventSourceResponse(generate())\n```\n\n```python\n# Local inference: Ollama with LangChain\nllm = ChatOllama(\n    model=\"deepseek-r1:70b\",\n    base_url=\"http://localhost:11434\",\n    temperature=0.0,\n    num_ctx=32768,\n)\n```\n\n```python\n# Fine-tuning: QLoRA with Unsloth\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Meta-Llama-3.1-8B\",\n    max_seq_length=2048, load_in_4bit=True,\n)\nmodel = FastLanguageModel.get_peft_model(model, r=16, lora_alpha=32)\n```\n\n## Function Calling\n\nEnable LLMs to use external tools and return structured data. Use strict mode schemas (2026 best practice) for reliability. Limit to 5-15 tools per request, validate all inputs with Pydantic/Zod, and return errors as tool results.\n\n- `calling-tool-definition.md` -- Strict mode schemas, OpenAI/Anthropic formats, LangChain binding\n- `calling-parallel.md` -- Parallel tool execution, asyncio.gather, strict mode constraints\n- `calling-validation.md` -- Input validation, error handling, tool execution loops\n\n## Streaming\n\nDeliver LLM responses in real-time for better UX. Use SSE for web, WebSocket for bidirectional. Handle backpressure with bounded queues.\n\n- `streaming-sse.md` -- FastAPI SSE endpoints, frontend consumers, async iterators\n- `streaming-structured.md` -- Streaming with tool calls, partial JSON parsing, chunk accumulation\n-",
    "contentTruncated": true
  },
  "mcp-advanced-patterns": {
    "content": "# MCP Advanced Patterns\n\nAdvanced Model Context Protocol patterns for production-grade MCP implementations.\n\n> **FastMCP 2.14.x** (Jan ): Enterprise auth, OpenAPI/FastAPI generation, server composition, proxying. Python 3.10-3.13.\n\n## Overview\n\n- Composing multiple tools into orchestrated workflows\n- Managing resource lifecycle and caching efficiently\n- Scaling MCP servers horizontally with load balancing\n- Building custom MCP servers with middleware and transports\n- Implementing auto-enable thresholds for context management\n\n## Tool Composition Pattern\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Any, Callable, Awaitable\n\n@dataclass\nclass ComposedTool:\n    \"\"\"Combine multiple tools into a single pipeline operation.\"\"\"\n    name: str\n    tools: dict[str, Callable[..., Awaitable[Any]]]\n    pipeline: list[str]\n\n    async def execute(self, input_data: dict[str, Any]) -> dict[str, Any]:\n        \"\"\"Execute tool pipeline sequentially.\"\"\"\n        result = input_data\n        for tool_name in self.pipeline:\n            tool = self.tools[tool_name]\n            result = await tool(result)\n        return result\n\n# Example: Search + Summarize composition\nsearch_summarize = ComposedTool(\n    name=\"search_and_summarize\",\n    tools={\n        \"search\": search_documents,\n        \"summarize\": summarize_content,\n    },\n    pipeline=[\"search\", \"summarize\"]\n)\n```\n\n## FastMCP Server with Lifecycle\n\n```python\nfrom contextlib import asynccontextmanager\nfrom collections.abc import AsyncIterator\nfrom dataclasses import dataclass\nfrom mcp.server.fastmcp import Context, FastMCP\n\n@dataclass\nclass AppContext:\n    \"\"\"Typed application context with shared resources.\"\"\"\n    db: Database\n    cache: CacheService\n    config: dict\n\n@asynccontextmanager\nasync def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:\n    \"\"\"Manage server startup and shutdown lifecycle.\"\"\"\n    # Initialize on startup\n    db = await Database.connect()\n    cache = await CacheService.connect()\n\n    try:\n        yield AppContext(db=db, cache=cache, config={\"timeout\": 30})\n    finally:\n        # Cleanup on shutdown\n        await cache.disconnect()\n        await db.disconnect()\n\nmcp = FastMCP(\"Production Server\", lifespan=app_lifespan)\n\n@mcp.tool()\ndef query_data(sql: str, ctx: Context) -> str:\n    \"\"\"Execute query using shared connection.\"\"\"\n    app_ctx = ctx.request_context.lifespan_context\n    return app_ctx.db.query(sql)\n```\n\n## Auto-Enable Thresholds (CC 2.1.9)\n\nConfigure MCP servers to auto-enable/disable based on context window usage:\n\n```yaml\n# .claude/settings.json\nmcp:\n  context7:\n    enabled: auto:75    # High-value docs, keep available longer\n  sequential-thinking:\n    enabled: auto:60    # Complex reasoning needs room\n  memory:\n    enabled: auto:90    # Knowledge graph - preserve until compaction\n  playwright:\n    enabled: auto:50    # Browser-heavy, disable early\n```\n\n**Threshold Guidelines:**\n| Threshold | Use Case | Rationale |\n|-----------|----------|-----------|\n| aut",
    "contentTruncated": true
  },
  "mcp-security-hardening": {
    "content": "# MCP Security Hardening\n\nDefense-in-depth security patterns for Model Context Protocol (MCP) integrations.\n\n## Overview\n\n- Securing MCP server implementations\n- Validating tool descriptions before LLM exposure\n- Implementing zero-trust tool allowlists\n- Detecting tool poisoning attacks (TPA)\n- Managing tool permissions and capabilities\n\n## Core Security Principle\n\n> **Treat ALL tool descriptions as untrusted input.**\n> **Validate tool identity with hash verification.**\n> **Apply least privilege to all tool capabilities.**\n\n## Threat Model Summary\n\n| Attack Vector | Defense | Implementation |\n|---------------|---------|----------------|\n| Tool Poisoning (TPA) | Zero-trust allowlist | Hash verification, mandatory vetting |\n| Prompt Injection | Description sanitization | Regex filtering, encoding detection |\n| Rug Pull | Change detection | Hash comparison on each invocation |\n| Data Exfiltration | Output filtering | Sensitive pattern removal |\n| Session Hijacking | Secure sessions | Cryptographic IDs, no auth in sessions |\n\n## Layer 1: Tool Description Sanitization\n\n```python\nimport re\n\nFORBIDDEN_PATTERNS = [\n    r\"ignore previous\", r\"system prompt\", r\"<.*instruction.*>\",\n    r\"IMPORTANT:\", r\"override\", r\"admin\", r\"sudo\",\n    r\"\\\\x[0-9a-fA-F]{2}\",  # Hex encoding\n    r\"&#x?[0-9a-fA-F]+;\",  # HTML entities\n]\n\ndef sanitize_tool_description(description: str) -> str:\n    \"\"\"Remove instruction-like phrases or encoding tricks.\"\"\"\n    if not description:\n        return \"\"\n    sanitized = description\n    for pattern in FORBIDDEN_PATTERNS:\n        sanitized = re.sub(pattern, \"[REDACTED]\", sanitized, flags=re.I)\n    return sanitized.strip()\n\ndef detect_injection_attempt(description: str) -> str | None:\n    \"\"\"Detect prompt injection patterns.\"\"\"\n    indicators = [\n        (r\"ignore.*previous\", \"instruction_override\"),\n        (r\"you are now\", \"role_hijack\"),\n        (r\"forget.*above\", \"context_wipe\"),\n    ]\n    for pattern, attack_type in indicators:\n        if re.search(pattern, description, re.I):\n            return attack_type\n    return None\n```\n\n## Layer 2: Zero-Trust Tool Allowlist\n\n```python\nfrom hashlib import sha256\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\n\n@dataclass\nclass AllowedTool:\n    name: str\n    description_hash: str\n    capabilities: list[str]\n    approved_at: datetime\n    approved_by: str\n    max_calls_per_minute: int = 60\n    requires_human_approval: bool = False\n\nclass MCPToolAllowlist:\n    \"\"\"Zero-trust allowlist - every tool must be explicitly vetted.\"\"\"\n\n    def __init__(self):\n        self._allowed_tools: dict[str, AllowedTool] = {}\n        self._call_counts: dict[str, list[datetime]] = {}\n\n    def register(self, tool: AllowedTool) -> None:\n        self._allowed_tools[tool.name] = tool\n        self._call_counts[tool.name] = []\n\n    def compute_hash(self, description: str) -> str:\n        return sha256(description.encode('utf-8')).hexdigest()\n\n    def validate(self, tool_name: str, description: str) -",
    "contentTruncated": true
  },
  "mcp-server-building": {
    "content": "# MCP Server Building\n\nBuild custom MCP servers to extend Claude with tools, resources, and prompts.\n\n## Architecture\n\n```\n+-------------+     JSON-RPC      +-------------+\n|   Claude    |<----------------->| MCP Server  |\n|   (Host)    |   stdio/SSE/WS    |  (Tools)    |\n+-------------+                   +-------------+\n```\n\n**Three Primitives:**\n- **Tools**: Functions Claude can call (with user approval)\n- **Resources**: Data Claude can read (files, API responses)\n- **Prompts**: Pre-defined prompt templates\n\n## Quick Start\n\n### Minimal Python Server (stdio)\n\n```python\nfrom mcp.server import Server\nfrom mcp.server.stdio import stdio_server\nfrom mcp.types import Tool, TextContent\n\nserver = Server(\"my-tools\")\n\n@server.list_tools()\nasync def list_tools() -> list[Tool]:\n    return [\n        Tool(\n            name=\"greet\",\n            description=\"Greet a user by name\",\n            inputSchema={\n                \"type\": \"object\",\n                \"properties\": {\n                    \"name\": {\"type\": \"string\", \"description\": \"Name to greet\"}\n                },\n                \"required\": [\"name\"]\n            }\n        )\n    ]\n\n@server.call_tool()\nasync def call_tool(name: str, arguments: dict) -> list[TextContent]:\n    if name == \"greet\":\n        return [TextContent(type=\"text\", text=f\"Hello, {arguments['name']}!\")]\n    raise ValueError(f\"Unknown tool: {name}\")\n\nasync def main():\n    async with stdio_server() as (read, write):\n        await server.run(read, write, server.create_initialization_options())\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(main())\n```\n\n### TypeScript Server (production)\n\n```typescript\nimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport {\n  CallToolRequestSchema,\n  ListToolsRequestSchema,\n} from \"@modelcontextprotocol/sdk/types.js\";\n\nconst server = new Server(\n  { name: \"my-tools\", version: \"1.0.0\" },\n  { capabilities: { tools: {} } }\n);\n\nserver.setRequestHandler(ListToolsRequestSchema, async () => ({\n  tools: [{\n    name: \"fetch_url\",\n    description: \"Fetch content from a URL\",\n    inputSchema: {\n      type: \"object\",\n      properties: { url: { type: \"string\" } },\n      required: [\"url\"],\n    },\n  }],\n}));\n\nserver.setRequestHandler(CallToolRequestSchema, async (request) => {\n  if (request.params.name === \"fetch_url\") {\n    const { url } = request.params.arguments as { url: string };\n    const response = await fetch(url);\n    return { content: [{ type: \"text\", text: await response.text() }] };\n  }\n  throw new Error(\"Unknown tool\");\n});\n\nawait server.connect(new StdioServerTransport());\n```\n\n## Detailed Guides\n\n- **Transport patterns**: See [references/transport-patterns.md](references/transport-patterns.md) for stdio, SSE, WebSocket\n- **Tool definitions**: See [references/tool-definitions.md](references/tool-definitions.md) for schemas, error handling, caching\n- **Resource patterns**: See [references/resource",
    "contentTruncated": true
  },
  "memory": {
    "content": "# Memory - Read & Access Operations\n\nUnified read-side memory skill with subcommands for searching, loading, syncing, history, and visualization.\n\n## Usage\n\n```bash\n/ork:memory search <query>  # Search knowledge graph\n/ork:memory load             # Load context at session start\n/ork:memory history          # View decision timeline\n/ork:memory viz              # Visualize knowledge graph\n/ork:memory status           # Show memory system health\n```\n\n---\n\n## CRITICAL: Use AskUserQuestion When No Subcommand\n\nIf invoked without a subcommand, ask the user what they want:\n\n```python\nAskUserQuestion(\n  questions=[{\n    \"question\": \"What memory operation do you need?\",\n    \"header\": \"Operation\",\n    \"options\": [\n      {\"label\": \"search\", \"description\": \"Search decisions and patterns in knowledge graph\"},\n      {\"label\": \"load\", \"description\": \"Load relevant context for this session\"},\n      {\"label\": \"history\", \"description\": \"View decision timeline\"},\n      {\"label\": \"viz\", \"description\": \"Visualize knowledge graph as Mermaid\"},\n      {\"label\": \"status\", \"description\": \"Check memory system health\"}\n    ],\n    \"multiSelect\": false\n  }]\n)\n```\n\n---\n\n## Subcommands\n\n### `search` - Search Knowledge Graph\n\nSearch past decisions, patterns, and entities from the knowledge graph.\n\n**Usage:**\n```bash\n/ork:memory search <query>                    # Search knowledge graph\n/ork:memory search --category <cat> <query>   # Filter by category\n/ork:memory search --limit <n> <query>        # Limit results (default: 10)\n/ork:memory search --agent <agent-id> <query> # Filter by agent scope\n/ork:memory search --global <query>           # Search cross-project best practices\n```\n\n**Flags:**\n\n| Flag | Behavior |\n|------|----------|\n| (default) | Search graph |\n| `--limit <n>` | Max results (default: 10) |\n| `--category <cat>` | Filter by category |\n| `--agent <agent-id>` | Filter results to a specific agent's memories |\n| `--global` | Search cross-project best practices |\n\n**Context-Aware Result Limits:**\n\nResult limits automatically adjust based on `context_window.used_percentage`:\n\n| Context Usage | Default Limit | Behavior |\n|---------------|---------------|----------|\n| 0-70% | 10 results | Full results with details |\n| 70-85% | 5 results | Reduced, summarized results |\n| >85% | 3 results | Minimal with \"more available\" hint |\n\n**Search Workflow:**\n\n1. Parse flags (--category, --limit, --agent, --global)\n2. Build filters from flags:\n   ```\n   Check for --category <cat> flag ‚Üí metadata.category: \"<cat>\"\n   Check for --agent <agent-id> flag ‚Üí agent_id: \"ork:{agent-id}\"\n   Check for --global flag ‚Üí user_id: \"orchestkit-global-best-practices\"\n   ```\n3. Search knowledge graph via `mcp__memory__search_nodes`:\n   ```json\n   { \"query\": \"user's search query\" }\n   ```\n\n**Entity Types to Look For:**\n- `Technology`: Tools, frameworks, databases (pgvector, PostgreSQL, React)\n- `Agent`: OrchestKit agents (database-engineer, backend-system-architect)\n- `Pattern`: Named patterns (cursor-pag",
    "contentTruncated": true
  },
  "memory-fabric": {
    "content": "# Memory Fabric - Graph Orchestration\n\nKnowledge graph orchestration via mcp__memory__* for entity extraction, query parsing, deduplication, and cross-reference boosting.\n\n## Overview\n\n- Comprehensive memory retrieval from the knowledge graph\n- Cross-referencing entities within graph storage\n- Ensuring no relevant memories are missed\n- Building unified context from graph queries\n\n## Architecture Overview\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                    Memory Fabric Layer                      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                                             ‚îÇ\n‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ\n‚îÇ   ‚îÇ   Query     ‚îÇ              ‚îÇ   Query     ‚îÇ              ‚îÇ\n‚îÇ   ‚îÇ   Parser    ‚îÇ              ‚îÇ   Executor  ‚îÇ              ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ\n‚îÇ          ‚îÇ                            ‚îÇ                     ‚îÇ\n‚îÇ          ‚ñº                            ‚ñº                     ‚îÇ\n‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ\n‚îÇ   ‚îÇ            Graph Query Dispatch              ‚îÇ          ‚îÇ\n‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ\n‚îÇ                          ‚îÇ                                  ‚îÇ\n‚îÇ                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ\n‚îÇ                ‚îÇ  mcp__memory__*    ‚îÇ                       ‚îÇ\n‚îÇ                ‚îÇ  (Knowledge Graph) ‚îÇ                       ‚îÇ\n‚îÇ                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ\n‚îÇ                          ‚îÇ                                  ‚îÇ\n‚îÇ                          ‚ñº                                  ‚îÇ\n‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ\n‚îÇ        ‚îÇ        Result Normalizer                ‚îÇ          ‚îÇ\n‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ\n‚îÇ                              ‚îÇ                              ‚îÇ\n‚îÇ                              ‚ñº                              ‚îÇ\n‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ\n‚îÇ        ‚îÇ     Deduplication Engine (>85% sim)     ‚îÇ          ‚îÇ\n‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ\n‚îÇ                              ‚îÇ                              ‚îÇ\n‚îÇ                              ‚ñº                              ‚îÇ\n‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ\n‚îÇ        ‚îÇ  Cross-Reference Booster                ‚îÇ          ‚îÇ\n‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ\n‚îÇ                              ‚îÇ                              ‚îÇ\n‚îÇ                              ‚ñº                              ‚îÇ\n‚îÇ        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ\n‚îÇ        ‚îÇ  Final Ranking: recency √ó relevance     ‚îÇ          ‚îÇ\n‚îÇ        ‚îÇ                 √ó source_authority      ‚îÇ          ‚îÇ\n‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ\n‚îÇ                                                             ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ",
    "contentTruncated": true
  },
  "monitoring-observability": {
    "content": "# Monitoring & Observability\n\nComprehensive patterns for infrastructure monitoring, LLM observability, and quality drift detection. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Infrastructure Monitoring](#infrastructure-monitoring) | 3 | CRITICAL | Prometheus metrics, Grafana dashboards, alerting rules |\n| [LLM Observability](#llm-observability) | 3 | HIGH | Langfuse tracing, cost tracking, evaluation scoring |\n| [Drift Detection](#drift-detection) | 3 | HIGH | Statistical drift, quality regression, drift alerting |\n\n**Total: 9 rules across 3 categories**\n\n## Quick Start\n\n```python\n# Prometheus metrics with RED method\nfrom prometheus_client import Counter, Histogram\n\nhttp_requests = Counter('http_requests_total', 'Total requests', ['method', 'endpoint', 'status'])\nhttp_duration = Histogram('http_request_duration_seconds', 'Request latency',\n    buckets=[0.01, 0.05, 0.1, 0.5, 1, 2, 5])\n```\n\n```python\n# Langfuse LLM tracing\nfrom langfuse import observe, get_client\n\n@observe()\nasync def analyze_content(content: str):\n    get_client().update_current_trace(\n        user_id=\"user_123\", session_id=\"session_abc\",\n        tags=[\"production\", \"orchestkit\"],\n    )\n    return await llm.generate(content)\n```\n\n```python\n# PSI drift detection\nimport numpy as np\n\npsi_score = calculate_psi(baseline_scores, current_scores)\nif psi_score >= 0.25:\n    alert(\"Significant quality drift detected!\")\n```\n\n## Infrastructure Monitoring\n\nPrometheus metrics, Grafana dashboards, and alerting for application health.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Prometheus Metrics | `rules/monitoring-prometheus.md` | RED method, counters, histograms, cardinality |\n| Grafana Dashboards | `rules/monitoring-grafana.md` | Golden Signals, SLO/SLI, health checks |\n| Alerting Rules | `rules/monitoring-alerting.md` | Severity levels, grouping, escalation, fatigue prevention |\n\n## LLM Observability\n\nLangfuse-based tracing, cost tracking, and evaluation for LLM applications.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Langfuse Traces | `rules/llm-langfuse-traces.md` | @observe decorator, OTEL spans, agent graphs |\n| Cost Tracking | `rules/llm-cost-tracking.md` | Token usage, spend alerts, Metrics API |\n| Eval Scoring | `rules/llm-eval-scoring.md` | Custom scores, evaluator tracing, quality monitoring |\n\n## Drift Detection\n\nStatistical and quality drift detection for production LLM systems.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Statistical Drift | `rules/drift-statistical.md` | PSI, KS test, KL divergence, EWMA |\n| Quality Drift | `rules/drift-quality.md` | Score regression, baseline comparison, canary prompts |\n| Drift Alerting | `rules/drift-alerting.md` | Dynamic thresholds, correlation, anti-patterns |\n\n## Key Decisions\n\n| Decision | Recommendation | Rationale |\n|----------|----------------|",
    "contentTruncated": true
  },
  "monorepo-context": {
    "content": "# Monorepo Context Patterns\n\n## Overview\n\nClaude Code 2.1.20 introduces `--add-dir` for multi-directory context, enabling monorepo-aware sessions where each service maintains its own CLAUDE.md instructions.\n\n## Monorepo Detection\n\nIndicators that a project is a monorepo:\n\n| Indicator | Tool |\n|-----------|------|\n| `pnpm-workspace.yaml` | pnpm |\n| `lerna.json` | Lerna |\n| `nx.json` | Nx |\n| `turbo.json` | Turborepo |\n| `rush.json` | Rush |\n| 3+ nested `package.json` files | Generic |\n\n## Per-Service CLAUDE.md\n\nEach service can have its own context instructions:\n\n```\nmonorepo/\n  CLAUDE.md               # Root context (workspace-wide rules)\n  packages/\n    api/\n      CLAUDE.md           # API-specific patterns\n      package.json\n    web/\n      CLAUDE.md           # Frontend-specific patterns\n      package.json\n    shared/\n      CLAUDE.md           # Shared library context\n      package.json\n```\n\n## --add-dir Usage\n\nStart Claude Code with additional directory context:\n\n```bash\n# From api service, add shared library context\nclaude --add-dir ../shared\n\n# Multiple directories\nclaude --add-dir ../shared --add-dir ../web\n```\n\n## Environment Variable\n\nEnable automatic CLAUDE.md loading from additional directories:\n\n```bash\nexport CLAUDE_CODE_ADDITIONAL_DIRECTORIES_CLAUDE_MD=1\n```\n\nWhen set, Claude Code reads CLAUDE.md from all `--add-dir` directories.\n\n## Root vs Service Context Separation\n\n### Root CLAUDE.md\n\n- Workspace-wide conventions (commit messages, branch naming)\n- Cross-service dependency rules\n- CI/CD pipeline overview\n- Shared tooling configuration\n\n### Service CLAUDE.md\n\n- Service-specific patterns and frameworks\n- Local test commands\n- API contracts and schemas\n- Service-specific environment variables\n\n## Related Skills\n\n- `configure` - OrchestKit configuration including monorepo detection\n- `architecture-patterns` - Folder structure enforcement",
    "contentTruncated": false
  },
  "performance": {
    "content": "# Performance\n\nComprehensive performance optimization patterns for frontend, backend, and LLM inference.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Core Web Vitals](#core-web-vitals) | 3 | CRITICAL | LCP, INP, CLS optimization with 2026 thresholds |\n| [Render Optimization](#render-optimization) | 3 | HIGH | React Compiler, memoization, virtualization |\n| [Lazy Loading](#lazy-loading) | 3 | HIGH | Code splitting, route splitting, preloading |\n| [Image Optimization](#image-optimization) | 3 | HIGH | Next.js Image, AVIF/WebP, responsive images |\n| [Profiling & Backend](#profiling--backend) | 3 | MEDIUM | React DevTools, py-spy, bundle analysis |\n| [LLM Inference](#llm-inference) | 3 | MEDIUM | vLLM, quantization, speculative decoding |\n\n**Total: 18 rules across 6 categories**\n\n## Core Web Vitals\n\nGoogle's Core Web Vitals with 2026 stricter thresholds.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| LCP Optimization | `rules/cwv-lcp.md` | Preload hero, SSR, fetchpriority=\"high\" |\n| INP Optimization | `rules/cwv-inp.md` | scheduler.yield, useTransition, requestIdleCallback |\n| CLS Prevention | `rules/cwv-cls.md` | Explicit dimensions, aspect-ratio, font-display |\n\n### 2026 Thresholds\n\n| Metric | Current Good | 2026 Good |\n|--------|--------------|-----------|\n| LCP | <= 2.5s | <= 2.0s |\n| INP | <= 200ms | <= 150ms |\n| CLS | <= 0.1 | <= 0.08 |\n\n## Render Optimization\n\nReact render performance patterns for React 19+.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| React Compiler | `rules/render-compiler.md` | Auto-memoization, \"Memo\" badge verification |\n| Manual Memoization | `rules/render-memo.md` | useMemo/useCallback escape hatches, state colocation |\n| Virtualization | `rules/render-virtual.md` | TanStack Virtual for 100+ item lists |\n\n## Lazy Loading\n\nCode splitting and lazy loading with React.lazy and Suspense.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| React.lazy + Suspense | `rules/loading-lazy.md` | Component lazy loading, error boundaries |\n| Route Splitting | `rules/loading-splitting.md` | React Router 7.x, Vite manual chunks |\n| Preloading | `rules/loading-preload.md` | Prefetch on hover, modulepreload hints |\n\n## Image Optimization\n\nProduction image optimization for modern web applications.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Next.js Image | `rules/images-nextjs.md` | Image component, priority, blur placeholder |\n| Format Selection | `rules/images-formats.md` | AVIF/WebP, quality 75-85, picture element |\n| Responsive Images | `rules/images-responsive.md` | sizes prop, art direction, CDN loaders |\n\n## Profiling & Backend\n\nProfiling tools and backend optimization patterns.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| React Profiling | `rules/profiling-react.md` | DevTools Profiler, flamegraph, render counts |\n| Backend Profiling | `rules/profiling-backend.md` | py-spy, cProfile, m",
    "contentTruncated": true
  },
  "plan-viz": {
    "content": "# Plan Visualization\n\nRender planned changes as structured ASCII visualizations with risk analysis, execution order, and impact metrics. Every section answers a specific reviewer question.\n\n**Core principle:** Encode judgment into visualization, not decoration.\n\n```bash\n/plan-viz                          # Auto-detect from current branch\n/plan-viz billing module redesign  # Describe the plan\n/plan-viz #234                     # Pull from GitHub issue\n```\n\n---\n\n## STEP 0: Detect or Clarify Plan Context\n\n**First**, attempt auto-detection by running `scripts/detect-plan-context.sh`:\n\n```bash\nbash \"$SKILL_DIR/scripts/detect-plan-context.sh\"\n```\n\nThis outputs branch name, issue number (if any), commit count, and file change summary.\n\n**If auto-detection finds a clear plan** (branch with commits diverging from main, or issue number in args), proceed to Step 1.\n\n**If ambiguous**, clarify with AskUserQuestion:\n\n```python\nAskUserQuestion(\n  questions=[{\n    \"question\": \"What should I visualize?\",\n    \"header\": \"Source\",\n    \"options\": [\n      {\"label\": \"Current branch changes (Recommended)\", \"description\": \"Auto-detect from git diff against main\"},\n      {\"label\": \"Describe the plan\", \"description\": \"I'll explain what I'm planning to change\"},\n      {\"label\": \"GitHub issue\", \"description\": \"Pull plan from a specific issue number\"},\n      {\"label\": \"Quick file diff only\", \"description\": \"Just show the change manifest, skip analysis\"}\n    ],\n    \"multiSelect\": false\n  }]\n)\n```\n\n---\n\n## STEP 1: Gather Data\n\nRun `scripts/analyze-impact.sh` for precise counts:\n\n```bash\nbash \"$SKILL_DIR/scripts/analyze-impact.sh\"\n```\n\nThis produces: files by action (add/modify/delete), line counts, test files affected, and dependency changes.\n\nFor architecture-level understanding, spawn an Explore agent on the affected directories:\n\n```python\nTask(\n  subagent_type=\"Explore\",\n  prompt=\"Explore the architecture of {affected_directories}. Return: component diagram, key data flows, health scores per module. Use the ascii-visualizer skill for diagrams.\",\n  model=\"haiku\"\n)\n```\n\n---\n\n## STEP 2: Render Tier 1 Header (Always)\n\nUse `assets/tier1-header.md` template. Fill in from gathered data. This is always shown first.\n\n```\nPLAN: {plan_name} ({issue_ref})  |  {phase_count} phases  |  {file_count} files  |  +{added} -{removed} lines\nRisk: {risk_level}  |  Confidence: {confidence}  |  Reversible until {last_safe_phase}\nBranch: {branch} -> {base_branch}\n\n[1] Changes  [2] Execution  [3] Risks  [4] Decisions  [5] Impact  [all]\n```\n\n**Risk level** = highest risk across all phases (LOW/MEDIUM/HIGH/CRITICAL).\n**Confidence** = LOW if >50% of changes are in untested code, MEDIUM if mixed, HIGH if well-tested paths.\n**Reversible until** = last phase before an irreversible operation (DROP, DELETE data, breaking API change).\n\n---\n\n## STEP 3: Ask Which Sections to Expand\n\n```python\nAskUserQuestion(\n  questions=[{\n    \"question\": \"Which sections to render?\",\n    \"header\": \"Sections\",\n    \"options\": [",
    "contentTruncated": true
  },
  "platform-upgrade-knowledge": {
    "content": "# Platform Upgrade Knowledge\n\nComprehensive reference for evaluating Claude model transitions, Claude Code version bumps, and OrchestKit plugin upgrades. Provides the evaluation criteria, compatibility matrices, and migration effort estimates used by the `upgrade-assessment` skill.\n\n## Overview\n\nPlatform upgrades span three independent axes, each with distinct compatibility concerns:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                   Platform Upgrade Axes                  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  Claude Model   ‚îÇ  Claude Code    ‚îÇ  OrchestKit Plugin  ‚îÇ\n‚îÇ  (AI backend)   ‚îÇ  (CLI runtime)  ‚îÇ  (skills/hooks/     ‚îÇ\n‚îÇ                 ‚îÇ                 ‚îÇ   agents layer)     ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  Context window ‚îÇ  Hook types     ‚îÇ  Skill format       ‚îÇ\n‚îÇ  Output limits  ‚îÇ  Skill format   ‚îÇ  Agent format       ‚îÇ\n‚îÇ  Capabilities   ‚îÇ  Agent format   ‚îÇ  Hook source        ‚îÇ\n‚îÇ  Model ID       ‚îÇ  Tool registry  ‚îÇ  Manifest schema    ‚îÇ\n‚îÇ  Pricing        ‚îÇ  Permission fmt ‚îÇ  Build system       ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\nEach axis can be upgraded independently, but interactions between them must be validated.\n\n## When to Use\n\n- As reference knowledge for the `upgrade-assessment` command skill\n- When planning a migration strategy across model or platform versions\n- When estimating effort for a platform upgrade\n- When classifying breaking changes by severity\n\n---\n\n## Model Capability Evolution\n\n### Context Windows\n\n| Model | Context Window | Max Output | Released |\n|-------|---------------|------------|----------|\n| Claude 3 Haiku | 200K | 4,096 | Mar 2024 |\n| Claude 3.5 Sonnet | 200K | 8,192 | Jun 2024 |\n| Claude 3.5 Sonnet v2 | 200K | 8,192 | Oct 2024 |\n| Claude 3.5 Haiku | 200K | 8,192 | Nov 2024 |\n| Claude Sonnet 4 | 200K | 64,000 | May 2025 |\n| Claude Opus 4 | 200K | 32,000 | May 2025 |\n| Claude Sonnet 4.5 | 1,000K | 64,000 | Sep 2025 |\n| Claude Opus 4.6 | 1,000K | 128,000 | Jan 2026 |\n\n### Feature Evolution\n\n| Feature | First Available | Notes |\n|---------|----------------|-------|\n| Tool use | Claude 3 | All current models |\n| Vision (image input) | Claude 3 | All current models |\n| Extended thinking | Claude Sonnet 4 | Opus 4, Sonnet 4.5, Opus 4.6 |\n| Computer use | Claude Sonnet 4 | Beta, not all models |\n| PDF input | Claude 3.5 Sonnet v2 | All newer models |\n| Token counting API | Claude 3.5 | All current models |\n| Prompt caching | Claude 3.5 | Automatic on Anthropic API |\n| Batch API | Claude 3.5 | All current models |\n| Citations | Claude Sonnet 4 | API feature |\n| Code execution | Claude Opus 4 | Sandbox environment |\n| MCP (Model Context Protocol) | Claude Sonnet 4 | Via Claude Code / Desktop |\n| Files API | Claude Opus 4.6 | Direct file attachment |\n| Data residency (`inference_geo`) | Claude Opus 4.6 | `\"global\"` or `\"us\"` for enterprise compliance |\n\n### Data Residency Controls (Enterprise)\n",
    "contentTruncated": true
  },
  "product-frameworks": {
    "content": "# Product Frameworks\n\nComprehensive product management frameworks covering business analysis, market intelligence, strategy, prioritization, metrics, personas, requirements, and user research. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Business & Market](#business--market) | 4 | HIGH | ROI/NPV/IRR calculations, TCO analysis, TAM/SAM/SOM sizing, competitive landscape |\n| [Strategy & Prioritization](#strategy--prioritization) | 4 | HIGH | Value proposition canvas, go/no-go gates, RICE scoring, WSJF ranking |\n| [Metrics & OKRs](#metrics--okrs) | 4 | HIGH | OKR writing, KPI trees, leading/lagging indicators, instrumentation |\n| [Research & Requirements](#research--requirements) | 4 | HIGH | User personas, journey maps, interview guides, PRDs |\n\n**Total: 16 rules across 4 categories**\n\n## Quick Start\n\n```markdown\n## ROI Quick Calculation\nROI = (Net Benefits - Total Costs) / Total Costs x 100%\n\n## RICE Prioritization\nRICE Score = (Reach x Impact x Confidence) / Effort\n\n## OKR Structure\nObjective: Qualitative, inspiring goal\n  KR1: Quantitative measure (from X to Y)\n  KR2: Quantitative measure (from X to Y)\n\n## User Story Format\nAs a [persona], I want [goal], so that [benefit].\n```\n\n## Business & Market\n\nFinancial analysis and market intelligence frameworks for investment decisions.\n\n- **`business-roi`** -- ROI, NPV, IRR, payback period calculations with Python examples\n- **`business-cost-benefit`** -- TCO analysis, build vs buy comparison, sensitivity analysis\n- **`market-tam-sam-som`** -- TAM/SAM/SOM market sizing with top-down and bottom-up methods\n- **`market-competitive`** -- Porter's Five Forces, SWOT, competitive landscape mapping\n\n## Strategy & Prioritization\n\nStrategic decision frameworks and quantitative prioritization methods.\n\n- **`strategy-value-prop`** -- Value Proposition Canvas, JTBD framework, fit assessment\n- **`strategy-go-no-go`** -- Stage gate criteria, scoring template, decision thresholds\n- **`prioritize-rice`** -- RICE scoring with reach, impact, confidence, effort scales\n- **`prioritize-wsjf`** -- WSJF cost of delay, time criticality, MoSCoW method\n\n## Metrics & OKRs\n\nGoal-setting and measurement frameworks for metrics-driven teams.\n\n- **`metrics-okr`** -- OKR structure, writing objectives and key results, examples\n- **`metrics-kpi-trees`** -- Revenue and product health KPI trees, North Star metric\n- **`metrics-leading-lagging`** -- Leading vs lagging indicators, balanced dashboards\n- **`metrics-instrumentation`** -- Metric definition template, event naming, alerting\n\n## Research & Requirements\n\nUser research methods and requirements documentation patterns.\n\n- **`research-personas`** -- User persona template, empathy maps, persona examples\n- **`research-journey-mapping`** -- Customer journey maps, service blueprints, experience curves\n- **`research-user-interviews`** -- Interview guides, usab",
    "contentTruncated": true
  },
  "prompt-engineering-suite": {
    "content": "# Prompt Engineering Suite\n\nDesign, version, and optimize prompts for production LLM applications.\n\n## Overview\n\n- Designing prompts for new LLM features\n- Improving accuracy with Chain-of-Thought reasoning\n- Few-shot learning with example selection\n- Managing prompts in production (versioning, A/B testing)\n- Automatic prompt optimization with DSPy\n\n## Quick Reference\n\n### Chain-of-Thought Pattern\n\n```python\nfrom langchain_core.prompts import ChatPromptTemplate\n\nCOT_SYSTEM = \"\"\"You are a helpful assistant that solves problems step-by-step.\n\nWhen solving problems:\n1. Break down the problem into clear steps\n2. Show your reasoning for each step\n3. Verify your answer before responding\n4. If uncertain, acknowledge limitations\n\nFormat your response as:\nSTEP 1: [description]\nReasoning: [your thought process]\n\nSTEP 2: [description]\nReasoning: [your thought process]\n\n...\n\nFINAL ANSWER: [your conclusion]\"\"\"\n\ncot_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", COT_SYSTEM),\n    (\"human\", \"Problem: {problem}\\n\\nThink through this step-by-step.\"),\n])\n```\n\n### Few-Shot with Dynamic Examples\n\n```python\nfrom langchain_core.prompts import FewShotChatMessagePromptTemplate\n\nexamples = [\n    {\"input\": \"What is 2+2?\", \"output\": \"4\"},\n    {\"input\": \"What is the capital of France?\", \"output\": \"Paris\"},\n]\n\nfew_shot = FewShotChatMessagePromptTemplate(\n    examples=examples,\n    example_prompt=ChatPromptTemplate.from_messages([\n        (\"human\", \"{input}\"),\n        (\"ai\", \"{output}\"),\n    ]),\n)\n\nfinal_prompt = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a helpful assistant. Answer concisely.\"),\n    few_shot,\n    (\"human\", \"{input}\"),\n])\n```\n\n### Prompt Versioning with Langfuse SDK v3\n\n```python\nfrom langfuse import Langfuse\n# Note: Langfuse SDK v3 is OTEL-native (acquired by ClickHouse Jan )\n\nlangfuse = Langfuse()\n\n# Get versioned prompt with label\nprompt = langfuse.get_prompt(\n    name=\"customer-support-v2\",\n    label=\"production\",  # production, staging, canary\n    cache_ttl_seconds=300,\n)\n\n# Compile with variables\ncompiled = prompt.compile(\n    customer_name=\"John\",\n    issue=\"billing question\"\n)\n```\n\n### DSPy 3.1.0 Automatic Optimization\n\n```python\nimport dspy\n\nclass OptimizedQA(dspy.Module):\n    def __init__(self):\n        self.generate = dspy.Predict(\"question -> answer\")\n\n    def forward(self, question):\n        return self.generate(question=question)\n\n# Optimize with MIPROv2 (recommended) or BootstrapFewShot\noptimizer = dspy.MIPROv2(metric=answer_match)  # Data+demo-aware Bayesian optimization\noptimized = optimizer.compile(OptimizedQA(), trainset=examples)\n\n# Alternative: GEPA (July 2025) - Reflective Prompt Evolution\n# Uses model introspection to analyze failures and propose better prompts\n```\n\n## Pattern Selection Guide\n\n| Pattern | When to Use | Example Use Case |\n|---------|-------------|------------------|\n| Zero-shot | Simple, well-defined tasks | Classification, extraction |\n| Few-shot | Complex tasks needing examples | Format conve",
    "contentTruncated": true
  },
  "pwa-patterns": {
    "content": "# PWA Patterns\n\nProgressive Web App patterns using Workbox 7.x for service worker management, offline-first strategies, and app-like experiences.\n\n## Service Worker Lifecycle\n\n```\nInstalling -> Waiting -> Active\n     ‚îÇ           ‚îÇ           ‚îÇ\n  install    activated    fetch events\n (precache)  when old SW  (runtime cache)\n              is gone\n```\n\n## Workbox: Generate Service Worker\n\n```javascript\n// build-sw.js (Node.js)\nconst { generateSW } = require('workbox-build');\n\nasync function buildServiceWorker() {\n  await generateSW({\n    globDirectory: 'dist/',\n    globPatterns: ['**/*.{html,js,css,png,jpg,json,woff2}'],\n    swDest: 'dist/sw.js',\n    clientsClaim: true,\n    skipWaiting: true,\n    navigateFallback: '/index.html',\n    navigateFallbackDenylist: [/^\\/api\\//],\n    runtimeCaching: [\n      {\n        urlPattern: /^https:\\/\\/api\\.example\\.com\\//,\n        handler: 'NetworkFirst',\n        options: { cacheName: 'api-cache', networkTimeoutSeconds: 10 },\n      },\n      {\n        urlPattern: /\\.(?:png|jpg|jpeg|svg|gif|webp)$/,\n        handler: 'CacheFirst',\n        options: { cacheName: 'images', expiration: { maxEntries: 60, maxAgeSeconds: 30 * 24 * 60 * 60 } },\n      },\n    ],\n  });\n}\n```\n\n## Caching Strategies\n\n```javascript\n// CacheFirst: Static assets that rarely change\nregisterRoute(/\\.(?:js|css|woff2)$/, new CacheFirst({\n  cacheName: 'static-v1',\n  plugins: [new ExpirationPlugin({ maxEntries: 100, maxAgeSeconds: 365 * 24 * 60 * 60 })],\n}));\n\n// NetworkFirst: API calls (fresh data preferred)\nregisterRoute(/\\/api\\//, new NetworkFirst({\n  cacheName: 'api-cache',\n  networkTimeoutSeconds: 10,\n  plugins: [new CacheableResponsePlugin({ statuses: [0, 200] })],\n}));\n\n// StaleWhileRevalidate: User avatars, non-critical images\nregisterRoute(/\\/avatars\\//, new StaleWhileRevalidate({ cacheName: 'avatars' }));\n\n// NetworkOnly: Auth endpoints\nregisterRoute(/\\/auth\\//, new NetworkOnly());\n```\n\n## VitePWA Integration\n\n```typescript\n// vite.config.ts\nimport { VitePWA } from 'vite-plugin-pwa';\n\nexport default defineConfig({\n  plugins: [\n    VitePWA({\n      registerType: 'autoUpdate',\n      workbox: {\n        globPatterns: ['**/*.{js,css,html,ico,png,svg,woff2}'],\n        runtimeCaching: [{ urlPattern: /^https:\\/\\/api\\./, handler: 'NetworkFirst' }],\n      },\n      manifest: {\n        name: 'My PWA App',\n        short_name: 'MyPWA',\n        theme_color: '#4f46e5',\n        icons: [\n          { src: '/icon-192.png', sizes: '192x192', type: 'image/png' },\n          { src: '/icon-512.png', sizes: '512x512', type: 'image/png' },\n        ],\n      },\n    }),\n  ],\n});\n```\n\n## Web App Manifest\n\n```json\n{\n  \"name\": \"My Progressive Web App\",\n  \"short_name\": \"MyPWA\",\n  \"start_url\": \"/\",\n  \"display\": \"standalone\",\n  \"background_color\": \"#ffffff\",\n  \"theme_color\": \"#4f46e5\",\n  \"icons\": [\n    { \"src\": \"/icons/icon-192.png\", \"sizes\": \"192x192\", \"type\": \"image/png\", \"purpose\": \"maskable\" },\n    { \"src\": \"/icons/icon-512.png\", \"sizes\": \"512x512\", \"type\": \"image/png\" }\n  ]\n}\n```\n\n",
    "contentTruncated": true
  },
  "python-backend": {
    "content": "# Python Backend\n\nPatterns for building production Python backends with asyncio, FastAPI, SQLAlchemy 2.0, and connection pooling. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Asyncio](#asyncio) | 3 | HIGH | TaskGroup, structured concurrency, cancellation handling |\n| [FastAPI](#fastapi) | 3 | HIGH | Dependencies, middleware, background tasks |\n| [SQLAlchemy](#sqlalchemy) | 3 | HIGH | Async sessions, relationships, migrations |\n| [Pooling](#pooling) | 3 | MEDIUM | Database pools, HTTP sessions, tuning |\n\n**Total: 12 rules across 4 categories**\n\n## Quick Start\n\n```python\n# FastAPI + SQLAlchemy async session\nasync def get_db() -> AsyncGenerator[AsyncSession, None]:\n    async with async_session_factory() as session:\n        try:\n            yield session\n            await session.commit()\n        except Exception:\n            await session.rollback()\n            raise\n\n@router.get(\"/users/{user_id}\")\nasync def get_user(user_id: UUID, db: AsyncSession = Depends(get_db)):\n    result = await db.execute(select(User).where(User.id == user_id))\n    return result.scalar_one_or_none()\n```\n\n```python\n# Asyncio TaskGroup with timeout\nasync def fetch_all(urls: list[str]) -> list[dict]:\n    async with asyncio.timeout(30):\n        async with asyncio.TaskGroup() as tg:\n            tasks = [tg.create_task(fetch_url(url)) for url in urls]\n    return [t.result() for t in tasks]\n```\n\n## Asyncio\n\nModern Python asyncio patterns using structured concurrency, TaskGroup, and Python 3.11+ features.\n\n### Key Patterns\n\n- **TaskGroup** replaces `gather()` with structured concurrency and auto-cancellation\n- **`asyncio.timeout()`** context manager for composable timeouts\n- **Semaphore** for concurrency limiting (rate-limit HTTP requests)\n- **`except*`** with ExceptionGroup for handling multiple task failures\n- **`asyncio.to_thread()`** for bridging sync code to async\n\n### Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Task spawning | TaskGroup not gather() |\n| Timeouts | asyncio.timeout() context manager |\n| Concurrency limit | asyncio.Semaphore |\n| Sync bridge | asyncio.to_thread() |\n| Cancellation | Always re-raise CancelledError |\n\n## FastAPI\n\nProduction-ready FastAPI patterns for lifespan, dependencies, middleware, and settings.\n\n### Key Patterns\n\n- **Lifespan** with `asynccontextmanager` for startup/shutdown resource management\n- **Dependency injection** with class-based services and `Depends()`\n- **Middleware stack**: CORS -> RequestID -> Timing -> Logging\n- **Pydantic Settings** with `.env` and field validation\n- **Exception handlers** with RFC 7807 Problem Details\n\n### Key Decisions\n\n| Decision | Recommendation |\n|----------|----------------|\n| Lifespan | asynccontextmanager (not events) |\n| Dependencies | Class-based services with DI |\n| Settings | Pydantic Settings with .env |\n| Response | ORJSONResponse ",
    "contentTruncated": true
  },
  "quality-gates": {
    "content": "# Quality Gates\n\nThis skill teaches agents how to assess task complexity, enforce quality gates, and prevent wasted work on incomplete or poorly-defined tasks.\n\n**Key Principle:** Stop and clarify before proceeding with incomplete information. Better to ask questions than to waste cycles on the wrong solution.\n\n---\n\n## Overview\n\n### Auto-Activate Triggers\n- Receiving a new task assignment\n- Starting a complex feature implementation\n- Before allocating work in Squad mode\n- When requirements seem unclear or incomplete\n- After 3 failed attempts at the same task\n- When blocked by dependencies\n\n### Manual Activation\n- User asks for complexity assessment\n- Planning a multi-step project\n- Before committing to a timeline\n\n---\n\n## Core Concepts\n\n### Complexity Scoring (1-5 Scale)\n\n| Level | Files | Lines | Time | Characteristics |\n|-------|-------|-------|------|-----------------|\n| 1 - Trivial | 1 | < 50 | < 30 min | No deps, no unknowns |\n| 2 - Simple | 1-3 | 50-200 | 30 min - 2 hr | 0-1 deps, minimal unknowns |\n| 3 - Moderate | 3-10 | 200-500 | 2-8 hr | 2-3 deps, some unknowns |\n| 4 - Complex | 10-25 | 500-1500 | 8-24 hr | 4-6 deps, significant unknowns |\n| 5 - Very Complex | 25+ | 1500+ | 24+ hr | 7+ deps, many unknowns |\n\n**See:** `references/complexity-scoring.md` for detailed examples and assessment formulas.\n\n### Blocking Thresholds\n\n| Condition | Threshold | Action |\n|-----------|-----------|--------|\n| Critical Questions | > 3 unanswered | BLOCK |\n| Missing Dependencies | Any blocking | BLOCK |\n| Failed Attempts | >= 3 | BLOCK & ESCALATE |\n| Evidence Failure | 2 fix attempts | BLOCK |\n| Complexity Overflow | Level 4-5 no plan | BLOCK |\n\n**WARNING Conditions** (proceed with caution):\n- Level 3 complexity\n- 1-2 unanswered questions\n- 1-2 failed attempts\n\n**See:** `references/blocking-thresholds.md` for escalation protocols and decision logic.\n\n---\n\n## References\n\n### Complexity Scoring\n**See:** `references/complexity-scoring.md`\n\nKey topics covered:\n- Detailed Level 1-5 characteristics and examples\n- Quick assessment formula\n- Assessment checklist\n\n### Blocking Thresholds & Escalation\n**See:** `references/blocking-thresholds.md`\n\nKey topics covered:\n- BLOCKING vs WARNING conditions\n- Escalation protocol and message templates\n- Gate decision logic\n- Attempt tracking\n\n### Quality Gate Workflows\n**See:** `references/workflows.md`\n\nKey topics covered:\n- Pre-task gate validation workflow\n- Stuck detection and escalation workflow\n- Complexity breakdown workflow (Level 4-5)\n- Requirements completeness check\n\n### Gate Patterns\n**See:** `references/gate-patterns.md`\n\nKey topics covered:\n- Gate validation process templates\n- Integration with context system\n- Common pitfalls\n\n### LLM Quality Validation\n**See:** `references/llm-quality-validation.md`\n\nKey topics covered:\n- LLM-as-judge patterns\n- Quality aspects (relevance, depth, coherence, accuracy, completeness)\n- Fail-open vs fail-closed strategies\n- Graceful degradation patterns\n- Triple-consumer artifac",
    "contentTruncated": true
  },
  "rag-retrieval": {
    "content": "# RAG Retrieval\n\nComprehensive patterns for building production RAG systems. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Core RAG](#core-rag) | 4 | CRITICAL | Basic RAG, citations, hybrid search, context management |\n| [Embeddings](#embeddings) | 3 | HIGH | Model selection, chunking, batch/cache optimization |\n| [Contextual Retrieval](#contextual-retrieval) | 3 | HIGH | Context-prepending, hybrid BM25+vector, pipeline |\n| [HyDE](#hyde) | 3 | HIGH | Vocabulary mismatch, hypothetical document generation |\n| [Agentic RAG](#agentic-rag) | 4 | HIGH | Self-RAG, CRAG, knowledge graphs, adaptive routing |\n| [Multimodal RAG](#multimodal-rag) | 3 | MEDIUM | Image+text retrieval, PDF chunking, cross-modal search |\n| [Query Decomposition](#query-decomposition) | 3 | MEDIUM | Multi-concept queries, parallel retrieval, RRF fusion |\n| [Reranking](#reranking) | 3 | MEDIUM | Cross-encoder, LLM scoring, combined signals |\n| [PGVector](#pgvector) | 4 | HIGH | PostgreSQL hybrid search, HNSW indexes, schema design |\n\n**Total: 30 rules across 9 categories**\n\n## Core RAG\n\nFundamental patterns for retrieval, generation, and pipeline composition.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Basic RAG | `rules/core-basic-rag.md` | Retrieve + context + generate with citations |\n| Hybrid Search | `rules/core-hybrid-search.md` | RRF fusion (k=60) for semantic + keyword |\n| Context Management | `rules/core-context-management.md` | Token budgeting + sufficiency check |\n| Pipeline Composition | `rules/core-pipeline-composition.md` | Composable Decompose ‚Üí HyDE ‚Üí Retrieve ‚Üí Rerank |\n\n## Embeddings\n\nEmbedding models, chunking strategies, and production optimization.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Models & API | `rules/embeddings-models.md` | Model selection, batch API, similarity |\n| Chunking | `rules/embeddings-chunking.md` | Semantic boundary splitting, 512 token sweet spot |\n| Advanced | `rules/embeddings-advanced.md` | Redis cache, Matryoshka dims, batch processing |\n\n## Contextual Retrieval\n\nAnthropic's context-prepending technique ‚Äî 67% fewer retrieval failures.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Context Prepending | `rules/contextual-prepend.md` | LLM-generated context + prompt caching |\n| Hybrid Search | `rules/contextual-hybrid.md` | 40% BM25 / 60% vector weight split |\n| Complete Pipeline | `rules/contextual-pipeline.md` | End-to-end indexing + hybrid retrieval |\n\n## HyDE\n\nHypothetical Document Embeddings for bridging vocabulary gaps.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Generation | `rules/hyde-generation.md` | Embed hypothetical doc, not query |\n| Per-Concept | `rules/hyde-per-concept.md` | Parallel HyDE for multi-topic queries |\n| Fallback | `rules/hyde-fallback.md` | 2-3s timeout ‚Üí direct embedding fallback |\n\n## Agentic RAG\n\nSe",
    "contentTruncated": true
  },
  "react-server-components-framework": {
    "content": "# React Server Components Framework\n\n## Overview\n\nReact Server Components (RSC) enable server-first rendering with client-side interactivity. This skill covers Next.js 16 App Router patterns, Server Components, Server Actions, and streaming.\n\n**When to use this skill:**\n- Building Next.js 16+ applications with the App Router\n- Designing component boundaries (Server vs Client Components)\n- Implementing data fetching with caching and revalidation\n- Creating mutations with Server Actions\n- Optimizing performance with streaming and Suspense\n\n---\n\n## Quick Reference\n\n### Server vs Client Components\n\n| Feature | Server Component | Client Component |\n|---------|-----------------|------------------|\n| Directive | None (default) | `'use client'` |\n| Async/await | Yes | No |\n| Hooks | No | Yes |\n| Browser APIs | No | Yes |\n| Database access | Yes | No |\n| Client JS bundle | Zero | Ships to client |\n\n**Key Rule**: Server Components can render Client Components, but Client Components cannot directly import Server Components (use `children` prop instead).\n\n### Data Fetching Quick Reference\n\n**Next.js 16 Cache Components (Recommended):**\n\n```tsx\nimport { cacheLife, cacheTag } from 'next/cache'\n\n// Cached component with duration\nasync function CachedProducts() {\n  'use cache'\n  cacheLife('hours')\n  cacheTag('products')\n  return await db.product.findMany()\n}\n\n// Invalidate cache\nimport { revalidateTag } from 'next/cache'\nrevalidateTag('products')\n```\n\n**Legacy Fetch Options (Next.js 15):**\n\n```tsx\n// Static (cached indefinitely)\nawait fetch(url, { cache: 'force-cache' })\n\n// Revalidate every 60 seconds\nawait fetch(url, { next: { revalidate: 60 } })\n\n// Always fresh\nawait fetch(url, { cache: 'no-store' })\n\n// Tag-based revalidation\nawait fetch(url, { next: { tags: ['posts'] } })\n```\n\n### Server Actions Quick Reference\n\n```tsx\n'use server'\n\nexport async function createPost(formData: FormData) {\n  const title = formData.get('title') as string\n  const post = await db.post.create({ data: { title } })\n  revalidatePath('/posts')\n  redirect(\"/posts/\" + post.id)\n}\n```\n\n### Async Params/SearchParams (Next.js 16)\n\nRoute parameters and search parameters are now Promises that must be awaited:\n\n```tsx\n// app/posts/[slug]/page.tsx\nexport default async function PostPage({\n  params,\n  searchParams,\n}: {\n  params: Promise<{ slug: string }>\n  searchParams: Promise<{ page?: string }>\n}) {\n  const { slug } = await params\n  const { page } = await searchParams\n  return <Post slug={slug} page={page} />\n}\n```\n\n**Note:** Also applies to `layout.tsx`, `generateMetadata()`, and route handlers. See `references/nextjs-16-upgrade.md` for complete migration guide.\n\n---\n\n## References\n\n### Server Components\n**See: `references/server-components.md`**\n\nKey topics covered:\n- Async server components and direct database access\n- Data fetching patterns (parallel, sequential, cached)\n- Route segment config (dynamic, revalidate, PPR)\n- generateStaticParams for SSG\n- Error handling and composition patte",
    "contentTruncated": true
  },
  "release-management": {
    "content": "# Release Management\n\nAutomate releases with `gh release`, semantic versioning, and changelog generation.\n\n## Quick Reference\n\n### Create Release\n\n```bash\n# Auto-generate notes from PRs\ngh release create v1.2.0 --generate-notes\n\n# With custom title\ngh release create v1.2.0 --title \"Version 1.2.0: Performance Update\" --generate-notes\n\n# Draft release (review before publishing)\ngh release create v1.2.0 --draft --generate-notes\n\n# Pre-release (beta, rc)\ngh release create v1.2.0-beta.1 --prerelease --generate-notes\n\n# With custom notes\ngh release create v1.2.0 --notes \"## Highlights\n- New auth system\n- 50% faster search\"\n\n# From notes file\ngh release create v1.2.0 --notes-file RELEASE_NOTES.md\n```\n\n### List & View Releases\n\n```bash\n# List all releases\ngh release list\n\n# View specific release\ngh release view v1.2.0\n\n# View in browser\ngh release view v1.2.0 --web\n\n# JSON output\ngh release list --json tagName,publishedAt,isPrerelease\n```\n\n### Verify Releases (gh CLI 2.86.0+)\n\n```bash\n# Verify release attestation (sigstore)\ngh release verify v1.2.0\n\n# Verify specific asset\ngh release verify-asset v1.2.0 ./dist/app.zip\n\n# Verify with custom trust policy\ngh release verify v1.2.0 --owner myorg\n```\n\n### Manage Releases\n\n```bash\n# Edit release\ngh release edit v1.2.0 --title \"New Title\" --notes \"Updated notes\"\n\n# Delete release\ngh release delete v1.2.0\n\n# Upload assets\ngh release upload v1.2.0 ./dist/app.zip ./dist/app.tar.gz\n```\n\n---\n\n## Semantic Versioning\n\n```\nMAJOR.MINOR.PATCH\n  ‚îÇ     ‚îÇ     ‚îÇ\n  ‚îÇ     ‚îÇ     ‚îî‚îÄ‚îÄ Bug fixes (backwards compatible)\n  ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ New features (backwards compatible)\n  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Breaking changes\n\nExamples:\n  1.0.0 ‚Üí 1.0.1  (patch: bug fix)\n  1.0.1 ‚Üí 1.1.0  (minor: new feature)\n  1.1.0 ‚Üí 2.0.0  (major: breaking change)\n\nPre-release:\n  2.0.0-alpha.1  (early testing)\n  2.0.0-beta.1   (feature complete)\n  2.0.0-rc.1     (release candidate)\n```\n\n---\n\n## Release Workflow\n\n### Standard Release\n\n```bash\n# 1. Ensure main is up to date\ngit checkout main\ngit pull origin main\n\n# 2. Determine version bump\n# Check commits since last release\ngh release view --json tagName -q .tagName  # Current: v1.2.3\ngit log v1.2.3..HEAD --oneline\n\n# 3. Create and push tag\ngit tag -a v1.3.0 -m \"Release v1.3.0\"\ngit push origin v1.3.0\n\n# 4. Create GitHub release\ngh release create v1.3.0 \\\n  --title \"v1.3.0: Feature Name\" \\\n  --generate-notes\n\n# 5. Close milestone if used\ngh api -X PATCH repos/:owner/:repo/milestones/5 -f state=closed\n```\n\n### Hotfix Release\n\n```bash\n# 1. Branch from release tag\ngit checkout -b hotfix/v1.2.4 v1.2.3\n\n# 2. Fix and commit\ngit commit -m \"fix: Critical security patch\"\n\n# 3. Tag and release\ngit tag -a v1.2.4 -m \"Hotfix: Security patch\"\ngit push origin v1.2.4\ngh release create v1.2.4 --title \"v1.2.4: Security Hotfix\" \\\n  --notes \"Critical security fix for authentication bypass\"\n\n# 4. Merge fix to main\ngit checkout main\ngit cherry-pick <commit-sha>\ngit push origin main\n```\n\n---\n\n## Changelog Generation\n\n### Auto-Generated (from ",
    "contentTruncated": true
  },
  "remember": {
    "content": "# Remember - Store Decisions and Patterns\n\nStore important decisions, patterns, or context in the knowledge graph for future sessions. Supports tracking success/failure outcomes for building a Best Practice Library.\n\n## Architecture\n\nThe remember skill uses **knowledge graph** as storage:\n\n1. **Knowledge Graph**: Entity and relationship storage via `mcp__memory__create_entities` and `mcp__memory__create_relations` - FREE, zero-config, always works\n\n**Benefits:**\n- Zero configuration required - works out of the box\n- Explicit relationship queries (e.g., \"what does X use?\")\n- Cross-referencing between entities\n- No cloud dependency\n\n**Automatic Entity Extraction:**\n- Extracts capitalized terms as potential entities (PostgreSQL, React, pgvector)\n- Detects agent names (database-engineer, backend-system-architect)\n- Identifies pattern names (cursor-pagination, connection-pooling)\n- Recognizes \"X uses Y\", \"X recommends Y\", \"X requires Y\" relationship patterns\n\n## Usage\n\n### Store Decisions (Default)\n```\n/remember <text>\n/remember --category <category> <text>\n/remember --success <text>     # Mark as successful pattern\n/remember --failed <text>      # Mark as anti-pattern\n/remember --success --category <category> <text>\n\n# Agent-scoped memory\n/remember --agent <agent-id> <text>         # Store in agent-specific scope\n/remember --global <text>                   # Store as cross-project best practice\n```\n\n## Flags\n\n| Flag | Behavior |\n|------|----------|\n| (default) | Write to graph |\n| `--success` | Mark as successful pattern |\n| `--failed` | Mark as anti-pattern |\n| `--category <cat>` | Set category |\n| `--agent <agent-id>` | Scope memory to a specific agent |\n| `--global` | Store as cross-project best practice |\n\n## Categories\n\n- `decision` - Why we chose X over Y (default)\n- `architecture` - System design and patterns\n- `pattern` - Code conventions and standards\n- `blocker` - Known issues and workarounds\n- `constraint` - Limitations and requirements\n- `preference` - User/team preferences\n- `pagination` - Pagination strategies\n- `database` - Database patterns\n- `authentication` - Auth approaches\n- `api` - API design patterns\n- `frontend` - Frontend patterns\n- `performance` - Performance optimizations\n\n## Outcome Flags\n\n- `--success` - Pattern that worked well (positive outcome)\n- `--failed` - Pattern that caused problems (anti-pattern)\n\nIf neither flag is provided, the memory is stored as neutral (informational).\n\n## Workflow\n\n### 1. Parse Input\n\n```\nCheck for --success flag ‚Üí outcome: success\nCheck for --failed flag ‚Üí outcome: failed\nCheck for --category <category> flag\nCheck for --agent <agent-id> flag ‚Üí agent_id: \"ork:{agent-id}\"\nCheck for --global flag ‚Üí use global user_id\nExtract the text to remember\nIf no category specified, auto-detect from content\n```\n\n### 2. Auto-Detect Category\n\n| Keywords | Category |\n|----------|----------|\n| chose, decided, selected | decision |\n| architecture, design, system | architecture |\n| pattern, convention, style | ",
    "contentTruncated": true
  },
  "responsive-patterns": {
    "content": "# Responsive Patterns\n\nModern responsive design patterns using Container Queries, fluid typography, and mobile-first strategies for React applications (2026 best practices).\n\n## Overview\n\n- Building reusable components that adapt to their container\n- Implementing fluid typography that scales smoothly\n- Creating responsive layouts without media query overload\n- Building design system components for multiple contexts\n- Optimizing for variable container sizes (sidebars, modals, grids)\n\n## Core Concepts\n\n### Container Queries vs Media Queries\n\n| Feature | Media Queries | Container Queries |\n|---------|---------------|-------------------|\n| Responds to | Viewport size | Container size |\n| Component reuse | Context-dependent | Truly portable |\n| Browser support | Universal | Baseline 2023+ |\n| Use case | Page layouts | Component layouts |\n\n## CSS Patterns\n\n### 1. Container Query Basics\n\n```css\n/* Define a query container */\n.card-container {\n  container-type: inline-size;\n  container-name: card;\n}\n\n/* Style based on container width */\n@container card (min-width: 400px) {\n  .card {\n    display: grid;\n    grid-template-columns: 200px 1fr;\n  }\n}\n\n@container card (max-width: 399px) {\n  .card {\n    display: flex;\n    flex-direction: column;\n  }\n}\n```\n\n### 2. Container Query Units (cqi, cqb)\n\n```css\n/* Use cqi (container query inline) over cqw */\n.card-title {\n  /* 5% of container's inline size */\n  font-size: clamp(1rem, 5cqi, 2rem);\n}\n\n.card-content {\n  /* Responsive padding based on container */\n  padding: 2cqi;\n}\n\n/* cqb for block dimension (height-aware containers) */\n.sidebar-item {\n  height: 10cqb;\n}\n```\n\n### 3. Fluid Typography with clamp()\n\n```css\n/* Accessible fluid typography */\n:root {\n  /* Base font respects user preferences (rem) */\n  --font-size-base: 1rem;\n\n  /* Fluid scale with min/max bounds */\n  --font-size-sm: clamp(0.875rem, 0.8rem + 0.25vw, 1rem);\n  --font-size-md: clamp(1rem, 0.9rem + 0.5vw, 1.25rem);\n  --font-size-lg: clamp(1.25rem, 1rem + 1vw, 2rem);\n  --font-size-xl: clamp(1.5rem, 1rem + 2vw, 3rem);\n  --font-size-2xl: clamp(2rem, 1rem + 3vw, 4rem);\n}\n\nh1 { font-size: var(--font-size-2xl); }\nh2 { font-size: var(--font-size-xl); }\nh3 { font-size: var(--font-size-lg); }\np { font-size: var(--font-size-md); }\nsmall { font-size: var(--font-size-sm); }\n```\n\n### 4. Container-Based Fluid Typography\n\n```css\n/* For component-scoped fluid text */\n.widget {\n  container-type: inline-size;\n}\n\n.widget-title {\n  /* Fluid within container, respecting user rem */\n  font-size: clamp(1rem, 0.5rem + 5cqi, 2rem);\n}\n\n.widget-body {\n  font-size: clamp(0.875rem, 0.5rem + 3cqi, 1.125rem);\n}\n```\n\n### 5. Mobile-First Breakpoints\n\n```css\n/* Mobile-first: start small, add complexity */\n.layout {\n  display: flex;\n  flex-direction: column;\n  gap: 1rem;\n}\n\n/* Tablet and up */\n@media (min-width: 768px) {\n  .layout {\n    flex-direction: row;\n  }\n}\n\n/* Desktop */\n@media (min-width: 1024px) {\n  .layout {\n    max-width: 1200px;\n    margin-inline: auto;\n  }\n}\n```\n\n### 6. ",
    "contentTruncated": true
  },
  "review-pr": {
    "content": "# Review PR\n\nDeep code review using 6-7 parallel specialized agents.\n\n## Quick Start\n\n```bash\n/review-pr 123\n/review-pr feature-branch\n```\n\n> **Opus 4.6**: Parallel agents use native adaptive thinking for deeper analysis. Complexity-aware routing matches agent model to review difficulty.\n\n---\n\n## STEP 0: Verify User Intent with AskUserQuestion\n\n**BEFORE creating tasks**, clarify review focus:\n\n```python\nAskUserQuestion(\n  questions=[{\n    \"question\": \"What type of review do you need?\",\n    \"header\": \"Focus\",\n    \"options\": [\n      {\"label\": \"Full review (Recommended)\", \"description\": \"Security + code quality + tests + architecture\"},\n      {\"label\": \"Security focus\", \"description\": \"Prioritize security vulnerabilities\"},\n      {\"label\": \"Performance focus\", \"description\": \"Focus on performance implications\"},\n      {\"label\": \"Quick review\", \"description\": \"High-level review, skip deep analysis\"}\n    ],\n    \"multiSelect\": false\n  }]\n)\n```\n\n**Based on answer, adjust workflow:**\n- **Full review**: All 6-7 parallel agents\n- **Security focus**: Prioritize security-auditor, reduce other agents\n- **Performance focus**: Add performance-engineer agent\n- **Quick review**: Single code-quality-reviewer agent only\n\n---\n\n## STEP 0b: Select Orchestration Mode\n\nChoose **Agent Teams** (mesh ‚Äî reviewers cross-reference findings) or **Task tool** (star ‚Äî all report to lead):\n\n1. `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1` ‚Üí **Agent Teams mode**\n2. Agent Teams unavailable ‚Üí **Task tool mode** (default)\n3. Otherwise: Full review with 6+ agents and cross-cutting concerns ‚Üí recommend **Agent Teams**; Quick/focused review ‚Üí **Task tool**\n\n| Aspect | Task Tool | Agent Teams |\n|--------|-----------|-------------|\n| Communication | All reviewers report to lead | Reviewers cross-reference findings |\n| Security + quality overlap | Lead deduplicates | security-auditor messages code-quality-reviewer directly |\n| Cost | ~200K tokens | ~500K tokens |\n| Best for | Quick/focused reviews | Full reviews with cross-cutting concerns |\n\n> **Fallback:** If Agent Teams encounters issues, fall back to Task tool for remaining review.\n\n---\n\n## ‚ö†Ô∏è CRITICAL: Task Management is MANDATORY (CC 2.1.16)\n\n**BEFORE doing ANYTHING else, create tasks to track progress:**\n\n```python\n# 1. Create main review task IMMEDIATELY\nTaskCreate(\n  subject=\"Review PR #{number}\",\n  description=\"Comprehensive code review with parallel agents\",\n  activeForm=\"Reviewing PR #{number}\"\n)\n\n# 2. Create subtasks for each phase\nTaskCreate(subject=\"Gather PR information\", activeForm=\"Gathering PR information\")\nTaskCreate(subject=\"Launch review agents\", activeForm=\"Dispatching review agents\")\nTaskCreate(subject=\"Run validation checks\", activeForm=\"Running validation checks\")\nTaskCreate(subject=\"Synthesize review\", activeForm=\"Synthesizing review\")\nTaskCreate(subject=\"Submit review\", activeForm=\"Submitting review\")\n\n# 3. Update status as you progress\nTaskUpdate(taskId=\"2\", status=\"in_progress\")  # When starting\nTaskUpdate(taskId=\"",
    "contentTruncated": true
  },
  "right-sized-backend": {
    "content": "# Right-Sized Backend Architecture\n\nStop defaulting to enterprise patterns for interview take-homes. Stop using flat files for production SaaS. This skill provides context-aware architecture recommendations calibrated to project scope.\n\n## The Problem\n\nAI coding assistants and developers share the same failure mode: applying the architecture they know best regardless of context. A senior engineer's muscle memory says \"clean architecture, repository pattern, domain events\" -- but that is the wrong answer for a 4-hour take-home. Conversely, a startup founder's \"just ship it\" instinct produces unmaintainable code when the product finds traction.\n\n## Architecture Sizing Matrix\n\n| Signal | Flat/Simple | Layered | Clean/Hexagonal |\n|--------|-------------|---------|-----------------|\n| **Timeline** | Hours to days | Weeks to months | Months to years |\n| **Team size** | 1 developer | 2-5 developers | 5+ developers |\n| **Expected lifespan** | Disposable / demo | 1-3 years | 3+ years |\n| **Domain complexity** | CRUD, single entity | 3-10 entities, some rules | Complex invariants, multiple bounded contexts |\n| **Compliance** | None | Basic (HTTPS, hashing) | SOC2, HIPAA, PCI |\n| **Users** | < 100 | 100 - 10,000 | 10,000+ |\n| **LOC estimate** | 200-800 | 1,000-10,000 | 10,000+ |\n\n## Decision Flowchart\n\n```\nIs this a take-home or hackathon?\n  YES --> Flat architecture. Single file or 3-5 files. Done.\n  NO  -->\n\nIs this a prototype or MVP with < 3 months runway?\n  YES --> Simple layered. Routes + services + models. No abstractions.\n  NO  -->\n\nDo you have > 5 engineers or complex domain rules?\n  YES --> Clean architecture with ports/adapters.\n  NO  --> Layered architecture. Add abstractions only when pain appears.\n```\n\n## Scenario Guides\n\nEach scenario has a detailed reference file with file trees, code examples, LOC estimates, and explicit over-engineering traps.\n\n| Scenario | Reference | Right-Sized LOC | Over-Engineered LOC |\n|----------|-----------|-----------------|---------------------|\n| Interview take-home | [interview-takehome.md](references/interview-takehome.md) | 300-600 | 2,000-4,000 |\n| Startup MVP / SaaS | [startup-mvp.md](references/startup-mvp.md) | 2,000-5,000 | 15,000-30,000 |\n| Enterprise migration | [enterprise.md](references/enterprise.md) | Varies | N/A (complexity is justified) |\n| Open source CLI tool | [oss-cli-plugin.md](references/oss-cli-plugin.md) | 500-2,000 | 5,000-10,000 |\n\n## Quick Reference: What to Use When\n\n### ORM Approach\n\n| Context | Recommendation | Anti-Pattern |\n|---------|---------------|--------------|\n| Interview | Raw SQL or simple ORM (SQLModel, Prisma) | Repository + Unit of Work + generic base |\n| MVP | Simple ORM with models in same file as routes | Abstract repository protocol + factory |\n| Production | ORM with repository pattern per aggregate | Every table gets its own repository class |\n| Enterprise | Full repository + Unit of Work | Over-abstracting simple lookups |\n\n### Authentication\n\n| Context | Recomm",
    "contentTruncated": true
  },
  "root-cause-analysis": {
    "content": "# Root Cause Analysis\n\nSystematic approaches for identifying the true source of problems, not just symptoms.\n\n## RCA Methods Overview\n\n| Method | Best For | Complexity | Time |\n|--------|----------|------------|------|\n| 5 Whys | Simple, linear problems | Low | 15-30 min |\n| Fishbone | Multi-factor problems | Medium | 30-60 min |\n| Fault Tree | Critical systems, safety | High | 1-4 hours |\n| Timeline Analysis | Incident investigation | Medium | 30-90 min |\n\n## 5 Whys Method\n\nIteratively ask \"why\" to drill down from symptom to root cause.\n\n### Process\n\n```\nProblem Statement: [Clear description of the issue]\n    ‚îÇ\n    ‚ñº\nWhy #1: [First level cause]\n    ‚îÇ\n    ‚ñº\nWhy #2: [Deeper cause]\n    ‚îÇ\n    ‚ñº\nWhy #3: [Even deeper]\n    ‚îÇ\n    ‚ñº\nWhy #4: [Getting to root]\n    ‚îÇ\n    ‚ñº\nWhy #5: [Root cause identified]\n    ‚îÇ\n    ‚ñº\nAction: [Fix that addresses root cause]\n```\n\n### Example: Production Outage\n\n```markdown\n**Problem:** Website was down for 2 hours\n\n**Why 1:** Why was the website down?\n‚Üí The application server ran out of memory and crashed.\n\n**Why 2:** Why did the server run out of memory?\n‚Üí A memory leak in the image processing service accumulated over time.\n\n**Why 3:** Why was there a memory leak?\n‚Üí The service wasn't releasing image buffers after processing.\n\n**Why 4:** Why weren't buffers being released?\n‚Üí The cleanup code had a bug introduced in last week's release.\n\n**Why 5:** Why wasn't the bug caught before release?\n‚Üí We don't have automated memory leak detection in our test suite.\n\n**Root Cause:** Missing automated memory leak testing\n**Action:** Add memory profiling to CI pipeline, add cleanup tests\n```\n\n### 5 Whys Best Practices\n\n| Do | Don't |\n|----|-------|\n| Base answers on evidence | Guess or assume |\n| Stay focused on one causal chain | Branch too early |\n| Keep asking until actionable | Stop at symptoms |\n| Involve people closest to issue | Assign blame |\n| Document your reasoning | Skip steps |\n\n### When 5 Whys Falls Short\n\n- Multiple contributing factors (use Fishbone)\n- Complex system interactions (use Fault Tree)\n- Organizational/process issues (need broader analysis)\n\n## Fishbone Diagram (Ishikawa)\n\nVisualize multiple potential causes organized by category.\n\n### Standard Categories (6 M's)\n\n```\n                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n        Methods ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ\n                    ‚îÇ             ‚îÇ\n      Machines ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ\n                    ‚îÇ             ‚îú‚îÄ‚îÄ‚îÄ‚îÄ PROBLEM\n     Materials ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ\n                    ‚îÇ             ‚îÇ\n    Measurement ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ\n                    ‚îÇ             ‚îÇ\n    Environment ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ\n                    ‚îÇ             ‚îÇ\n       People ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ\n                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Software-Specific Categories\n\n```\n                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n          Code ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ\n                    ‚îÇ             ‚îÇ\n Infrastructure ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ\n                    ‚îÇ             ‚îú‚îÄ‚îÄ‚îÄ‚îÄ BUG/INCIDENT\n   Dependencies ‚îÄ‚îÄ‚îÄ‚îÄ‚î§             ‚îÇ\n   ",
    "contentTruncated": true
  },
  "run-tests": {
    "content": "# Run Tests\n\nTest execution with parallel analysis agents for failures.\n\n## Quick Start\n\n```bash\n/run-tests\n/run-tests backend\n/run-tests frontend\n/run-tests tests/unit/test_auth.py\n```\n\n## Test Scope\n\n| Argument | Scope |\n|----------|-------|\n| Empty/`all` | All tests |\n| `backend` | Backend only |\n| `frontend` | Frontend only |\n| `path/to/test.py` | Specific file |\n| `test_name` | Specific test |\n\n## Phase 1: Execute Tests\n\n```bash\n# Backend with coverage\ncd backend\npoetry run pytest tests/unit/ -v --tb=short \\\n  --cov=app --cov-report=term-missing\n\n# Frontend with coverage\ncd frontend\nnpm run test -- --coverage\n```\n\n## Phase 2: Failure Analysis\n\nIf tests fail, launch 3 parallel analyzers:\n1. **Backend Failure Analysis** - Root cause, fix suggestions\n2. **Frontend Failure Analysis** - Component issues, mock problems\n3. **Coverage Gap Analysis** - Low coverage areas\n\n## Phase 3: Generate Report\n\n```markdown\n# Test Results Report\n\n## Summary\n| Suite | Total | Passed | Failed | Coverage |\n|-------|-------|--------|--------|----------|\n| Backend | X | Y | Z | XX% |\n| Frontend | X | Y | Z | XX% |\n\n## Status: [ALL PASS | SOME FAILURES]\n\n## Failures (if any)\n| Test | Error | Fix |\n|------|-------|-----|\n| test_name | AssertionError | [suggestion] |\n```\n\n## Quick Commands\n\n```bash\n# All backend tests\npoetry run pytest tests/unit/ -v --tb=short\n\n# With coverage\npoetry run pytest tests/unit/ --cov=app\n\n# Quick (no tracebacks)\npoetry run pytest tests/unit/ --tb=no -q\n\n# Specific test\npoetry run pytest tests/unit/ -k \"test_name\" -v\n\n# Frontend\nnpm run test -- --coverage\n\n# Watch mode\nnpm run test -- --watch\n```\n\n## Key Options\n\n| Option | Purpose |\n|--------|---------|\n| `--maxfail=3` | Stop after 3 failures |\n| `-x` | Stop on first failure |\n| `--lf` | Run only last failed |\n| `-v` | Verbose output |\n| `--tb=short` | Shorter tracebacks |\n\n## Related Skills\n\n- `testing-patterns` - Comprehensive testing patterns and best practices\n\n## Key Decisions\n\n| Decision | Choice | Rationale |\n|----------|--------|-----------|\n| Parallel Analyzers | 3 agents | Backend, frontend, and coverage analysis in parallel |\n| Default Traceback | `--tb=short` | Balance between detail and readability |\n| Stop Threshold | `--maxfail=3` | Quick feedback without overwhelming output |\n| Coverage Tool | pytest-cov / jest | Native integration with test frameworks |\n\n## References\n\n- [Test Commands](references/test-commands.md)",
    "contentTruncated": false
  },
  "security-patterns": {
    "content": "# Security Patterns\n\nComprehensive security patterns for building hardened applications. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Authentication](#authentication) | 3 | CRITICAL | JWT tokens, OAuth 2.1/PKCE, RBAC/permissions |\n| [Defense-in-Depth](#defense-in-depth) | 2 | CRITICAL | Multi-layer security, zero-trust architecture |\n| [Input Validation](#input-validation) | 3 | HIGH | Schema validation (Zod/Pydantic), output encoding, file uploads |\n| [OWASP Top 10](#owasp-top-10) | 2 | CRITICAL | Injection prevention, broken authentication fixes |\n| [LLM Safety](#llm-safety) | 3 | HIGH | Prompt injection defense, output guardrails, content filtering |\n| [PII Masking](#pii-masking) | 2 | HIGH | PII detection/redaction with Presidio, Langfuse, LLM Guard |\n\n**Total: 15 rules across 6 categories**\n\n## Quick Start\n\n```python\n# Argon2id password hashing\nfrom argon2 import PasswordHasher\nph = PasswordHasher()\npassword_hash = ph.hash(password)\nph.verify(password_hash, password)\n```\n\n```python\n# JWT access token (15-min expiry)\nimport jwt\nfrom datetime import datetime, timedelta, timezone\npayload = {\n    'sub': user_id, 'type': 'access',\n    'exp': datetime.now(timezone.utc) + timedelta(minutes=15),\n}\ntoken = jwt.encode(payload, SECRET_KEY, algorithm='HS256')\n```\n\n```typescript\n// Zod v4 schema validation\nimport { z } from 'zod';\nconst UserSchema = z.object({\n  email: z.string().email(),\n  name: z.string().min(2).max(100),\n  role: z.enum(['user', 'admin']).default('user'),\n});\nconst result = UserSchema.safeParse(req.body);\n```\n\n```python\n# PII masking with Langfuse\nimport re\nfrom langfuse import Langfuse\n\ndef mask_pii(data, **kwargs):\n    if isinstance(data, str):\n        data = re.sub(r'\\b[\\w.-]+@[\\w.-]+\\.\\w+\\b', '[REDACTED_EMAIL]', data)\n        data = re.sub(r'\\b\\d{3}-\\d{2}-\\d{4}\\b', '[REDACTED_SSN]', data)\n    return data\n\nlangfuse = Langfuse(mask=mask_pii)\n```\n\n## Authentication\n\nSecure authentication with OAuth 2.1, Passkeys/WebAuthn, JWT tokens, and role-based access control.\n\n| Rule | Description |\n|------|-------------|\n| `auth-jwt.md` | JWT creation, verification, expiry, refresh token rotation |\n| `auth-oauth.md` | OAuth 2.1 with PKCE, DPoP, Passkeys/WebAuthn |\n| `auth-rbac.md` | Role-based access control, permission decorators, MFA |\n\n**Key Decisions:** Argon2id > bcrypt | Access tokens 15 min | PKCE required | Passkeys > TOTP > SMS\n\n## Defense-in-Depth\n\nMulti-layer security architecture with no single point of failure.\n\n| Rule | Description |\n|------|-------------|\n| `defense-layers.md` | 8-layer security architecture (edge to observability) |\n| `defense-zero-trust.md` | Immutable request context, tenant isolation, audit logging |\n\n**Key Decisions:** Immutable dataclass context | Query-level tenant filtering | No IDs in LLM prompts\n\n## Input Validation\n\nValidate and sanitize all untrusted input using Zod v",
    "contentTruncated": true
  },
  "security-scanning": {
    "content": "# Security Scanning\n\nAutomate vulnerability detection in code and dependencies.\n\n## Dependency Scanning\n\n### JavaScript (npm)\n\n```bash\n# Run audit\nnpm audit --json > security-audit.json\n\n# Check severity counts\nCRITICAL=$(npm audit --json | jq '.metadata.vulnerabilities.critical')\nHIGH=$(npm audit --json | jq '.metadata.vulnerabilities.high')\n\nif [ \"$CRITICAL\" -gt 0 ] || [ \"$HIGH\" -gt 0 ]; then\n  echo \"üö® $CRITICAL critical, $HIGH high vulnerabilities\"\nfi\n\n# Auto-fix\nnpm audit fix\n```\n\n### Python (pip-audit)\n\n```bash\npip-audit --format=json > security-audit.json\n\n# Using safety\nsafety check --json > security-audit.json\n```\n\n## Static Analysis (SAST)\n\n### Semgrep\n\n```bash\n# Run with security rules\nsemgrep --config=auto --json > semgrep-results.json\n\n# Count findings\nCRITICAL=$(cat semgrep-results.json | jq '[.results[] | select(.extra.severity == \"ERROR\")] | length')\n```\n\n### Bandit (Python)\n\n```bash\nbandit -r . -f json -o bandit-report.json\n\nHIGH=$(cat bandit-report.json | jq '[.results[] | select(.issue_severity == \"HIGH\")] | length')\n```\n\n## Secret Detection\n\n```bash\n# TruffleHog\ntrufflehog git file://. --json > secrets-scan.json\n\n# Gitleaks\ngitleaks detect --source . --report-format json\n\n# Check results\nSECRET_COUNT=$(cat secrets-scan.json | jq '. | length')\nif [ \"$SECRET_COUNT\" -gt 0 ]; then\n  echo \"üö® $SECRET_COUNT secrets detected!\"\nfi\n```\n\n## Container Scanning\n\n```bash\n# Trivy\ntrivy image myapp:latest --format json > trivy-scan.json\n\nCRITICAL=$(cat trivy-scan.json | jq '[.Results[].Vulnerabilities[]? | select(.Severity == \"CRITICAL\")] | length')\n```\n\n## Pre-commit Hooks (2026 Best Practice)\n\nShift-left security by catching issues before commit:\n\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  # Secret detection - MUST HAVE\n  - repo: https://github.com/gitleaks/gitleaks\n    rev: v8.18.0\n    hooks:\n      - id: gitleaks\n\n  # Python security\n  - repo: https://github.com/PyCQA/bandit\n    rev: 1.7.7\n    hooks:\n      - id: bandit\n        args: [\"-c\", \"pyproject.toml\", \"-r\", \".\"]\n        exclude: ^tests/\n\n  # Semgrep for SAST\n  - repo: https://github.com/semgrep/semgrep\n    rev: v1.52.0\n    hooks:\n      - id: semgrep\n        args: [\"--config\", \"auto\", \"--error\"]\n\n  # Detect AWS credentials, private keys\n  - repo: https://github.com/Yelp/detect-secrets\n    rev: v1.4.0\n    hooks:\n      - id: detect-secrets\n        args: [\"--baseline\", \".secrets.baseline\"]\n```\n\n```bash\n# Install and setup\npip install pre-commit\npre-commit install\n\n# Run on all files (first time)\npre-commit run --all-files\n\n# Update hooks to latest versions\npre-commit autoupdate\n```\n\n**Baseline for detect-secrets (ignore false positives):**\n```bash\n# Generate baseline\ndetect-secrets scan > .secrets.baseline\n\n# Audit false positives\ndetect-secrets audit .secrets.baseline\n```\n\n## CI Integration\n\n```yaml\n# GitHub Actions\n- name: Security scan\n  run: |\n    npm audit --json > audit.json\n    CRITICAL=$(jq '.metadata.vulnerabilities.critical' audit.json)\n    if [ \"$CRITICAL\" -gt 0 ]; then\n     ",
    "contentTruncated": true
  },
  "silent-failure-detection": {
    "content": "# Silent Failure Detection\n\nDetect when LLM agents fail silently - appearing to work while producing incorrect results.\n\n## Overview\n\n- Detecting when agents skip expected tool calls\n- Identifying gibberish or degraded output quality\n- Monitoring for infinite loops and token consumption spikes\n- Setting up statistical baselines for anomaly detection\n- Alerting on non-error failures (service up but logic broken)\n\n## Quick Reference\n\n### Tool Skipping Detection\n\n```python\nfrom langfuse import Langfuse\n\ndef check_tool_usage(trace_id: str, expected_tools: list[str]) -> dict:\n    \"\"\"\n    Detect when agent skips expected tool calls.\n\n    Based on Akamai's middleware bug: agents stopped using tools\n    when hidden middleware injected unexpected instructions.\n    \"\"\"\n    langfuse = Langfuse()\n    trace = langfuse.fetch_trace(trace_id)\n\n    # Extract tool calls from trace\n    actual_tools = [\n        span.name for span in trace.observations\n        if span.type == \"tool\"\n    ]\n\n    missing_tools = set(expected_tools) - set(actual_tools)\n\n    if missing_tools:\n        return {\n            \"alert\": True,\n            \"type\": \"tool_skipping\",\n            \"missing\": list(missing_tools),\n            \"message\": f\"Agent skipped expected tools: {missing_tools}\"\n        }\n    return {\"alert\": False}\n```\n\n### Gibberish/Quality Detection\n\n```python\nfrom langfuse.decorators import observe, langfuse_context\n\n@observe(name=\"quality_check\")\nasync def detect_gibberish(response: str) -> dict:\n    \"\"\"\n    Detect low-quality or gibberish outputs using LLM-as-judge.\n    \"\"\"\n    # Quick heuristics first\n    if len(response) < 10:\n        return {\"alert\": True, \"type\": \"too_short\"}\n\n    if len(set(response.split())) / len(response.split()) < 0.3:\n        return {\"alert\": True, \"type\": \"repetitive\"}\n\n    # LLM-as-judge for quality\n    judge_prompt = f\"\"\"\n    Rate this response quality (0-1):\n    - 0: Gibberish, nonsensical, or completely wrong\n    - 0.5: Partially correct but missing key information\n    - 1: High quality, accurate, complete\n\n    Response: {response[:1000]}\n\n    Score (just the number):\n    \"\"\"\n\n    score = await llm.generate(judge_prompt)\n    score_value = float(score.strip())\n\n    langfuse_context.score(name=\"quality_check\", value=score_value)\n\n    if score_value < 0.5:\n        return {\"alert\": True, \"type\": \"low_quality\", \"score\": score_value}\n    return {\"alert\": False, \"score\": score_value}\n```\n\n### Loop Detection\n\n```python\nclass LoopDetector:\n    \"\"\"Detect infinite loops and token consumption spikes.\"\"\"\n\n    def __init__(\n        self,\n        max_iterations: int = 10,\n        token_spike_multiplier: float = 3.0,\n        baseline_tokens: int = 2000\n    ):\n        self.max_iterations = max_iterations\n        self.token_spike_multiplier = token_spike_multiplier\n        self.baseline_tokens = baseline_tokens\n        self.iteration_count = 0\n        self.total_tokens = 0\n\n    def check(self, tokens_used: int) -> dict:\n        self.iteration_count += 1\n        ",
    "contentTruncated": true
  },
  "skill-analyzer": {
    "content": "# Skill Analyzer\n\nReference patterns for extracting structured metadata from SKILL.md files.\n\n> **Note**: Actual analysis is performed by `demo-producer/scripts/generate.sh`. This skill provides reference patterns.\n\n## Output Structure\n\n```typescript\ninterface SkillMetadata {\n  name: string;\n  description: string;\n  tags: string[];\n  version: string;\n  userInvocable: boolean;\n  context: 'fork' | 'inherit' | 'none';\n\n  // Extracted content\n  phases: WorkflowPhase[];\n  examples: CodeExample[];\n  keyFeatures: string[];\n  relatedSkills: string[];\n}\n\ninterface WorkflowPhase {\n  name: string;\n  description: string;\n  tools: string[];\n  isParallel: boolean;\n}\n\ninterface CodeExample {\n  language: string;\n  code: string;\n  description: string;\n}\n```\n\n## Extraction Rules\n\n### Frontmatter Parsing (Bash)\n```bash\n# Extract name\nname=$(grep \"^name:\" SKILL.md | head -1 | cut -d: -f2- | xargs)\n\n# Extract description\ndescription=$(grep \"^description:\" SKILL.md | head -1 | cut -d: -f2- | xargs)\n\n# Extract tags\ntags=$(grep \"^tags:\" SKILL.md | sed 's/tags: \\[//' | sed 's/\\]//' | tr -d '\"')\n```\n\n### Phase Detection\n- Look for `## Phase N:` or `### Phase N:` headers\n- Extract tools from code blocks (Grep, Glob, Read, Task, etc.)\n- Detect parallel execution from \"PARALLEL\" comments or multiple tool calls\n\n### Example Detection\n- Find code blocks with language tags\n- Extract surrounding context as description\n- Identify quick start examples\n\n### Feature Detection\n- Parse bullet points after \"Key Features\" or \"What it does\"\n- Extract from description field\n- Identify from tags\n\n## Usage in Demo Pipeline\n\n```bash\n# Integrated into demo-producer\n./skills/demo-producer/scripts/generate.sh skill explore\n\n# Internally calls extraction functions to:\n# 1. Parse SKILL.md frontmatter\n# 2. Extract phases from ## headers\n# 3. Identify related skills\n# 4. Generate demo script with extracted content\n```\n\n## Related Skills\n\n- `demo-producer`: Uses skill-analyzer output for script generation\n- `video-production`: Recording, composition, and content recipes for demos\n\n## References\n\nSee `references/` for detailed extraction patterns:\n- `frontmatter-parsing.md` - YAML frontmatter extraction\n- `phase-extraction.md` - Workflow phase detection",
    "contentTruncated": false
  },
  "skill-evolution": {
    "content": "# Skill Evolution Manager\n\nEnables skills to automatically improve based on usage patterns, user edits, and success rates. Provides version control with safe rollback capability.\n\n## Overview\n\n- Reviewing how skills are performing across sessions\n- Identifying patterns in user edits to skill outputs\n- Applying learned improvements to skill templates\n- Rolling back problematic skill changes\n- Tracking skill version history and success rates\n\n## Quick Reference\n\n| Command | Description |\n|---------|-------------|\n| `/ork:skill-evolution` | Show evolution report for all skills |\n| `/ork:skill-evolution analyze <skill-id>` | Analyze specific skill patterns |\n| `/ork:skill-evolution evolve <skill-id>` | Review and apply suggestions |\n| `/ork:skill-evolution history <skill-id>` | Show version history |\n| `/ork:skill-evolution rollback <skill-id> <version>` | Restore previous version |\n\n---\n\n## How It Works\n\nThe skill evolution system operates in three phases:\n\n```\nCOLLECT                    ANALYZE                    ACT\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                    ‚îÄ‚îÄ‚îÄ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ PostTool    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Evolution   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ /ork:skill  ‚îÇ\n‚îÇ Edit        ‚îÇ  patterns ‚îÇ Analyzer    ‚îÇ suggest   ‚îÇ evolve      ‚îÇ\n‚îÇ Tracker     ‚îÇ           ‚îÇ Engine      ‚îÇ           ‚îÇ command     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n     ‚îÇ                          ‚îÇ                          ‚îÇ\n     ‚ñº                          ‚ñº                          ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ edit-       ‚îÇ           ‚îÇ evolution-  ‚îÇ           ‚îÇ versions/   ‚îÇ\n‚îÇ patterns.   ‚îÇ           ‚îÇ registry.   ‚îÇ           ‚îÇ snapshots   ‚îÇ\n‚îÇ jsonl       ‚îÇ           ‚îÇ json        ‚îÇ           ‚îÇ             ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Edit Pattern Categories\n\nThe system tracks these common edit patterns:\n\n| Pattern | Description | Detection |\n|---------|-------------|-----------|\n| `add_pagination` | User adds pagination to API responses | `limit.*offset`, `cursor.*pagination` |\n| `add_rate_limiting` | User adds rate limiting | `rate.?limit`, `throttl` |\n| `add_error_handling` | User adds try/catch blocks | `try.*catch`, `except` |\n| `add_types` | User adds TypeScript/Python types | `interface\\s`, `Optional` |\n| `add_validation` | User adds input validation | `validate`, `Pydantic`, `Zod` |\n| `add_logging` | User adds logging/observability | `logger\\.`, `console.log` |\n| `remove_comments` | User removes generated comments | Pattern removal detection |\n| `add_auth_check` | User adds authentication checks | `@auth`, `@require_auth` |\n\n### Suggestion Thresholds\n\n| Threshold | Default | Description |\n|-----------|---------|-------------|\n| Minimum Samples | 5 | Uses before generating suggestions |\n| Add Threshold | 70% | Frequency to suggest adding pattern |\n| Auto-Apply Confidence | 85% | Confidence for auto-applicat",
    "contentTruncated": true
  },
  "stacked-prs": {
    "content": "# Stacked PRs\n\nBreak large features into small, reviewable PRs that depend on each other. Merge in order for clean history.\n\n## Quick Reference\n\n```\nmain ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè\n                                              /\nPR #3 (final)  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚Üê Merge last\n                                       /\nPR #2 (middle) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îò          ‚Üê Depends on #1\n                                  /\nPR #1 (base)   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ                ‚Üê Merge first\n                              /\nfeature/auth ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ‚óè                    ‚Üê Development\n```\n\n---\n\n## Workflow\n\n### 1. Plan the Stack\n\n```bash\n# Identify logical chunks\n# Example: Auth feature\n# PR 1: Add User model + migrations\n# PR 2: Add auth service + tests\n# PR 3: Add login UI + integration tests\n```\n\n### 2. Create Base Branch\n\n```bash\ngit checkout main\ngit pull origin main\ngit checkout -b feature/auth-base\n\n# Implement first chunk\ngit add -p\ngit commit -m \"feat(#100): Add User model\"\ngit commit -m \"feat(#100): Add user migrations\"\n\n# Push and create first PR\ngit push -u origin feature/auth-base\ngh pr create --base main --title \"feat(#100): Add User model [1/3]\" \\\n  --body \"## Stack\n- PR 1/3: User model (this PR)\n- PR 2/3: Auth service (depends on this)\n- PR 3/3: Login UI (depends on #2)\n\n## Changes\n- Add User model with validation\n- Add database migrations\"\n```\n\n### 3. Stack Next PR\n\n```bash\n# Branch from first PR's branch (not main!)\ngit checkout -b feature/auth-service\n\n# Implement second chunk\ngit add -p\ngit commit -m \"feat(#100): Add auth service\"\ngit commit -m \"test(#100): Add auth service tests\"\n\n# Push and create PR targeting FIRST branch\ngit push -u origin feature/auth-service\ngh pr create --base feature/auth-base \\\n  --title \"feat(#100): Add auth service [2/3]\" \\\n  --body \"## Stack\n- PR 1/3: User model (#101)\n- PR 2/3: Auth service (this PR)\n- PR 3/3: Login UI (depends on this)\n\n**Depends on #101** - merge that first\"\n```\n\n### 4. Continue Stacking\n\n```bash\ngit checkout -b feature/auth-ui\n\n# Implement third chunk\ngit commit -m \"feat(#100): Add login form\"\ngit commit -m \"test(#100): Add login integration tests\"\n\ngit push -u origin feature/auth-ui\ngh pr create --base feature/auth-service \\\n  --title \"feat(#100): Add login UI [3/3]\"\n```\n\n---\n\n## Managing the Stack\n\n### When Base PR Gets Feedback\n\n```bash\n# Make changes to base PR\ngit checkout feature/auth-base\ngit add -p\ngit commit -m \"fix: Address review feedback\"\ngit push\n\n# Rebase dependent PRs\ngit checkout feature/auth-service\ngit rebase feature/auth-base\ngit push --force-with-lease\n\ngit checkout feature/auth-ui\ngit rebase feature/auth-service\ngit push --force-with-lease\n```\n\n### When Base PR Merges\n\n```bash\n# After PR #1 merges to main\ngit checkout main\ngit pull origin main\n\n# Update PR #2 to target main now\ngh pr edit 102 --base main\n\n# Rebase PR #2 on main\ngit checkout feature/auth-service\ngit rebase main\ngit push --force-with-lease\n\n# Repeat for PR #3 after #2 merges",
    "contentTruncated": true
  },
  "strawberry-graphql": {
    "content": "# Strawberry GraphQL Patterns\n\nType-safe GraphQL in Python with code-first schema definition.\n\n## Overview\n\n- Complex data relationships (nested queries, multiple entities)\n- Client-driven data fetching (mobile apps, SPAs)\n- Real-time features (subscriptions for live updates)\n- Federated microservice architecture\n\n## When NOT to Use\n\n- Simple CRUD APIs (REST is simpler)\n- Internal microservice communication (use gRPC)\n\n## Schema Definition\n\n```python\nimport strawberry\nfrom datetime import datetime\nfrom strawberry import Private\n\n@strawberry.enum\nclass UserStatus:\n    ACTIVE = \"active\"\n    INACTIVE = \"inactive\"\n\n@strawberry.type\nclass User:\n    id: strawberry.ID\n    email: str\n    name: str\n    status: UserStatus\n    password_hash: Private[str]  # Not exposed in schema\n\n    @strawberry.field\n    def display_name(self) -> str:\n        return f\"{self.name} ({self.email})\"\n\n    @strawberry.field\n    async def posts(self, info: strawberry.Info, limit: int = 10) -> list[\"Post\"]:\n        return await info.context.post_loader.load_by_user(self.id, limit)\n\n@strawberry.type\nclass Post:\n    id: strawberry.ID\n    title: str\n    content: str\n    author_id: strawberry.ID\n\n    @strawberry.field\n    async def author(self, info: strawberry.Info) -> User:\n        return await info.context.user_loader.load(self.author_id)\n\n@strawberry.input\nclass CreateUserInput:\n    email: str\n    name: str\n    password: str\n```\n\n## Query and Mutation\n\n```python\n@strawberry.type\nclass Query:\n    @strawberry.field\n    async def user(self, info: strawberry.Info, id: strawberry.ID) -> User | None:\n        return await info.context.user_service.get(id)\n\n    @strawberry.field\n    async def me(self, info: strawberry.Info) -> User | None:\n        user_id = info.context.current_user_id\n        return await info.context.user_service.get(user_id) if user_id else None\n\n@strawberry.type\nclass Mutation:\n    @strawberry.mutation\n    async def create_user(self, info: strawberry.Info, input: CreateUserInput) -> User:\n        return await info.context.user_service.create(\n            email=input.email, name=input.name, password=input.password\n        )\n\n    @strawberry.mutation\n    async def delete_user(self, info: strawberry.Info, id: strawberry.ID) -> bool:\n        await info.context.user_service.delete(id)\n        return True\n```\n\n## DataLoader (N+1 Prevention)\n\n```python\nfrom strawberry.dataloader import DataLoader\n\nclass UserLoader(DataLoader[str, User]):\n    def __init__(self, user_repo):\n        super().__init__(load_fn=self.batch_load)\n        self.user_repo = user_repo\n\n    async def batch_load(self, keys: list[str]) -> list[User]:\n        users = await self.user_repo.get_many(keys)\n        user_map = {u.id: u for u in users}\n        return [user_map.get(key) for key in keys]\n\nclass GraphQLContext:\n    def __init__(self, request, user_service, user_repo, post_repo):\n        self.request = request\n        self.user_service = user_service\n        self.user_loader = UserLoader(user_repo)\n   ",
    "contentTruncated": true
  },
  "streaming-api-patterns": {
    "content": "# Streaming API Patterns\n\n## Overview\n\n**When to use this skill:**\n- Streaming LLM responses (ChatGPT-style interfaces)\n- Real-time notifications and updates\n- Live data feeds (stock prices, analytics)\n- Chat applications\n- Progress updates for long-running tasks\n- Collaborative editing features\n\n## Core Technologies\n\n### 1. Server-Sent Events (SSE)\n\n**Best for**: Server-to-client streaming (LLM responses, notifications)\n\n```typescript\n// Next.js Route Handler\nexport async function GET(req: Request) {\n  const encoder = new TextEncoder()\n\n  const stream = new ReadableStream({\n    async start(controller) {\n      // Send data\n      controller.enqueue(encoder.encode('data: Hello\\n\\n'))\n\n      // Keep connection alive\n      const interval = setInterval(() => {\n        controller.enqueue(encoder.encode(': keepalive\\n\\n'))\n      }, 30000)\n\n      // Cleanup\n      req.signal.addEventListener('abort', () => {\n        clearInterval(interval)\n        controller.close()\n      })\n    }\n  })\n\n  return new Response(stream, {\n    headers: {\n      'Content-Type': 'text/event-stream',\n      'Cache-Control': 'no-cache',\n      'Connection': 'keep-alive',\n    }\n  })\n}\n\n// Client\nconst eventSource = new EventSource('/api/stream')\neventSource.onmessage = (event) => {\n  console.log(event.data)\n}\n```\n\n### 2. WebSockets\n\n**Best for**: Bidirectional real-time communication (chat, collaboration)\n\n```typescript\n// WebSocket Server (Next.js with ws)\nimport { WebSocketServer } from 'ws'\n\nconst wss = new WebSocketServer({ port: 8080 })\n\nwss.on('connection', (ws) => {\n  ws.on('message', (data) => {\n    // Broadcast to all clients\n    wss.clients.forEach((client) => {\n      if (client.readyState === WebSocket.OPEN) {\n        client.send(data)\n      }\n    })\n  })\n})\n\n// Client\nconst ws = new WebSocket('ws://localhost:8080')\nws.onmessage = (event) => console.log(event.data)\nws.send(JSON.stringify({ type: 'message', text: 'Hello' }))\n```\n\n### 3. ReadableStream API\n\n**Best for**: Processing large data streams with backpressure\n\n```typescript\nasync function* generateData() {\n  for (let i = 0; i < 1000; i++) {\n    await new Promise(resolve => setTimeout(resolve, 100))\n    yield \"data-\" + i\n  }\n}\n\nconst stream = new ReadableStream({\n  async start(controller) {\n    for await (const chunk of generateData()) {\n      controller.enqueue(new TextEncoder().encode(chunk + '\\n'))\n    }\n    controller.close()\n  }\n})\n```\n\n## LLM Streaming Pattern\n\n```typescript\n// Server\nimport OpenAI from 'openai'\n\nconst openai = new OpenAI()\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json()\n\n  const stream = await openai.chat.completions.create({\n    model: 'gpt-5.2',\n    messages,\n    stream: true\n  })\n\n  const encoder = new TextEncoder()\n\n  return new Response(\n    new ReadableStream({\n      async start(controller) {\n        for await (const chunk of stream) {\n          const content = chunk.choices[0]?.delta?.content\n          if (content) {\n            controller.enqueue(enco",
    "contentTruncated": true
  },
  "system-design-interrogation": {
    "content": "# System Design Interrogation\n\n## The Problem\n\nRushing to implementation without systematic design thinking leads to:\n- Scalability issues discovered too late\n- Security holes from missing tenant isolation\n- Data model mismatches\n- Frontend/backend contract conflicts\n- Poor user experience\n\n## The Solution: Question Before Implementing\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                    SYSTEM DESIGN INTERROGATION                             ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                                                            ‚îÇ\n‚îÇ                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                     ‚îÇ\n‚îÇ                        ‚îÇ   FEATURE   ‚îÇ                                     ‚îÇ\n‚îÇ                        ‚îÇ   REQUEST   ‚îÇ                                     ‚îÇ\n‚îÇ                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                     ‚îÇ\n‚îÇ                               ‚îÇ                                            ‚îÇ\n‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ\n‚îÇ    ‚îÇ                          ‚îÇ                          ‚îÇ                ‚îÇ\n‚îÇ    ‚ñº                          ‚ñº                          ‚ñº                ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ\n‚îÇ  ‚îÇ SCALE  ‚îÇ             ‚îÇ  DATA  ‚îÇ              ‚îÇSECURITY‚îÇ               ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ\n‚îÇ      ‚îÇ                      ‚îÇ                       ‚îÇ                     ‚îÇ\n‚îÇ  ‚Ä¢ Users?               ‚Ä¢ Where?               ‚Ä¢ Who access?              ‚îÇ\n‚îÇ  ‚Ä¢ Volume?              ‚Ä¢ Pattern?             ‚Ä¢ Isolation?               ‚îÇ\n‚îÇ  ‚Ä¢ Growth?              ‚Ä¢ Search?              ‚Ä¢ Attacks?                 ‚îÇ\n‚îÇ      ‚îÇ                      ‚îÇ                       ‚îÇ                     ‚îÇ\n‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ\n‚îÇ                             ‚îÇ                                             ‚îÇ\n‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ\n‚îÇ    ‚îÇ                        ‚îÇ                        ‚îÇ                   ‚îÇ\n‚îÇ    ‚ñº                        ‚ñº                        ‚ñº                   ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ\n‚îÇ  ‚îÇ   UX   ‚îÇ           ‚îÇCOHERENCE ‚îÇ            ‚îÇ TRADE- ‚îÇ                ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ  OFFS  ‚îÇ                ‚îÇ\n‚îÇ      ‚îÇ                     ‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ\n‚îÇ  ‚Ä¢ Latency?           ‚Ä¢ Contracts?           ‚Ä¢ Speed?                    ‚îÇ\n‚îÇ  ‚Ä¢ Feedback?          ‚Ä¢ Types?               ‚Ä¢ Quality?                  ‚îÇ\n‚îÇ  ‚Ä¢ Errors?            ‚Ä¢ API?                 ‚Ä¢ Cost?                     ‚îÇ\n‚îÇ      ‚îÇ                     ‚îÇ                      ‚îÇ                      ‚îÇ\n‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îÇ\n‚îÇ              ",
    "contentTruncated": true
  },
  "tanstack-query-advanced": {
    "content": "# TanStack Query Advanced\n\nProduction patterns for TanStack Query v5 - server state management done right.\n\n## Overview\n\n- Infinite scroll / pagination\n- Optimistic UI updates\n- Prefetching for instant navigation\n- Complex cache invalidation\n- Dependent/parallel queries\n- Mutations with rollback\n\n## Core Patterns\n\n### 1. Infinite Queries (Cursor-Based)\n\n```typescript\nimport { useInfiniteQuery } from '@tanstack/react-query';\n\ninterface Page {\n  items: Item[];\n  nextCursor: string | null;\n}\n\nfunction useInfiniteItems() {\n  return useInfiniteQuery({\n    queryKey: ['items'],\n    queryFn: async ({ pageParam }): Promise<Page> => {\n      const res = await fetch(`/api/items?cursor=${pageParam ?? ''}`);\n      return res.json();\n    },\n    initialPageParam: null as string | null,\n    getNextPageParam: (lastPage) => lastPage.nextCursor,\n    getPreviousPageParam: (firstPage) => firstPage.prevCursor,\n  });\n}\n\n// Component\nfunction ItemList() {\n  const { data, fetchNextPage, hasNextPage, isFetchingNextPage } = useInfiniteItems();\n\n  return (\n    <>\n      {data?.pages.flatMap((page) => page.items.map((item) => (\n        <ItemCard key={item.id} item={item} />\n      )))}\n      <button\n        onClick={() => fetchNextPage()}\n        disabled={!hasNextPage || isFetchingNextPage}\n      >\n        {isFetchingNextPage ? 'Loading...' : hasNextPage ? 'Load More' : 'No more'}\n      </button>\n    </>\n  );\n}\n```\n\n### 2. Optimistic Updates\n\n```typescript\nimport { useMutation, useQueryClient } from '@tanstack/react-query';\n\nfunction useUpdateTodo() {\n  const queryClient = useQueryClient();\n\n  return useMutation({\n    mutationFn: updateTodo,\n    onMutate: async (newTodo) => {\n      // Cancel outgoing refetches\n      await queryClient.cancelQueries({ queryKey: ['todos', newTodo.id] });\n\n      // Snapshot previous value\n      const previousTodo = queryClient.getQueryData(['todos', newTodo.id]);\n\n      // Optimistically update\n      queryClient.setQueryData(['todos', newTodo.id], newTodo);\n\n      // Return context for rollback\n      return { previousTodo };\n    },\n    onError: (err, newTodo, context) => {\n      // Rollback on error\n      queryClient.setQueryData(['todos', newTodo.id], context?.previousTodo);\n    },\n    onSettled: (data, error, variables) => {\n      // Always refetch after error or success\n      queryClient.invalidateQueries({ queryKey: ['todos', variables.id] });\n    },\n  });\n}\n```\n\n### 3. Prefetching Patterns\n\n```typescript\n// Prefetch on hover\nfunction UserLink({ userId }: { userId: string }) {\n  const queryClient = useQueryClient();\n\n  const prefetchUser = () => {\n    queryClient.prefetchQuery({\n      queryKey: ['user', userId],\n      queryFn: () => fetchUser(userId),\n      staleTime: 5 * 60 * 1000, // 5 minutes\n    });\n  };\n\n  return (\n    <Link to={`/users/${userId}`} onMouseEnter={prefetchUser}>\n      View User\n    </Link>\n  );\n}\n\n// Prefetch in loader (React Router)\nexport const loader = (queryClient: QueryClient) => async ({ params }) => {\n  await queryCl",
    "contentTruncated": true
  },
  "task-dependency-patterns": {
    "content": "# Task Dependency Patterns\n\n## Overview\n\nClaude Code 2.1.16 introduces a native Task Management System with four tools:\n- **TaskCreate**: Create new tasks with subject, description, and activeForm\n- **TaskUpdate**: Update status (pending ‚Üí in_progress ‚Üí completed), set dependencies\n- **TaskGet**: Retrieve full task details including blockers\n- **TaskList**: View all tasks with status and dependency summary\n\nTasks enable structured work tracking, parallel coordination, and clear progress visibility.\n\n## When to Use\n\n- Breaking down complex multi-step implementations\n- Coordinating parallel work across multiple files\n- Tracking progress on large features\n- Managing dependencies between related changes\n- Providing visibility into work status\n\n## Key Patterns\n\n### 1. Task Decomposition\n\nBreak complex work into atomic, trackable units:\n\n```\nFeature: Add user authentication\n\nTasks:\n#1. [pending] Create User model\n#2. [pending] Add auth endpoints (blockedBy: #1)\n#3. [pending] Implement JWT tokens (blockedBy: #2)\n#4. [pending] Add auth middleware (blockedBy: #3)\n#5. [pending] Write integration tests (blockedBy: #4)\n```\n\n### 2. Dependency Chains\n\nUse `addBlockedBy` to create execution order:\n\n```json\n// Task #3 cannot start until #1 and #2 complete\n{\"taskId\": \"3\", \"addBlockedBy\": [\"1\", \"2\"]}\n```\n\n### 3. Status Workflow\n\n```\npending ‚Üí in_progress ‚Üí completed\n   ‚Üì           ‚Üì\n(unblocked)  (active)\n\npending/in_progress ‚Üí deleted (CC 2.1.20)\n```\n\n- **pending**: Task created but not started\n- **in_progress**: Actively being worked on\n- **completed**: Work finished and verified\n- **deleted**: Task removed (CC 2.1.20) - permanently removes the task\n\n### Task Deletion (CC 2.1.20)\n\nCC 2.1.20 adds `status: \"deleted\"` to permanently remove tasks:\n\n```json\n// Delete a task\n{\"taskId\": \"3\", \"status\": \"deleted\"}\n```\n\n**When to delete:**\n- Orphaned tasks whose blockers have all failed\n- Tasks superseded by a different approach\n- Duplicate tasks created in error\n- Tasks from a cancelled pipeline\n\n**When NOT to delete:**\n- Tasks that might be retried later (keep as pending)\n- Tasks with useful history (mark completed instead)\n- Tasks blocked by in_progress work (wait for resolution)\n\n### 4. activeForm Pattern\n\nProvide present-continuous form for spinner display:\n\n| subject (imperative) | activeForm (continuous) |\n|---------------------|------------------------|\n| Run tests | Running tests |\n| Update schema | Updating schema |\n| Fix authentication | Fixing authentication |\n\n## Agent Teams (CC 2.1.33+)\n\nCC 2.1.33 introduces Agent Teams for multi-agent coordination with shared task lists and peer-to-peer messaging.\n\n### Team Workflow\n\n```\n1. TeamCreate(\"my-feature\")           ‚Üí Creates team + shared task list\n2. TaskCreate(subject, description)    ‚Üí Add tasks to shared list\n3. Task(prompt, team_name, name)       ‚Üí Spawn teammates\n4. TaskUpdate(owner: \"teammate-name\")  ‚Üí Assign tasks\n5. SendMessage(type: \"message\")        ‚Üí Direct teammate communication\n6. SendMessage(type: \"s",
    "contentTruncated": true
  },
  "temporal-io": {
    "content": "# Temporal.io Workflow Orchestration\n\nDurable execution engine for reliable distributed applications.\n\n## Overview\n\n- Long-running business processes (days/weeks/months)\n- Saga patterns requiring compensation/rollback\n- Microservice orchestration with retries\n- Systems requiring exactly-once execution guarantees\n- Complex state machines with human-in-the-loop\n- Scheduled and recurring workflows\n\n## Workflow Definition\n\n```python\nfrom temporalio import workflow\nfrom temporalio.common import RetryPolicy\nfrom datetime import timedelta\n\n@workflow.defn\nclass OrderWorkflow:\n    def __init__(self):\n        self._status = \"pending\"\n        self._order_id: str | None = None\n\n    @workflow.run\n    async def run(self, order_data: OrderInput) -> OrderResult:\n        self._order_id = await workflow.execute_activity(\n            create_order, order_data,\n            start_to_close_timeout=timedelta(seconds=30),\n            retry_policy=RetryPolicy(maximum_attempts=3, initial_interval=timedelta(seconds=1)),\n        )\n        self._status = \"processing\"\n\n        # Parallel activities\n        payment, inventory = await asyncio.gather(\n            workflow.execute_activity(process_payment, PaymentInput(order_id=self._order_id), start_to_close_timeout=timedelta(minutes=5)),\n            workflow.execute_activity(reserve_inventory, InventoryInput(order_id=self._order_id), start_to_close_timeout=timedelta(minutes=2)),\n        )\n\n        self._status = \"completed\"\n        return OrderResult(order_id=self._order_id, payment_id=payment.id)\n\n    @workflow.query\n    def get_status(self) -> str:\n        return self._status\n\n    @workflow.signal\n    async def cancel_order(self, reason: str):\n        self._status = \"cancelling\"\n        await workflow.execute_activity(cancel_order_activity, CancelInput(order_id=self._order_id), start_to_close_timeout=timedelta(seconds=30))\n        self._status = \"cancelled\"\n```\n\n## Activity Definition\n\n```python\nfrom temporalio import activity\nfrom temporalio.exceptions import ApplicationError\n\n@activity.defn\nasync def process_payment(input: PaymentInput) -> PaymentResult:\n    activity.logger.info(f\"Processing payment for order {input.order_id}\")\n    try:\n        async with httpx.AsyncClient() as client:\n            response = await client.post(\"https://payments.example.com/charge\", json={\"order_id\": input.order_id, \"amount\": input.amount})\n            response.raise_for_status()\n            return PaymentResult(**response.json())\n    except httpx.HTTPStatusError as e:\n        if e.response.status_code == 402:\n            raise ApplicationError(\"Payment declined\", non_retryable=True, type=\"PaymentDeclined\")\n        raise\n\n@activity.defn\nasync def send_notification(input: NotificationInput) -> None:\n    for i, recipient in enumerate(input.recipients):\n        activity.heartbeat(f\"Sending {i+1}/{len(input.recipients)}\")  # For long operations\n        await send_email(recipient, input.subject, input.body)\n```\n\n## Worker and Client\n\n```python\nfrom",
    "contentTruncated": true
  },
  "testing-patterns": {
    "content": "# Testing Patterns\n\nComprehensive patterns for building production test suites. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Unit Testing](#unit-testing) | 3 | CRITICAL | AAA pattern, parametrized tests, fixture scoping |\n| [Integration Testing](#integration-testing) | 3 | HIGH | API endpoints, database tests, component integration |\n| [E2E Testing](#e2e-testing) | 3 | HIGH | Playwright, AI agents, page objects |\n| [Pytest Advanced](#pytest-advanced) | 3 | HIGH | Custom markers, xdist parallel, plugins |\n| [API Mocking](#api-mocking) | 3 | HIGH | MSW 2.x, VCR.py, LLM API mocking |\n| [Test Data](#test-data) | 3 | MEDIUM | Factories, fixtures, seeding/cleanup |\n| [Verification](#verification) | 3 | MEDIUM | Property-based, stateful, contract testing |\n| [Performance](#performance) | 3 | MEDIUM | k6 load tests, Locust, test types |\n| [LLM Testing](#llm-testing) | 3 | HIGH | Mock responses, DeepEval, structured output |\n| [Accessibility](#accessibility) | 3 | MEDIUM | jest-axe, Playwright axe, CI gates |\n\n**Total: 30 rules across 10 categories**\n\n## Quick Start\n\n```python\n# pytest: AAA pattern with fixtures\n@pytest.fixture\ndef user(db_session):\n    return UserFactory.create(role=\"admin\")\n\ndef test_user_can_publish(user, article):\n    result = article.publish(by=user)\n    assert result.status == \"published\"\n```\n\n```typescript\n// Vitest + MSW: API integration test\nconst server = setupServer(\n  http.get('/api/users', () => HttpResponse.json([{ id: 1 }]))\n);\ntest('renders user list', async () => {\n  render(<UserList />);\n  expect(await screen.findByText('User 1')).toBeInTheDocument();\n});\n```\n\n## Unit Testing\n\nIsolated business logic tests with fast, deterministic execution.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| AAA Pattern | `rules/unit-aaa-pattern.md` | Arrange-Act-Assert with Vitest/pytest |\n| Parametrized Tests | `rules/unit-parametrized.md` | `test.each`, `@pytest.mark.parametrize`, indirect |\n| Fixture Scoping | `rules/unit-fixture-scoping.md` | function/module/session scope selection |\n\n## Integration Testing\n\nComponent interactions, API endpoints, and database integration.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| API Testing | `rules/integration-api.md` | Supertest, httpx AsyncClient, FastAPI TestClient |\n| Database Testing | `rules/integration-database.md` | In-memory SQLite, transaction rollback, test containers |\n| Component Integration | `rules/integration-component.md` | React Testing Library, QueryClientProvider |\n\n## E2E Testing\n\nEnd-to-end validation with Playwright 1.58+.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Playwright Core | `rules/e2e-playwright.md` | Semantic locators, auto-wait, flaky detection |\n| AI Agents | `rules/e2e-ai-agents.md` | Planner/Generator/Healer, init-agents |\n| Page Objects | `rules/e2e-page-objects.md` | Page ",
    "contentTruncated": true
  },
  "type-safety-validation": {
    "content": "# Type Safety & Validation\n\n## Overview\n\n**When to use this skill:**\n- Building type-safe APIs (REST, RPC, GraphQL)\n- Validating user input and external data\n- Ensuring database queries are type-safe\n- Creating end-to-end typed full-stack applications\n- Implementing strict validation rules\n\n## Core Stack Quick Reference\n\n| Tool | Purpose | Key Pattern |\n|------|---------|-------------|\n| **Zod** | Runtime validation | `z.object({}).safeParse(data)` |\n| **tRPC** | Type-safe APIs | `t.procedure.input(schema).query()` |\n| **Prisma** | Type-safe ORM | Auto-generated types from schema |\n| **TypeScript 5.7+** | Compile-time safety | `satisfies`, const params, decorators |\n\n## Zod Essentials\n\n```typescript\nimport { z } from 'zod'\n\n// Define schema\nconst UserSchema = z.object({\n  id: z.string().uuid(),\n  email: z.string().email(),\n  age: z.number().int().positive().max(120),\n  role: z.enum(['admin', 'user', 'guest']),\n  createdAt: z.date().default(() => new Date())\n})\n\n// Infer TypeScript type\ntype User = z.infer<typeof UserSchema>\n\n// Validate with error handling\nconst result = UserSchema.safeParse(data)\nif (result.success) {\n  const user: User = result.data\n} else {\n  console.error(result.error.issues)\n}\n```\n\n**See:** `references/zod-patterns.md` for transforms, refinements, discriminated unions, and recursive types.\n\n## tRPC Essentials\n\n```typescript\nimport { initTRPC } from '@trpc/server'\nimport { z } from 'zod'\n\nconst t = initTRPC.create()\n\nexport const appRouter = t.router({\n  getUser: t.procedure\n    .input(z.object({ id: z.string() }))\n    .query(async ({ input }) => {\n      return await db.user.findUnique({ where: { id: input.id } })\n    }),\n\n  createUser: t.procedure\n    .input(z.object({ email: z.string().email(), name: z.string() }))\n    .mutation(async ({ input }) => {\n      return await db.user.create({ data: input })\n    })\n})\n\nexport type AppRouter = typeof appRouter\n```\n\n**See:** `references/trpc-setup.md` for middleware, authentication, React integration, and error handling.\n\n## Exhaustive Type Checking\n\n```typescript\n// ALWAYS use assertNever for compile-time exhaustiveness\nfunction assertNever(x: never): never {\n  throw new Error(\"Unexpected value: \" + x)\n}\n\ntype Status = 'pending' | 'running' | 'completed' | 'failed'\n\nfunction getStatusColor(status: Status): string {\n  switch (status) {\n    case 'pending': return 'gray'\n    case 'running': return 'blue'\n    case 'completed': return 'green'\n    case 'failed': return 'red'\n    default: return assertNever(status) // Compile-time check!\n  }\n}\n\n// Exhaustive record mapping\nconst statusColors = {\n  pending: 'gray',\n  running: 'blue',\n  completed: 'green',\n  failed: 'red',\n} as const satisfies Record<Status, string>\n```\n\n**See:** `references/typescript-advanced.md` for handler objects, type guards, and anti-patterns.\n\n## Branded Types\n\n**TypeScript (with Zod):**\n```typescript\nconst UserId = z.string().uuid().brand<'UserId'>()\nconst AnalysisId = z.string().uuid().brand<'AnalysisId'>()\n\ntype ",
    "contentTruncated": true
  },
  "ui-components": {
    "content": "# UI Components\n\nComprehensive patterns for building accessible UI component libraries with shadcn/ui and Radix Primitives. Covers CVA variants, OKLCH theming, cn() utility, component extension, asChild composition, dialog/menu patterns, and data-attribute styling. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [shadcn/ui](#shadcnui) | 3 | HIGH | CVA variants, component customization, form patterns, data tables |\n| [Radix Primitives](#radix-primitives) | 3 | HIGH | Dialogs, polymorphic composition, data-attribute styling |\n\n**Total: 6 rules across 2 categories**\n\n## Quick Start\n\n```tsx\n// CVA variant system with cn() utility\nimport { cva, type VariantProps } from 'class-variance-authority'\nimport { cn } from '@/lib/utils'\n\nconst buttonVariants = cva(\n  'inline-flex items-center justify-center rounded-md font-medium transition-colors',\n  {\n    variants: {\n      variant: {\n        default: 'bg-primary text-primary-foreground hover:bg-primary/90',\n        destructive: 'bg-destructive text-destructive-foreground',\n        outline: 'border border-input bg-background hover:bg-accent',\n        ghost: 'hover:bg-accent hover:text-accent-foreground',\n      },\n      size: {\n        default: 'h-10 px-4 py-2',\n        sm: 'h-9 px-3',\n        lg: 'h-11 px-8',\n      },\n    },\n    defaultVariants: { variant: 'default', size: 'default' },\n  }\n)\n```\n\n```tsx\n// Radix Dialog with asChild composition\nimport { Dialog } from 'radix-ui'\n\n<Dialog.Root>\n  <Dialog.Trigger asChild>\n    <Button>Open</Button>\n  </Dialog.Trigger>\n  <Dialog.Portal>\n    <Dialog.Overlay className=\"fixed inset-0 bg-black/50\" />\n    <Dialog.Content className=\"data-[state=open]:animate-in\">\n      <Dialog.Title>Title</Dialog.Title>\n      <Dialog.Description>Description</Dialog.Description>\n      <Dialog.Close>Close</Dialog.Close>\n    </Dialog.Content>\n  </Dialog.Portal>\n</Dialog.Root>\n```\n\n## shadcn/ui\n\nBeautifully designed, accessible components built on CVA variants, cn() utility, and OKLCH theming.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Customization | `rules/shadcn-customization.md` | CVA variants, cn() utility, OKLCH theming, component extension |\n| Forms | `rules/shadcn-forms.md` | Form field wrappers, react-hook-form integration, validation |\n| Data Table | `rules/shadcn-data-table.md` | TanStack Table integration, column definitions, sorting/filtering |\n\n## Radix Primitives\n\nUnstyled, accessible React primitives for building high-quality design systems.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Dialog | `rules/radix-dialog.md` | Dialog, AlertDialog, controlled state, animations |\n| Composition | `rules/radix-composition.md` | asChild, Slot, nested triggers, polymorphic rendering |\n| Styling | `rules/radix-styling.md` | Data attributes, Tailwind arbitrary variants, focus management |\n\n## Key Decisions\n\n| Decision | Re",
    "contentTruncated": true
  },
  "upgrade-assessment": {
    "content": "# Upgrade Assessment\n\nEvaluate platform upgrade readiness for Claude model transitions, Claude Code version bumps, and OrchestKit plugin updates. Produces a structured JSON assessment report with a 0-10 readiness score across 6 dimensions.\n\n## When to Use\n\n- Before upgrading the Claude model (e.g., Sonnet 4 to Opus 4.6)\n- Before upgrading Claude Code to a new major/minor version\n- Before upgrading OrchestKit to a new major version\n- When evaluating whether a team environment is ready for a platform change\n- As part of release planning for model or platform migrations\n\n## Quick Start\n\n```bash\n/ork:upgrade-assessment           # Interactive assessment\n/ork:upgrade-assessment --json    # Machine-readable output\n```\n\n---\n\n## 6-Phase Workflow\n\n### Phase 0: Scope Definition\n\n**Tool:** `AskUserQuestion`\n\nDetermine the assessment scope before scanning. Ask the user:\n\n> What type of upgrade are you assessing?\n> 1. **Full platform** - Model + CC version + OrchestKit (comprehensive)\n> 2. **Model only** - Switching Claude model (e.g., Sonnet 4.5 to Opus 4.6)\n> 3. **CC version only** - Claude Code version bump (e.g., 2.1.32 to 2.1.33)\n> 4. **OrchestKit only** - Plugin version upgrade (e.g., 5.x to 6.x)\n\nRecord the scope and target versions. If the user does not specify target versions, research the latest available in Phase 2.\n\n---\n\n### Phase 1: Detection\n\n**Tools:** `Bash`, `Read`, `Grep`, `Glob`\n\n#### Precondition Checks\n\nBefore scanning, verify the environment is assessable:\n\n```bash\n# Verify we're in an OrchestKit project\n[ -f CLAUDE.md ] || { echo \"ERROR: No CLAUDE.md found ‚Äî not an OrchestKit project\"; exit 1; }\n[ -d src/skills ] || { echo \"ERROR: No src/skills/ directory\"; exit 1; }\n[ -d src/agents ] || { echo \"ERROR: No src/agents/ directory\"; exit 1; }\n[ -f src/hooks/hooks.json ] || { echo \"WARNING: No hooks.json ‚Äî hook assessment will be skipped\"; }\n```\n\nIf any required directory is missing, abort with a clear error. If optional components (hooks) are missing, continue with reduced scope and note it in the report.\n\n#### Environment Detection\n\nDetect the current environment state:\n\n```bash\n# 1. Current Claude model\n# Check CLAUDE.md, settings, or environment for model references\ngrep -r \"claude-\" CLAUDE.md .claude/ 2>/dev/null | head -20\n\n# 2. Claude Code version\nclaude --version 2>/dev/null || echo \"CC version not detectable from CLI\"\n\n# 3. OrchestKit version\n# Check CLAUDE.md or package.json for version field\ngrep \"Current.*:\" CLAUDE.md | head -5\n\n# 4. Hooks configuration\ncat src/hooks/hooks.json | python3 -c \"import sys,json; d=json.load(sys.stdin); print(f'Hooks: {len(d.get(\\\"hooks\\\",[]))} entries')\" 2>/dev/null\n\n# 5. Skill and agent counts\nls src/skills/ | wc -l\nls src/agents/ | wc -l\n```\n\n**Output:** Environment snapshot including:\n- Current model ID (e.g., `claude-sonnet-4-5`)\n- Current CC version (e.g., `2.1.33`)\n- Current OrchestKit version (e.g., `6.0.0`)\n- Hook count and bundle count\n- Skill count and agent count\n\n---\n\n### Phase 2: Researc",
    "contentTruncated": true
  },
  "verify": {
    "content": "# Verify Feature\n\nComprehensive verification using parallel specialized agents with nuanced grading (0-10 scale) and improvement suggestions.\n\n## Quick Start\n\n```bash\n/verify authentication flow\n/verify user profile feature\n/verify --scope=backend database migrations\n```\n\n> **Opus 4.6**: Agents use native adaptive thinking (no MCP sequential-thinking needed). Extended 128K output supports comprehensive verification reports.\n\n---\n\n## STEP 0: Verify User Intent with AskUserQuestion\n\n**BEFORE creating tasks**, clarify verification scope:\n\n```python\nAskUserQuestion(\n  questions=[{\n    \"question\": \"What scope for this verification?\",\n    \"header\": \"Scope\",\n    \"options\": [\n      {\"label\": \"Full verification (Recommended)\", \"description\": \"All tests + security + code quality + grades\"},\n      {\"label\": \"Tests only\", \"description\": \"Run unit + integration + e2e tests\"},\n      {\"label\": \"Security audit\", \"description\": \"Focus on security vulnerabilities\"},\n      {\"label\": \"Code quality\", \"description\": \"Lint, types, complexity analysis\"},\n      {\"label\": \"Quick check\", \"description\": \"Just run tests, skip detailed analysis\"}\n    ],\n    \"multiSelect\": false\n  }]\n)\n```\n\n**Based on answer, adjust workflow:**\n- **Full verification**: All 8 phases, all 5 parallel agents\n- **Tests only**: Skip phases 2 (security), 5 (UI/UX analysis)\n- **Security audit**: Focus on security-auditor agent\n- **Code quality**: Focus on code-quality-reviewer agent\n- **Quick check**: Run tests only, skip grading and suggestions\n\n---\n\n## STEP 0b: Select Orchestration Mode\n\nChoose **Agent Teams** (mesh ‚Äî verifiers share findings) or **Task tool** (star ‚Äî all report to lead):\n\n1. `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1` ‚Üí **Agent Teams mode**\n2. Agent Teams unavailable ‚Üí **Task tool mode** (default)\n3. Otherwise: Full verification with cross-domain concerns ‚Üí recommend **Agent Teams**; Single-scope verification ‚Üí **Task tool**\n\n| Aspect | Task Tool | Agent Teams |\n|--------|-----------|-------------|\n| Finding correlation | Lead cross-references scores | Agents discuss overlapping concerns |\n| Security + test overlap | Independent scoring | security-auditor alerts test-generator about gaps |\n| Cost | ~200K tokens | ~500K tokens |\n| Best for | Focused verification | Full-stack verification with 5 agents |\n\n> **Fallback:** If Agent Teams encounters issues, fall back to Task tool for remaining verification.\n\n---\n\n## Task Management (CC 2.1.16)\n\n```python\n# Create main verification task\nTaskCreate(\n  subject=\"Verify [feature-name] implementation\",\n  description=\"Comprehensive verification with nuanced grading\",\n  activeForm=\"Verifying [feature-name] implementation\"\n)\n\n# Create subtasks for 8-phase process\nphases = [\"Run code quality checks\", \"Execute security audit\",\n          \"Verify test coverage\", \"Validate API\", \"Check UI/UX\",\n          \"Calculate grades\", \"Generate suggestions\", \"Compile report\"]\nfor phase in phases:\n    TaskCreate(subject=phase, activeForm=f\"{phase}ing\")\n```\n\n---\n\n## ",
    "contentTruncated": true
  },
  "video-production": {
    "content": "# Video Production\n\nComprehensive pipeline for creating production-quality tech demo videos. Each category has individual rule files in `rules/` loaded on-demand.\n\n## Quick Reference\n\n| Category | Rules | Impact | When to Use |\n|----------|-------|--------|-------------|\n| [Pre-Production](#pre-production) | 5 | CRITICAL | Storyboarding, narration scripts, hooks, recipes, pacing |\n| [Recording](#recording) | 2 | HIGH | Terminal recording with VHS/asciinema, Manim animations |\n| [Composition](#composition) | 4 | HIGH | Remotion compositions, scene cards, callouts, thumbnails |\n| [Audio](#audio) | 4 | HIGH | Music selection, mixing, ElevenLabs TTS, SFX placement |\n| [Visual Effects](#visual-effects) | 4 | MEDIUM | 3D graphics, data visualization, Lottie, effects library |\n| [Captions & Output](#captions--output) | 2 | MEDIUM | Captions/subtitles, showcase templates, final rendering |\n\n**Total: 21 rules across 6 categories**\n\n## Pre-Production\n\nPlanning, scripting, and structuring video content before recording.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Storyboarding | `rules/preproduction-storyboard.md` | AIDA framework, scene templates, shot planning |\n| Narration | `rules/preproduction-narration.md` | Scene-by-scene scripts, timing markers, WPM pacing |\n| Hook Formulas | `rules/preproduction-hooks.md` | 12 proven hook patterns for scroll-stopping intros |\n| Content Recipes | `rules/preproduction-recipes.md` | Content type recipes (skill demo, agent demo, tutorial) |\n| Pacing | `rules/preproduction-pacing.md` | Video rhythm, attention curves, platform-specific timing |\n\n## Recording\n\nCapturing terminal sessions and creating animated visualizations.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Terminal Recording | `rules/recording-terminal.md` | VHS tape format, asciinema recording, CC simulation |\n| Manim Animations | `rules/recording-manim.md` | Manim animations for workflow and architecture visualization |\n\n## Composition\n\nAssembling video elements into final compositions with Remotion.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Remotion Composition | `rules/composition-remotion.md` | Remotion patterns, folder organization, animation presets |\n| Scene Intro Cards | `rules/composition-scenes.md` | Transition cards, timing patterns, animation sequences |\n| Callout Positioning | `rules/composition-callouts.md` | Debug grids, coordinate systems, responsive positioning |\n| Thumbnails | `rules/composition-thumbnails.md` | Thumbnail formulas, first-frame optimization, platform specs |\n\n## Audio\n\nMusic selection, sound effects, mixing, and text-to-speech narration.\n\n| Rule | File | Key Pattern |\n|------|------|-------------|\n| Music Selection | `rules/audio-music.md` | Music matching matrix, BPM guidelines, mood-to-genre mapping |\n| Audio Mixing | `rules/audio-mixing.md` | ffmpeg filters, ducking patterns, volume balancing, LUFS |\n| ElevenLabs TTS | `rules/audio-elevenlabs.md` | ElevenLabs API i",
    "contentTruncated": true
  },
  "vision-language-models": {
    "content": "# Vision Language Models ()\n\nIntegrate vision capabilities from leading multimodal models for image understanding, document analysis, and visual reasoning.\n\n## Overview\n\n- Image captioning and description generation\n- Visual question answering (VQA)\n- Document/chart/diagram analysis with OCR\n- Multi-image comparison and reasoning\n- Bounding box detection and region analysis\n- Video frame analysis\n\n## Model Comparison (January )\n\n| Model | Context | Strengths | Vision Input |\n|-------|---------|-----------|--------------|\n| **GPT-5.2** | 128K | Best general reasoning, multimodal | Up to 10 images |\n| **Claude Opus 4.6** | 1M | Best coding, sustained agent tasks, adaptive thinking | Up to 100 images |\n| **Gemini 2.5 Pro** | 1M+ | Longest context, video analysis | 3,600 images max |\n| **Gemini 3 Pro** | 1M | Deep Think, 100% AIME 2025 | Enhanced segmentation |\n| **Grok 4** | 2M | Real-time X integration, DeepSearch | Images + upcoming video |\n\n## Image Input Methods\n\n### Base64 Encoding (All Providers)\n\n```python\nimport base64\nimport mimetypes\n\ndef encode_image_base64(image_path: str) -> tuple[str, str]:\n    \"\"\"Encode local image to base64 with MIME type.\"\"\"\n    mime_type, _ = mimetypes.guess_type(image_path)\n    mime_type = mime_type or \"image/png\"\n\n    with open(image_path, \"rb\") as f:\n        base64_data = base64.standard_b64encode(f.read()).decode(\"utf-8\")\n\n    return base64_data, mime_type\n```\n\n### OpenAI GPT-5/4o Vision\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ndef analyze_image_openai(image_path: str, prompt: str) -> str:\n    \"\"\"Analyze image using GPT-5 or GPT-4o.\"\"\"\n    base64_data, mime_type = encode_image_base64(image_path)\n\n    response = client.chat.completions.create(\n        model=\"gpt-5.2\",  # or \"gpt-4.1\" for cost optimization\n        messages=[{\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": prompt},\n                {\"type\": \"image_url\", \"image_url\": {\n                    \"url\": f\"data:{mime_type};base64,{base64_data}\",\n                    \"detail\": \"high\"  # low, high, or auto\n                }}\n            ]\n        }],\n        max_tokens=4096  # Required for vision\n    )\n    return response.choices[0].message.content\n```\n\n### Claude 4.5 Vision (Anthropic)\n\n```python\nimport anthropic\n\nclient = anthropic.Anthropic()\n\ndef analyze_image_claude(image_path: str, prompt: str) -> str:\n    \"\"\"Analyze image using Claude Opus 4.6 or Sonnet 4.5.\"\"\"\n    base64_data, media_type = encode_image_base64(image_path)\n\n    response = client.messages.create(\n        model=\"claude-opus-4-6\",  # or claude-sonnet-4-5\n        max_tokens=4096,\n        messages=[{\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": media_type,\n                        \"data\": base64_data\n                    }\n                },\n                {\"ty",
    "contentTruncated": true
  },
  "vite-advanced": {
    "content": "# Vite Advanced Patterns\n\nAdvanced configuration for Vite 7+ including Environment API.\n\n## Vite 7 Environment API (Key 2026 Feature)\n\nMulti-environment support is now first-class:\n\n```typescript\nimport { defineConfig } from 'vite'\n\nexport default defineConfig({\n  environments: {\n    // Browser client\n    client: {\n      build: {\n        outDir: 'dist/client',\n        manifest: true,\n      },\n    },\n    // Node.js SSR\n    ssr: {\n      build: {\n        outDir: 'dist/server',\n        target: 'node20',\n      },\n    },\n    // Edge runtime (Cloudflare, etc.)\n    edge: {\n      resolve: {\n        noExternal: true,\n        conditions: ['edge', 'worker'],\n      },\n      build: {\n        outDir: 'dist/edge',\n      },\n    },\n  },\n})\n```\n\n**Key Changes:**\n- Environments have their own module graph\n- Plugins access `this.environment` in hooks\n- `createBuilder` API for coordinated builds\n- Node.js 20.19+ or 22.12+ required\n\n## Plugin Development\n\nBasic plugin structure:\n\n```typescript\nexport function myPlugin(): Plugin {\n  return {\n    name: 'my-plugin',\n\n    // Called once when config is resolved\n    configResolved(config) {\n      // Access resolved config\n    },\n\n    // Transform individual modules\n    transform(code, id) {\n      // this.environment available in Vite 7+\n      if (id.endsWith('.special')) {\n        return { code: transformCode(code) }\n      }\n    },\n\n    // Virtual modules\n    resolveId(id) {\n      if (id === 'virtual:my-module') {\n        return '\\0virtual:my-module'\n      }\n    },\n    load(id) {\n      if (id === '\\0virtual:my-module') {\n        return 'export const value = \"generated\"'\n      }\n    },\n  }\n}\n```\n\n## SSR Configuration\n\nDevelopment (middleware mode):\n\n```typescript\nimport { createServer } from 'vite'\n\nconst vite = await createServer({\n  server: { middlewareMode: true },\n  appType: 'custom',\n})\n\napp.use('*', async (req, res) => {\n  const url = req.originalUrl\n  let template = fs.readFileSync('index.html', 'utf-8')\n  template = await vite.transformIndexHtml(url, template)\n\n  const { render } = await vite.ssrLoadModule('/src/entry-server.tsx')\n  const html = template.replace('<!--outlet-->', await render(url))\n\n  res.send(html)\n})\n```\n\nProduction build scripts:\n\n```json\n{\n  \"scripts\": {\n    \"build:client\": \"vite build --outDir dist/client\",\n    \"build:server\": \"vite build --outDir dist/server --ssr src/entry-server.tsx\"\n  }\n}\n```\n\n## Build Optimization\n\n```typescript\nexport default defineConfig({\n  build: {\n    target: 'baseline-widely-available', // Vite 7 default\n    sourcemap: false,\n    rollupOptions: {\n      output: {\n        manualChunks: {\n          vendor: ['react', 'react-dom'],\n          router: ['react-router-dom'],\n        },\n      },\n    },\n  },\n})\n```\n\n## Quick Reference\n\n| Feature | Vite 7 Status |\n|---------|---------------|\n| Environment API | Stable |\n| ESM-only distribution | Default |\n| Node.js requirement | 20.19+ or 22.12+ |\n| `buildApp` hook | New for plugins |\n| `createBuilder` | Multi-env builds |\n\n## Vite",
    "contentTruncated": true
  },
  "web-research-workflow": {
    "content": "# Web Research Workflow\n\nUnified approach for web content research that automatically selects the right tool for each situation.\n\n## Quick Decision Tree\n\n```\nURL to research\n     ‚îÇ\n     ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ 1. Try WebFetch ‚îÇ ‚Üê Fast, free, no overhead\n‚îÇ    (always try) ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n     ‚îÇ\nContent OK? ‚îÄ‚îÄYes‚îÄ‚îÄ‚ñ∫ Parse and return\n     ‚îÇ\n     No (empty/partial/<500 chars)\n     ‚îÇ\n     ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ 2. TAVILY_API_KEY set?‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n     ‚îÇ          ‚îÇ\n    Yes         No ‚îÄ‚îÄ‚ñ∫ Skip to step 3\n     ‚îÇ\n     ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ Tavily search/extract/    ‚îÇ ‚Üê Raw markdown, batch URLs\n‚îÇ crawl/research            ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n     ‚îÇ\nContent OK? ‚îÄ‚îÄYes‚îÄ‚îÄ‚ñ∫ Parse and return\n     ‚îÇ\n     No (JS-rendered/auth-required)\n     ‚îÇ\n     ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ 3. Use agent-browser ‚îÇ ‚Üê Full browser, last resort\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n     ‚îÇ\n‚îú‚îÄ SPA (react/vue/angular) ‚îÄ‚îÄ‚ñ∫ wait --load networkidle\n‚îú‚îÄ Login required ‚îÄ‚îÄ‚ñ∫ auth flow + state save\n‚îú‚îÄ Dynamic content ‚îÄ‚îÄ‚ñ∫ wait --text \"Expected\"\n‚îî‚îÄ Multi-page ‚îÄ‚îÄ‚ñ∫ crawl pattern\n```\n\n## Tavily Enhanced Research\n\nWhen `TAVILY_API_KEY` is set, Tavily provides a powerful middle tier between WebFetch and agent-browser. It returns raw markdown content, supports batch URL extraction, and offers semantic search with relevance scoring.\n\n**When to use Tavily over WebFetch:**\n- WebFetch returned <500 chars (likely incomplete)\n- You need raw markdown content (not Haiku-summarized)\n- Batch extracting content from multiple URLs\n- Semantic search with relevance scoring\n- Site discovery/crawling (map API)\n\n**When to skip Tavily and go to agent-browser:**\n- Content requires JavaScript rendering (SPAs)\n- Authentication/login is required\n- Interactive elements need clicking\n- Content is behind CAPTCHAs\n\n### Tavily Search (Semantic Web Search)\n\nReturns relevance-scored results with raw markdown content:\n\n```bash\ncurl -s -X POST 'https://api.tavily.com/search' \\\n  -H 'Content-Type: application/json' \\\n  -H \"Authorization: Bearer $TAVILY_API_KEY\" \\\n  -d '{\n    \"query\": \"your search query\",\n    \"search_depth\": \"advanced\",\n    \"max_results\": 5,\n    \"include_raw_content\": \"markdown\"\n  }' | python3 -m json.tool\n```\n\nOptions:\n- `search_depth`: `\"basic\"` (fast) or `\"advanced\"` (thorough, 2x cost)\n- `topic`: `\"general\"` (default) or `\"news\"` or `\"finance\"`\n- `include_domains`: `[\"example.com\"]` ‚Äî restrict to specific sites\n- `exclude_domains`: `[\"reddit.com\"]` ‚Äî filter out sites\n- `days`: `3` ‚Äî limit to recent results (news/finance)\n- `include_raw_content`: `\"markdown\"` ‚Äî get full page content\n\n### Tavily Extract (Batch URL Content)\n\nExtract raw content from up to 20 URLs at once:\n\n```bash\ncurl -s -X POST 'https://api.tavily.com/extract' \\\n  -H 'Content-Type: application/json' \\\n  -H \"Authorization: Bearer $TAVILY_API_KEY\" \\\n  -d '{\n    \"urls\": [\n      \"https://docs.example.com/guide\",\n      \"https://competitor.com/pricing\"\n    ]\n  }' | python3 -m json.tool\n```\n\nReturns markdown content for ",
    "contentTruncated": true
  },
  "worktree-coordination": {
    "content": "# Worktree Coordination Skill\n\n> **Agent Teams (CC 2.1.33+):** When `CLAUDE_CODE_EXPERIMENTAL_AGENT_TEAMS=1` is set, native Agent Teams provides built-in teammate lifecycle management, peer-to-peer messaging, and shared task lists. This skill's custom file locking and coordination registry are superseded by Teams' native coordination. Use this skill only for **non-Teams worktree scenarios** (e.g., multiple independent Claude Code sessions without a shared team).\n\n## Commands\n\n### /worktree-status\nShow status of all active Claude Code instances.\n\n**Usage:** `/worktree-status [--json] [--clean]`\n\n**Actions:**\n1. Run `cc-worktree-status` to see all active instances\n2. Check for stale instances (no heartbeat > 5 min)\n3. View file locks across all instances\n\n**Output includes:**\n- Instance ID and branch\n- Current task (if set)\n- Health status (ACTIVE/STALE)\n- Files locked by each instance\n\n### /worktree-claim <file-path>\nExplicitly lock a file for this instance.\n\n**Usage:** `/worktree-claim src/auth/login.ts`\n\n**Actions:**\n1. Check if file is already locked\n2. If locked by another instance, show who holds it\n3. If available, acquire lock\n\n### /worktree-release <file-path>\nRelease lock on a file.\n\n**Usage:** `/worktree-release src/auth/login.ts`\n\n### /worktree-sync\nSync shared context and check for conflicts.\n\n**Usage:** `/worktree-sync [--check-conflicts] [--pull-decisions]`\n\n**Actions:**\n1. `--check-conflicts`: Run merge-tree against other active branches\n2. `--pull-decisions`: Show recent architectural decisions from other instances\n\n### /worktree-decision <decision>\nLog an architectural decision visible to all instances.\n\n**Usage:** `/worktree-decision \"Using Passport.js for OAuth\" --rationale \"Better middleware support\"`\n\n## Automatic Behaviors\n\n### File Lock Check (PreToolUse Hook)\nBefore any Write or Edit operation:\n1. Check if file is locked by another instance\n2. If locked ‚Üí BLOCK with details about lock holder\n3. If unlocked ‚Üí Acquire lock and proceed\n\n### Heartbeat (Lifecycle Hook)\nEvery 30 seconds:\n1. Update this instance's heartbeat timestamp\n2. Clean up stale instances (no heartbeat > 5 min)\n3. Release orphaned locks\n\n### Cleanup (Stop Hook)\nWhen Claude Code exits:\n1. Release all file locks held by this instance\n2. Unregister from coordination registry\n\n## File Lock States\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ  FILE: src/auth/oauth.ts                                ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  Status: LOCKED                                         ‚îÇ\n‚îÇ  Holder: cc-auth-a1b2c3                                 ‚îÇ\n‚îÇ  Branch: feature/user-authentication                    ‚îÇ\n‚îÇ  Task:   Implementing OAuth2 login flow                 ‚îÇ\n‚îÇ  Since:  2 minutes ago                                  ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ  Action: Wait for release or use /worktree-release     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n## Regist",
    "contentTruncated": true
  },
  "zero-downtime-migration": {
    "content": "# Zero-Downtime Migration ()\n\nDatabase migration patterns that ensure continuous service availability during schema changes.\n\n## Overview\n\n- Deploying schema changes to production systems with uptime requirements\n- Renaming or removing columns without breaking existing application code\n- Adding NOT NULL constraints to existing columns with data\n- Creating indexes on large tables without locking\n- Migrating data between columns or tables during live traffic\n- Using pgroll for automated expand-contract migrations\n\n## Quick Reference\n\n### Expand-Contract Overview\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                     EXPAND-CONTRACT PATTERN                              ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                                                          ‚îÇ\n‚îÇ  Phase 1: EXPAND              Phase 2: MIGRATE           Phase 3: CONTRACT‚îÇ\n‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ\n‚îÇ  Add new column              Backfill data              Remove old column ‚îÇ\n‚îÇ  (nullable)                  Update app to use new      (after app migrated)‚îÇ\n‚îÇ                              Both versions work                           ‚îÇ\n‚îÇ                                                                          ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ\n‚îÇ  ‚îÇold_col  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇold_col  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> ‚îÇnew_col  ‚îÇ      ‚îÇ\n‚îÇ  ‚îÇ         ‚îÇ                 ‚îÇnew_col  ‚îÇ                ‚îÇ         ‚îÇ      ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ\n‚îÇ                                                                          ‚îÇ\n‚îÇ  Rollback: Drop new          Rollback: Use old          Rollback: N/A    ‚îÇ\n‚îÇ                              (dual-write in app)        (commit)         ‚îÇ\n‚îÇ                                                                          ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### pgroll: Automated Expand-Contract\n\n```bash\n# Install pgroll ( recommended tool)\nbrew install xataio/pgroll/pgroll\n# or\ngo install github.com/xataio/pgroll@latest\n\n# Initialize pgroll in your database\npgroll init --postgres-url \"postgres://user:pass@localhost/db\"\n\n# Create a migration file (migrations/001_add_email_verified.json)\n```\n\n```json\n{\n  \"name\": \"001_add_email_verified\",\n  \"operations\": [\n    {\n      \"add_column\": {\n        \"table\": \"users\",\n        \"column\": {\n          \"name\": \"email_verified\",\n          \"type\": \"boolean\",\n          \"default\": \"false\",\n          \"nullable\": false\n        },\n        \"up\": \"false\"\n      }\n    }\n  ]\n}\n```\n\n```bash\n# Start migration (creates versioned schema)\npgroll start migrations/001_add_email_verified.json\n\n# App v1 uses: schema \"public_001_add_email_verified\"\n# App v2 uses: schema \"public\" (new version)\n\n# After verification, complete migration\npgroll complete\n\n# Rollback if issues\npgroll rollb",
    "contentTruncated": true
  },
  "zustand-patterns": {
    "content": "# Zustand Patterns\n\nModern state management with Zustand 5.x - lightweight, TypeScript-first, no boilerplate.\n\n## Overview\n\n- Global state without Redux complexity\n- Shared state across components without prop drilling\n- Persisted state with localStorage/sessionStorage\n- Computed/derived state with selectors\n- State that needs middleware (logging, devtools, persistence)\n\n## Core Patterns\n\n### 1. Basic Store with TypeScript\n\n```typescript\nimport { create } from 'zustand';\n\ninterface BearState {\n  bears: number;\n  increase: (by: number) => void;\n  reset: () => void;\n}\n\nconst useBearStore = create<BearState>()((set) => ({\n  bears: 0,\n  increase: (by) => set((state) => ({ bears: state.bears + by })),\n  reset: () => set({ bears: 0 }),\n}));\n```\n\n### 2. Slices Pattern (Modular Stores)\n\n```typescript\nimport { create, StateCreator } from 'zustand';\n\n// Auth slice\ninterface AuthSlice {\n  user: User | null;\n  login: (user: User) => void;\n  logout: () => void;\n}\n\nconst createAuthSlice: StateCreator<AuthSlice & CartSlice, [], [], AuthSlice> = (set) => ({\n  user: null,\n  login: (user) => set({ user }),\n  logout: () => set({ user: null }),\n});\n\n// Cart slice\ninterface CartSlice {\n  items: CartItem[];\n  addItem: (item: CartItem) => void;\n  clearCart: () => void;\n}\n\nconst createCartSlice: StateCreator<AuthSlice & CartSlice, [], [], CartSlice> = (set) => ({\n  items: [],\n  addItem: (item) => set((state) => ({ items: [...state.items, item] })),\n  clearCart: () => set({ items: [] }),\n});\n\n// Combined store\nconst useStore = create<AuthSlice & CartSlice>()((...a) => ({\n  ...createAuthSlice(...a),\n  ...createCartSlice(...a),\n}));\n```\n\n### 3. Immer Middleware (Immutable Updates)\n\n```typescript\nimport { create } from 'zustand';\nimport { immer } from 'zustand/middleware/immer';\n\ninterface TodoState {\n  todos: Todo[];\n  addTodo: (text: string) => void;\n  toggleTodo: (id: string) => void;\n  updateNested: (id: string, subtaskId: string, done: boolean) => void;\n}\n\nconst useTodoStore = create<TodoState>()(\n  immer((set) => ({\n    todos: [],\n    addTodo: (text) =>\n      set((state) => {\n        state.todos.push({ id: crypto.randomUUID(), text, done: false });\n      }),\n    toggleTodo: (id) =>\n      set((state) => {\n        const todo = state.todos.find((t) => t.id === id);\n        if (todo) todo.done = !todo.done;\n      }),\n    updateNested: (id, subtaskId, done) =>\n      set((state) => {\n        const todo = state.todos.find((t) => t.id === id);\n        const subtask = todo?.subtasks?.find((s) => s.id === subtaskId);\n        if (subtask) subtask.done = done;\n      }),\n  }))\n);\n```\n\n### 4. Persist Middleware\n\n```typescript\nimport { create } from 'zustand';\nimport { persist, createJSONStorage } from 'zustand/middleware';\n\ninterface SettingsState {\n  theme: 'light' | 'dark';\n  language: string;\n  setTheme: (theme: 'light' | 'dark') => void;\n}\n\nconst useSettingsStore = create<SettingsState>()(\n  persist(\n    (set) => ({\n      theme: 'light',\n      language: 'en',\n      setTheme: (t",
    "contentTruncated": true
  }
};
