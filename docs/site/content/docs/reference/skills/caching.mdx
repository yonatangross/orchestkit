---
title: "Caching"
description: "Caching patterns for backend, LLM prompt, semantic similarity, and cost tracking. Use when implementing Redis cache, prompt caching, semantic caching, cache cost monitoring, or optimizing cache hit rates."
---

<span className="badge badge-gray">Reference</span> <span className="badge badge-yellow">medium</span>

**Primary Agent:** [data-pipeline-engineer](/docs/reference/agents/data-pipeline-engineer)


# Caching

Comprehensive caching patterns for backend services and LLM applications. Each category has individual rule files in `rules/` loaded on-demand.

## Quick Reference

| Category | Rules | Impact | When to Use |
|----------|-------|--------|-------------|
| [Backend Caching](#backend-caching) | 3 | HIGH | Cache-aside, write-through, invalidation, stampede prevention |
| [Prompt Caching](#prompt-caching) | 3 | HIGH | Claude cache_control, OpenAI prefix caching, breakpoint strategies |
| [Semantic Caching](#semantic-caching) | 3 | MEDIUM | Vector similarity cache, multi-level hierarchy, Redis hybrid search |
| [Cost Tracking](#cost-tracking) | 3 | MEDIUM | Langfuse cost tracking, per-agent attribution, cache ROI metrics |

**Total: 12 rules across 4 categories**

## Backend Caching

Redis-based caching patterns for high-performance backends.

| Rule | File | Key Pattern |
|------|------|-------------|
| Cache-Aside | `rules/backend-cache-aside.md` | Lazy loading with get_or_set and Redis |
| Write-Through | `rules/backend-write-through.md` | Synchronous cache + DB writes, write-behind batching |
| Invalidation | `rules/backend-invalidation.md` | TTL, event-based, version-based + stampede prevention |

## Prompt Caching

Provider-native prompt caching for 90% token savings.

| Rule | File | Key Pattern |
|------|------|-------------|
| Claude Caching | `rules/prompt-claude.md` | cache_control with ephemeral TTL (5m/1h) |
| OpenAI Caching | `rules/prompt-openai.md` | Automatic prefix caching, no markers needed |
| Breakpoints | `rules/prompt-breakpoints.md` | Processing order, pricing, best practices |

## Semantic Caching

Cache LLM responses by semantic similarity with Redis vector search.

| Rule | File | Key Pattern |
|------|------|-------------|
| Vector Cache | `rules/semantic-vector.md` | Redis vector similarity with configurable thresholds |
| Multi-Level | `rules/semantic-multi-level.md` | L1 exact -> L2 semantic -> L3 prompt -> L4 LLM hierarchy |
| Redis Hybrid | `rules/semantic-redis.md` | Redis 8.4 FT.HYBRID for metadata + vector queries |

## Cost Tracking

Monitor cache effectiveness and LLM costs with Langfuse.

| Rule | File | Key Pattern |
|------|------|-------------|
| Langfuse Tracking | `rules/cost-langfuse.md` | Automatic cost tracking with @observe decorator |
| Cost Attribution | `rules/cost-attribution.md` | Per-agent hierarchical cost rollup |
| Effectiveness | `rules/cost-effectiveness.md` | Cache hit rate calculation and ROI metrics |

## Quick Start Example

```python
import redis.asyncio as redis
import json

class CacheAside:
    def __init__(self, redis_client: redis.Redis, default_ttl: int = 3600):
        self.redis = redis_client
        self.ttl = default_ttl

    async def get_or_set(self, key: str, fetch_fn, ttl: int | None = None):
        """Get from cache, or fetch and cache."""
        cached = await self.redis.get(key)
        if cached:
            return json.loads(cached)

        value = await fetch_fn()
        await self.redis.setex(key, ttl or self.ttl, json.dumps(value))
        return value
```

## Key Decisions

| Decision | Recommendation |
|----------|----------------|
| Default TTL | 1 hour for most data, 5 min for volatile |
| Serialization | orjson for performance |
| Key naming | `\{entity\}:\{id\}` or `\{entity\}:\{id\}:\{field\}` |
| Stampede | Use distributed locks for expensive computations |
| Invalidation | Event-based for writes, TTL for reads |
| Prompt cache TTL | 5m default, 1h if >10 reads/hour |
| Semantic threshold | Start at 0.92, tune based on hit rate |
| Embedding model | text-embedding-3-small (fast, cheap) |
| Cost tracking | Langfuse @observe with session_id grouping |

## Common Mistakes

1. Caching without TTL (memory leak)
2. Variable content before cached prefix (breaks prompt cache)
3. Semantic threshold too low (false positive cache hits)
4. Not promoting L2 hits to L1 (misses optimization)
5. Missing metadata for cost attribution
6. No stampede prevention for expensive computations
7. Ignoring prompt cache savings in cost calculations
8. Not linking child to parent trace in Langfuse
9. Cache as primary storage (data loss risk)
10. Silent cache failure handling (stale data)

## Evaluations

See `test-cases.json` for 12 test cases across all categories.

## Related Skills

- `rag-retrieval` - Vector generation and retrieval patterns
- `resilience-patterns` - Fallback and circuit breaker strategies
- `langfuse-observability` - Full observability with Langfuse
- `connection-pooling` - Redis connection pool management

## Capability Details

### cache-aside
**Keywords:** cache aside, lazy loading, cache miss, get or set, redis
**Solves:**
- Implement lazy loading cache with Redis
- Cache on read pattern
- Handle cache miss gracefully

### write-through
**Keywords:** write through, cache consistency, synchronous cache, write behind
**Solves:**
- Keep cache consistent with database
- Strong consistency caching
- High write throughput with write-behind

### cache-invalidation
**Keywords:** invalidation, cache bust, TTL, cache tags, stampede, thundering herd
**Solves:**
- Invalidate cache on data changes
- Prevent cache stampede
- Version-based namespace invalidation

### prompt-caching
**Keywords:** prompt cache, cache_control, ephemeral, prefix caching, breakpoints
**Solves:**
- Reduce LLM token costs with cached prompts
- Configure cache breakpoints for Claude
- Structure prompts for OpenAI automatic caching

### semantic-cache
**Keywords:** semantic, vector, embedding, similarity, redis, multi-level
**Solves:**
- Cache LLM responses by semantic similarity
- Build multi-level cache hierarchy (L1-L4)
- Use Redis 8.4 FT.HYBRID for hybrid search

### cost-tracking
**Keywords:** cost, langfuse, token, usage, attribution, ROI
**Solves:**
- Track LLM costs with Langfuse @observe
- Per-agent cost attribution
- Calculate cache hit rate and ROI
