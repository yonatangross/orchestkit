---
title: "Browser Tools"
description: "Browser automation and content capture patterns for Playwright, Puppeteer, web scraping, and structured data extraction. Use when automating browser workflows, capturing web content, or extracting structured data from web pages."
---
import { Accordions, Accordion } from "fumadocs-ui/components/accordion";

<span className="badge badge-gray">Reference</span> <span className="badge badge-yellow">medium</span>

**Primary Agent:** [web-research-analyst](/docs/reference/agents/web-research-analyst)


# Browser Tools

Browser automation and content capture patterns using agent-browser CLI, Playwright, and Puppeteer. Each category has individual rule files in `references/` loaded on-demand.

## Quick Reference

| Category | Rules | When to Use |
|----------|-------|-------------|
| [Playwright Setup](#playwright-setup) | 1 | Installing and configuring Playwright for automation |
| [Page Interaction](#page-interaction) | 1 | Clicking, filling forms, navigating with snapshot + refs |
| [Content Extraction](#content-extraction) | 1 | Extracting text, HTML, structured data from pages |
| [SPA Extraction](#spa-extraction) | 1 | React, Vue, Angular apps with client-side rendering |
| [Scraping Strategies](#scraping-strategies) | 1 | Multi-page crawls, pagination, recursive crawling |
| [Anti-Bot Handling](#anti-bot-handling) | 1 | Rate limiting, CAPTCHA, session management |
| [Authentication Flows](#authentication-flows) | 1 | Login forms, OAuth, SSO, session persistence |
| [Structured Output](#structured-output) | 1 | Converting scraped content to clean markdown/JSON |

**Total: 8 rules across 8 categories**

## Quick Start

```bash
# Install agent-browser
npm install -g agent-browser
agent-browser install                # Download Chromium

# Basic capture workflow
agent-browser open https://example.com
agent-browser wait --load networkidle
agent-browser snapshot -i            # Get interactive elements with @refs
agent-browser get text @e5           # Extract content by ref
agent-browser screenshot /tmp/page.png
agent-browser close
```

```bash
# Fallback decision tree
# 1. Try WebFetch first (fast, no browser overhead)
# 2. If empty/partial -> use agent-browser
# 3. If SPA -> wait --load networkidle
# 4. If login required -> authentication flow + state save
# 5. If dynamic -> wait @element or wait --text
```

```bash
# Authentication with state persistence
agent-browser open https://app.example.com/login
agent-browser snapshot -i
agent-browser fill @e1 "$EMAIL"
agent-browser fill @e2 "$PASSWORD"
agent-browser click @e3
agent-browser wait --url "**/dashboard"
agent-browser state save /tmp/auth-state.json
```

```bash
# SPA extraction (React/Vue/Angular)
agent-browser open https://react-app.example.com
agent-browser wait --load networkidle
agent-browser eval "document.querySelector('article').innerText"
```

## Playwright Setup

Browser automation setup using agent-browser CLI (93% less context than full Playwright MCP) or Playwright directly.

| Rule | Description |
|------|-------------|
| `playwright-setup.md` | Installation, configuration, environment variables, cloud providers |

**Key Decisions:** agent-browser CLI preferred | `snapshot -i` for element discovery | `--session` for parallel isolation

## Page Interaction

Interact with page elements using snapshot refs for clicking, filling, and navigating.

| Rule | Description |
|------|-------------|
| `page-interaction.md` | Click, fill, navigate, wait patterns using snapshot refs |

**Key Decisions:** Always re-snapshot after navigation | Use refs (@e1) not CSS selectors | Wait for networkidle after navigation

## Content Extraction

Extract text, HTML, and structured data from web pages.

| Rule | Description |
|------|-------------|
| `content-extraction.md` | Text extraction, HTML capture, JavaScript eval for custom extraction |

**Key Decisions:** Use targeted refs over full-body extraction | Remove noise elements before extraction | Cache extracted content

## SPA Extraction

Extract content from JavaScript-rendered Single Page Applications.

| Rule | Description |
|------|-------------|
| `spa-extraction.md` | React, Vue, Angular, Next.js, Nuxt, Docusaurus extraction patterns |

**Key Decisions:** Wait for hydration, not just DOM ready | Use framework-specific detection | Handle infinite scroll and lazy loading

## Scraping Strategies

Multi-page crawling, pagination handling, and recursive site extraction.

| Rule | Description |
|------|-------------|
| `scraping-strategies.md` | Multi-page crawl, pagination, recursive depth-limited crawling, parallel sessions |

**Key Decisions:** Extract links first, then visit | Depth-limit recursive crawls | Use parallel sessions for throughput

## Anti-Bot Handling

Rate limiting, CAPTCHA handling, session management, and respectful scraping.

| Rule | Description |
|------|-------------|
| `anti-bot-handling.md` | Rate limiting, robots.txt, CAPTCHA, error handling, resume capability |

**Key Decisions:** Always check robots.txt | Add delays between requests | Use headed mode for CAPTCHA | Implement resume capability

## Authentication Flows

Login forms, OAuth/SSO flows, session persistence, and multi-step authentication.

| Rule | Description |
|------|-------------|
| `auth-flows.md` | Form login, OAuth popup, SSO redirect, state save/restore, session management |

**Key Decisions:** Save state after login | Use headed mode for OAuth/SSO | Never hardcode credentials | Clean up state files

## Structured Output

Convert scraped content to clean, structured formats for downstream processing.

| Rule | Description |
|------|-------------|
| `structured-output.md` | Markdown conversion, JSON extraction, metadata preservation, content cleaning |

**Key Decisions:** Remove noise elements before extraction | Preserve metadata (title, URL, timestamp) | Validate extracted data structure

## Anti-Patterns (FORBIDDEN)

```bash
# Automation
agent-browser fill @e2 "hardcoded-password"    # Never hardcode credentials
agent-browser open "$UNVALIDATED_URL"          # Always validate URLs

# Scraping
# Crawling without checking robots.txt
# No delay between requests (hammering servers)
# Ignoring rate limit responses (429)

# Content capture
agent-browser get text body                    # Prefer targeted ref extraction
# Trusting page content without validation
# Not waiting for SPA hydration before extraction

# Session management
# Storing auth state in code repositories
# Not cleaning up state files after use
```

## Agent-Browser Key Commands

| Command | Purpose |
|---------|---------|
| `open &lt;url&gt;` | Navigate to URL |
| `snapshot -i` | Interactive elements with refs |
| `click @e1` | Click element |
| `fill @e2 "text"` | Clear + type into input |
| `get text @e1` | Extract element text |
| `get html @e1` | Get element HTML |
| `eval "&lt;js&gt;"` | Run custom JavaScript |
| `wait --load networkidle` | Wait for SPA render |
| `wait --text "Expected"` | Wait for specific text |
| `wait @e1` | Wait for element |
| `screenshot &lt;path&gt;` | Save screenshot |
| `state save &lt;file&gt;` | Persist cookies/storage |
| `state load &lt;file&gt;` | Restore session |
| `--session &lt;name&gt;` | Isolate parallel sessions |
| `--headed` | Show browser window |
| `console` | Read JS console |
| `network requests` | Monitor XHR/fetch |
| `record start &lt;path&gt;` | Start video recording |
| `record stop` | Stop recording |
| `close` | Close browser |

Run `agent-browser --help` for the full 60+ command reference.

## Detailed Documentation

| Resource | Description |
|----------|-------------|
| [references/playwright-setup.md](references/playwright-setup.md) | Installation, configuration, environment variables |
| [references/page-interaction.md](references/page-interaction.md) | Click, fill, navigate, wait patterns |
| [references/content-extraction.md](references/content-extraction.md) | Text, HTML, and JS-based content extraction |
| [references/spa-extraction.md](references/spa-extraction.md) | React, Vue, Angular, Next.js extraction |
| [references/scraping-strategies.md](references/scraping-strategies.md) | Multi-page crawl, pagination, parallel sessions |
| [references/anti-bot-handling.md](references/anti-bot-handling.md) | Rate limiting, robots.txt, CAPTCHA, resume |
| [references/auth-flows.md](references/auth-flows.md) | Login, OAuth, SSO, session persistence |
| [references/structured-output.md](references/structured-output.md) | Markdown/JSON conversion, metadata, validation |

## Related Skills

- `testing-patterns` - Comprehensive testing patterns including E2E and webapp testing
- `api-design` - API design patterns for endpoints discovered during scraping
- `data-visualization` - Visualizing extracted data

## Capability Details

### browser-automation
**Keywords:** browser, automation, headless, agent-browser, playwright, puppeteer, CLI
**Solves:**
- Automate browser tasks with agent-browser CLI
- Set up headless browser environments
- Run parallel browser sessions
- Cloud browser provider configuration

### content-capture
**Keywords:** capture, extract, scrape, content, text, HTML, screenshot, web page
**Solves:**
- Extract content from JavaScript-rendered pages
- Capture screenshots and visual verification
- Handle dynamic content loading
- WebFetch returns empty or partial content

### spa-extraction
**Keywords:** react, vue, angular, spa, javascript, client-side, hydration, ssr, next.js, nuxt
**Solves:**
- React/Vue/Angular app content extraction
- Wait for SPA hydration before extraction
- Handle infinite scroll and lazy loading
- Framework detection and specific wait strategies

### authentication
**Keywords:** login, authentication, session, cookie, protected, private, gated, OAuth, SSO
**Solves:**
- Content behind login wall
- Multi-step authentication flows
- OAuth and SSO with headed mode
- Session persistence across captures

### multi-page-crawl
**Keywords:** crawl, sitemap, navigation, multiple pages, documentation, pagination, recursive
**Solves:**
- Capture entire documentation sites
- Handle click-based and URL-based pagination
- Recursive depth-limited crawling
- Parallel crawling with sessions

### anti-bot
**Keywords:** rate limit, robots.txt, CAPTCHA, bot detection, delay, throttle
**Solves:**
- Respectful scraping with rate limits
- Handling CAPTCHA and bot detection
- Resume capability for interrupted crawls
- Error handling for failed pages


---

## References (8)

<Accordions type="multiple">

<Accordion title="Anti Bot Handling">

# Anti-Bot Handling

Patterns for respectful scraping, rate limiting, CAPTCHA handling, and resilient crawling.

## Respectful Scraping Principles

1. **Check robots.txt** before crawling any site
2. **Add delays** between page navigations
3. **Don't crawl faster** than a human would browse
4. **Honor rate limits** (HTTP 429 responses)
5. **Identify yourself** when possible

---

## Check robots.txt

```bash
# Check robots.txt before crawling
ROBOTS=$(curl -s "https://docs.example.com/robots.txt")

if echo "$ROBOTS" | grep -q "Disallow: /docs"; then
    echo "Crawling /docs is disallowed by robots.txt"
    exit 1
fi

# Parse specific rules
CRAWL_DELAY=$(echo "$ROBOTS" | grep -i "Crawl-delay" | head -1 | awk '{print $2}')
DELAY=${CRAWL_DELAY:-1}  # Default to 1 second if not specified
```

---

## Rate Limiting

### Between Requests

```bash
# Add delay between requests
for url in "${URLS[@]}"; do
    agent-browser open "$url"
    agent-browser wait --load networkidle
    agent-browser get text body > "/tmp/$(basename "$url").txt"
    sleep 1  # Respect server resources
done
```

### Adaptive Rate Limiting

```bash
#!/bin/bash
# Back off when seeing rate limit responses

DELAY=1

for url in "${URLS[@]}"; do
    agent-browser open "$url"

    # Check for rate limiting indicators
    STATUS=$(agent-browser eval "
        // Check for common rate limit responses
        const h1 = document.querySelector('h1');
        if (h1 && (h1.innerText.includes('429') || h1.innerText.includes('Too Many'))) {
            'rate-limited';
        } else if (document.title.includes('Access Denied')) {
            'blocked';
        } else {
            'ok';
        }
    ")

    case "$STATUS" in
        "rate-limited")
            echo "Rate limited, backing off..."
            DELAY=$((DELAY * 2))
            sleep $DELAY
            continue
            ;;
        "blocked")
            echo "Access denied for: $url"
            continue
            ;;
        *)
            agent-browser get text body > "/tmp/$(basename "$url").txt"
            DELAY=1  # Reset delay on success
            ;;
    esac

    sleep $DELAY
done
```

---

## CAPTCHA Handling

CAPTCHAs require manual intervention. Use headed mode:

```bash
# Switch to headed mode for CAPTCHA sites
AGENT_BROWSER_HEADED=1 agent-browser open https://captcha-site.com

echo "Please solve the CAPTCHA in the browser window..."

# Wait for user to complete CAPTCHA
agent-browser wait --url "**/content" --timeout 120000

# Save state to avoid future CAPTCHAs
agent-browser state save /tmp/captcha-solved.json

# Continue automated extraction
agent-browser get text body
```

---

## Error Handling

### Handle Failed Pages

```bash
# Graceful error handling per page
for url in "${URLS[@]}"; do
    if ! agent-browser open "$url" 2>/dev/null; then
        echo "Failed to load: $url" >> /tmp/failed-urls.txt
        continue
    fi

    agent-browser wait --load networkidle

    # Verify content loaded
    HAS_CONTENT=$(agent-browser eval "document.body.innerText.trim().length > 100")
    if [[ "$HAS_CONTENT" != "true" ]]; then
        echo "Empty content: $url" >> /tmp/failed-urls.txt
        continue
    fi

    agent-browser get text body > "/tmp/$(basename "$url").txt"
done
```

### Retry Logic

```bash
#!/bin/bash
# Retry failed pages with exponential backoff

fetch_with_retry() {
    local url="$1"
    local output="$2"
    local max_retries=3
    local retry=0
    local delay=1

    while [[ $retry -lt $max_retries ]]; do
        if agent-browser open "$url" 2>/dev/null; then
            agent-browser wait --load networkidle
            agent-browser get text body > "$output"
            if [[ -s "$output" ]]; then
                return 0
            fi
        fi

        ((retry++))
        echo "Retry $retry/$max_retries for: $url (waiting ${delay}s)"
        sleep $delay
        delay=$((delay * 2))
    done

    echo "Failed after $max_retries retries: $url" >> /tmp/failed-urls.txt
    return 1
}
```

---

## Resume Capability

### Skip Already Crawled Pages

```bash
# Resume interrupted crawls
CRAWLED_DIR="/tmp/crawled"
mkdir -p "$CRAWLED_DIR"

for url in "${URLS[@]}"; do
    HASH=$(echo "$url" | md5sum | cut -d' ' -f1)
    OUTPUT="$CRAWLED_DIR/$HASH.txt"

    if [[ -f "$OUTPUT" ]]; then
        echo "Skipping (already crawled): $url"
        continue
    fi

    agent-browser open "$url"
    agent-browser wait --load networkidle
    agent-browser get text body > "$OUTPUT"
done
```

### Progress Tracking

```bash
#!/bin/bash
# Track crawl progress

PROGRESS_FILE="/tmp/crawl-progress.json"
TOTAL=${#URLS[@]}
COMPLETED=0

for url in "${URLS[@]}"; do
    ((COMPLETED++))
    echo "[$COMPLETED/$TOTAL] Crawling: $url"

    agent-browser open "$url"
    agent-browser wait --load networkidle
    agent-browser get text body > "/tmp/page-$COMPLETED.txt"

    # Save progress
    echo "{\"completed\": $COMPLETED, \"total\": $TOTAL, \"last_url\": \"$url\"}" > "$PROGRESS_FILE"
done
```

---

## Iframe and Shadow DOM

### Content in Iframes

```bash
# Switch to iframe
agent-browser frame @e3
agent-browser get text body
# Return to main frame
agent-browser frame main
```

---

## Troubleshooting

| Issue | Solution |
|-------|----------|
| Empty content | Add `wait --load networkidle` after navigation |
| Partial render | Use `wait --text "Expected content"` |
| Login required | Use authentication flow with `state save/load` |
| CAPTCHA blocking | Use headed mode for manual intervention |
| Content in iframe | Use `frame @e#` then extract |
| Rate limited (429) | Increase delay, implement exponential backoff |
| Access denied | Check robots.txt, try headed mode |


</Accordion>

<Accordion title="Auth Flows">

# Authentication Flows

Patterns for accessing login-protected content using agent-browser.

## Authentication Methods

### Method Comparison

| Method | Use Case | Complexity | User Involvement |
|--------|----------|------------|------------------|
| Form login | Username/password sites | Low | Credentials needed |
| OAuth popup | Google/GitHub login | Medium | User must complete |
| SSO redirect | Enterprise sites | High | User must complete |
| State restore | Reuse existing session | Low | Pre-export state |

### Decision Tree

```
Protected content needed
         |
         v
    Have saved state?
         |
    +- Yes --> Load state: agent-browser state load auth.json
    |
    +- No --> Check login type
                   |
         +- Simple form --> Fill form with refs
         +- OAuth popup --> Pause for user (--headed)
         +- SSO --> Pause for user (--headed)
```

---

## Form-Based Login

### Basic Login Flow

```bash
# 1. Navigate to login page
agent-browser open https://app.example.com/login

# 2. Wait for form to load
agent-browser wait --load networkidle

# 3. Get form structure
agent-browser snapshot -i
# Output shows: @e1 [input] "Email", @e2 [input] "Password", @e3 [button] "Sign In"

# 4. Fill credentials (from environment variables)
agent-browser fill @e1 "$EMAIL"
agent-browser fill @e2 "$PASSWORD"

# 5. Submit form
agent-browser click @e3

# 6. Wait for redirect to dashboard
agent-browser wait --url "**/dashboard"

# 7. Save state for reuse
agent-browser state save /tmp/auth-state.json

# 8. Now navigate to protected content
agent-browser open https://app.example.com/private-docs
```

### Multi-Step Login (Email then Password)

```bash
agent-browser open https://app.example.com/login
agent-browser snapshot -i

# Step 1: Email
agent-browser fill @e1 "$EMAIL"
agent-browser click @e2  # Next button

# Step 2: Password
agent-browser wait --fn "document.querySelector('[type=password]') !== null"
agent-browser snapshot -i
agent-browser fill @e1 "$PASSWORD"
agent-browser click @e2  # Sign in button

agent-browser wait --url "**/dashboard"
```

### Handle Login Errors

```bash
# Check for error messages after login attempt
ERROR=$(agent-browser eval "
const err = document.querySelector('.error-message, .alert-error, [role=\"alert\"]');
err ? err.innerText : '';
")

if [[ -n "$ERROR" ]]; then
    echo "Login failed: $ERROR"
fi
```

---

## OAuth/SSO Flows

For OAuth (Google, GitHub) and SSO, use headed mode for user interaction:

```bash
# 1. Start in headed mode
AGENT_BROWSER_HEADED=1 agent-browser open https://app.example.com/login

# 2. Click OAuth button
agent-browser snapshot -i
agent-browser click @e4  # "Sign in with Google"

# 3. PAUSE - User must complete OAuth flow
echo "Please complete sign-in in the browser window..."

# 4. Wait for redirect back to app
agent-browser wait --url "**/dashboard" --timeout 120000

# 5. Save state for future sessions
agent-browser state save /tmp/oauth-state.json
```

---

## Session Management

### Save and Restore State

```bash
# SAVE: After successful login
agent-browser state save /tmp/auth-state.json

# RESTORE: In new session
agent-browser state load /tmp/auth-state.json
agent-browser open https://app.example.com/dashboard
```

### Check Login State

```bash
# Verify if already logged in
IS_LOGGED_IN=$(agent-browser eval "
const hasLogout = document.querySelector('[href*=\"logout\"], .logout-button');
const hasProfile = document.querySelector('.user-avatar, .profile-menu');
!!(hasLogout || hasProfile);
")

if [[ "$IS_LOGGED_IN" == "true" ]]; then
    echo "Already logged in"
else
    echo "Need to authenticate"
fi
```

### Handle Session Expiry

```bash
# Check if redirected to login
CURRENT_URL=$(agent-browser get url)

if [[ "$CURRENT_URL" == *"/login"* ]]; then
    echo "Session expired, re-authenticating..."
    rm -f /tmp/auth-state.json
    # Trigger login flow again
fi
```

---

## Persist Across Captures

When doing multiple captures, maintain login state:

```bash
# Login once
agent-browser open https://app.example.com/login
# ... fill credentials ...
agent-browser state save /tmp/auth.json

# Capture multiple pages (session persists)
PAGES=(
    "https://app.example.com/docs/intro"
    "https://app.example.com/docs/guide"
    "https://app.example.com/docs/api"
)

for page_url in "${PAGES[@]}"; do
    agent-browser open "$page_url"
    agent-browser wait --load networkidle
    agent-browser get text body > "/tmp/$(basename $page_url).txt"
done
```

---

## Security Considerations

### Never Store Credentials in Code

```bash
# BAD - Don't do this
PASSWORD="hardcoded-password"

# GOOD - Use environment variables
agent-browser fill @e2 "$APP_PASSWORD"
```

### Secure State Files

```bash
# Set restrictive permissions
chmod 600 /tmp/auth-state.json

# Store in secure location
STATE_FILE="$HOME/.config/agent-browser/auth-state.json"
mkdir -p "$(dirname "$STATE_FILE")"

# Clean up after use
trap 'rm -f "$STATE_FILE"' EXIT
```

### Handle Sensitive Sites with Headed Mode

For sites with 2FA/MFA, CAPTCHA challenges, or device verification, use headed mode for manual completion:

```bash
AGENT_BROWSER_HEADED=1 agent-browser open https://secure-site.com/login
echo "Please complete authentication manually..."
agent-browser wait --url "**/authenticated"
agent-browser state save /tmp/secure-auth.json
```

---

## Common Sites

### GitHub Private Repos

```bash
agent-browser state load /tmp/github-auth.json
agent-browser open https://github.com/org/private-repo
agent-browser wait --load networkidle
```

### Confluence/Jira (SSO)

```bash
AGENT_BROWSER_HEADED=1 agent-browser open https://company.atlassian.net
echo "Complete SSO authentication..."
agent-browser wait --url "**/wiki" --timeout 120000
agent-browser state save /tmp/atlassian-auth.json
```

### Notion

```bash
AGENT_BROWSER_HEADED=1 agent-browser open https://notion.so
echo "Complete Notion login..."
agent-browser wait --url "**/workspace"
agent-browser state save /tmp/notion-auth.json
```


</Accordion>

<Accordion title="Content Extraction">

# Content Extraction

Patterns for extracting text, HTML, and structured data from web pages using agent-browser.

## Extraction Methods

| Method | Use Case | Context Cost |
|--------|----------|-------------|
| `get text @e#` | Extract specific element text | Low |
| `get html @e#` | Get element HTML structure | Medium |
| `eval "&lt;js&gt;"` | Custom JavaScript extraction | Variable |
| `get text body` | Full page text (last resort) | High |

---

## Targeted Extraction with Refs

### Identify Content Area

```bash
# Navigate and snapshot
agent-browser open https://docs.example.com/article
agent-browser wait --load networkidle
agent-browser snapshot -i

# Identify main content ref from snapshot output
# Example output: @e5 [article] "Main Content Area"
agent-browser get text @e5
```

### Extract Multiple Elements

```bash
# Get specific sections
agent-browser get text @e3   # Title
agent-browser get text @e5   # Main content
agent-browser get text @e8   # Sidebar

# Get HTML for structured content
agent-browser get html @e5   # Preserves formatting
```

---

## JavaScript-Based Extraction

### Custom Selectors

```bash
# Extract by CSS selector
agent-browser eval "document.querySelector('article').innerText"

# Extract by class
agent-browser eval "document.querySelector('.main-content').innerText"

# Extract by ID
agent-browser eval "document.getElementById('content').innerText"
```

### Extract Links

```bash
# All links on page
agent-browser eval "JSON.stringify(Array.from(document.querySelectorAll('a')).map(a => ({text: a.innerText, href: a.href})))"

# Navigation links only
agent-browser eval "JSON.stringify(Array.from(document.querySelectorAll('nav a')).map(a => ({text: a.innerText.trim(), href: a.href})))"
```

### Extract Tables

```bash
# Convert table to JSON
agent-browser eval "
const rows = document.querySelectorAll('table tr');
const headers = Array.from(rows[0].querySelectorAll('th')).map(th => th.innerText.trim());
const data = Array.from(rows).slice(1).map(row =>
    Object.fromEntries(
        Array.from(row.querySelectorAll('td')).map((td, i) => [headers[i], td.innerText.trim()])
    )
);
JSON.stringify(data, null, 2);
"
```

### Extract Metadata

```bash
# Page title and description
agent-browser eval "JSON.stringify({
    title: document.title,
    description: document.querySelector('meta[name=\"description\"]')?.content || '',
    canonical: document.querySelector('link[rel=\"canonical\"]')?.href || '',
    ogImage: document.querySelector('meta[property=\"og:image\"]')?.content || ''
})"
```

---

## Clean Content Extraction

### Remove Noise Before Extraction

```bash
# Remove non-content elements
agent-browser eval "
['nav', 'header', 'footer', '.sidebar', '.ads', '.cookie-banner', '.popup']
    .forEach(sel => document.querySelectorAll(sel).forEach(el => el.remove()));
const main = document.querySelector('main, article, .content, #content');
main ? main.innerText : document.body.innerText;
"
```

### Extract with Formatting

```bash
# Get clean text with line breaks preserved
agent-browser eval "
const content = document.querySelector('article');
// Replace block elements with newlines
const clone = content.cloneNode(true);
clone.querySelectorAll('br').forEach(br => br.replaceWith('\\n'));
clone.querySelectorAll('p, div, h1, h2, h3, h4, h5, h6, li').forEach(el => {
    el.prepend(document.createTextNode('\\n'));
});
clone.innerText.replace(/\\n{3,}/g, '\\n\\n').trim();
"
```

---

## Network-Level Extraction

### Monitor API Calls

```bash
# See XHR/fetch requests the page makes
agent-browser network requests

# Find API endpoints serving data
agent-browser eval "
performance.getEntriesByType('resource')
    .filter(r => r.initiatorType === 'xmlhttprequest' || r.initiatorType === 'fetch')
    .map(r => r.name)
"
```

### Intercept JSON Responses

When the page loads data via API, it may be easier to call the API directly:

```bash
# Find the API endpoint from network tab
agent-browser network requests
# Output shows: GET https://api.example.com/v1/articles?page=1

# Call API directly (faster than browser extraction)
curl -s https://api.example.com/v1/articles?page=1 | jq .
```

---

## Iframe Content

```bash
# Switch to iframe context
agent-browser frame @e3

# Extract from iframe
agent-browser get text body

# Return to main frame
agent-browser frame main
```

---

## Console Output

```bash
# Read JavaScript console messages
agent-browser console

# Useful for debugging extraction issues
# Shows errors, warnings, and log output
```

---

## Best Practices

1. **Use targeted extraction** -- `get text @e#` over `get text body` to minimize noise
2. **Remove noise elements first** -- Delete nav, header, footer, ads before extraction
3. **Cache extracted content** -- Save to file to avoid re-scraping
4. **Try WebFetch first** -- 10x faster for static content, no browser overhead
5. **Check network tab** -- API endpoints may provide cleaner data than DOM extraction
6. **Handle empty content** -- Always check if extraction returned meaningful content


</Accordion>

<Accordion title="Page Interaction">

# Page Interaction

Patterns for interacting with web page elements using agent-browser snapshot + refs workflow.

## Core Concept: Snapshot + Refs

Run `agent-browser snapshot -i` to get interactive elements tagged `@e1`, `@e2`, etc. Use these refs for all subsequent interactions. Re-snapshot after navigation or significant DOM changes. This yields **93% less context** than full-DOM approaches.

```bash
# 1. Take snapshot to discover elements
agent-browser snapshot -i
# Output: @e1 [input] "Search", @e2 [button] "Submit", @e3 [a] "About"

# 2. Interact using refs
agent-browser fill @e1 "search query"
agent-browser click @e2

# 3. Re-snapshot after page change
agent-browser snapshot -i
```

---

## Navigation

### Basic Navigation

```bash
# Navigate to URL
agent-browser open https://example.com

# Wait for page load
agent-browser wait --load networkidle

# Wait for specific URL pattern
agent-browser wait --url "**/dashboard"
```

### Client-Side Routing

For SPAs with client-side routing, content changes without full page reload:

```bash
# Click navigation link
agent-browser click @e5
# Wait for new content (not page load)
agent-browser wait --text "New Page Title"
# Re-snapshot for new elements
agent-browser snapshot -i
```

---

## Form Filling

### Basic Form

```bash
agent-browser open https://app.example.com/form
agent-browser snapshot -i

# Fill fields by ref
agent-browser fill @e1 "John Doe"           # Name
agent-browser fill @e2 "john@example.com"    # Email
agent-browser fill @e3 "Hello, world!"       # Message

# Submit
agent-browser click @e4                      # Submit button
agent-browser wait --load networkidle
```

### Multi-Step Forms

```bash
# Step 1: Personal info
agent-browser snapshot -i
agent-browser fill @e1 "John Doe"
agent-browser fill @e2 "john@example.com"
agent-browser click @e3  # Next

# Step 2: Address (new form, re-snapshot)
agent-browser wait --load networkidle
agent-browser snapshot -i
agent-browser fill @e1 "123 Main St"
agent-browser fill @e2 "New York"
agent-browser click @e3  # Next

# Step 3: Review and submit
agent-browser wait --load networkidle
agent-browser snapshot -i
agent-browser click @e1  # Confirm
```

### Dropdowns and Selects

```bash
# Click dropdown to open
agent-browser click @e5
agent-browser wait 500
agent-browser snapshot -i
# Click option
agent-browser click @e8

# Or use eval for native select
agent-browser eval "document.querySelector('select#country').value = 'US'"
```

---

## Clicking

### Standard Clicks

```bash
agent-browser click @e1           # Left click
```

### Wait After Click

```bash
# Click and wait for result
agent-browser click @e3
agent-browser wait --load networkidle    # Wait for navigation
agent-browser wait --text "Success"      # Wait for confirmation
agent-browser wait @e5                   # Wait for element to appear
```

---

## Waiting Strategies

| Strategy | When to Use |
|----------|-------------|
| `wait --load networkidle` | After navigation, SPA rendering |
| `wait --text "Expected"` | When specific content must appear |
| `wait @e#` | When specific element must appear |
| `wait --url "**/path"` | After redirects |
| `wait --fn "expression"` | Custom JavaScript condition |
| `wait 2000` | Fixed delay (last resort) |

```bash
# Wait for JavaScript condition
agent-browser wait --fn "document.querySelector('.loaded') !== null"

# Wait with timeout
agent-browser wait --text "Results" --timeout 30000
```

---

## Screenshots and Recording

```bash
# Full page screenshot
agent-browser screenshot /tmp/page.png

# Start/stop video recording
agent-browser record start /tmp/recording.webm
# ... perform actions ...
agent-browser record stop
```

---

## JavaScript Evaluation

```bash
# Run arbitrary JavaScript
agent-browser eval "document.title"
agent-browser eval "window.location.href"
agent-browser eval "document.querySelectorAll('a').length"

# Modify page state
agent-browser eval "document.querySelector('#modal').style.display = 'none'"
```

---

## Best Practices

1. **Always snapshot before interacting** -- Refs may change after navigation
2. **Re-snapshot after significant DOM changes** -- New elements get new refs
3. **Use waits, not fixed delays** -- `wait --load networkidle` over `wait 2000`
4. **Use refs, not CSS selectors** -- Refs are stable within a snapshot
5. **Check for errors after form submission** -- Snapshot and look for error elements


</Accordion>

<Accordion title="Playwright Setup">

# Playwright Setup & Configuration

Installation and configuration patterns for agent-browser CLI and Playwright-based browser automation.

## agent-browser CLI (Recommended)

Headless browser CLI by Vercel. 93% less context than full Playwright MCP thanks to snapshot + refs workflow.

### Installation

```bash
# Install globally
npm install -g agent-browser
agent-browser install                # Download Chromium

# With system dependencies (Linux CI/Docker)
agent-browser install --with-deps

# Optional: Install as Claude Code skill
npx skills add vercel-labs/agent-browser
```

### Verify Installation

```bash
# Check version
agent-browser --version

# Quick test
agent-browser open https://example.com
agent-browser snapshot -i
agent-browser close
```

---

## Environment Variables

```bash
# Session management
AGENT_BROWSER_SESSION="my-session"     # Default session name
AGENT_BROWSER_PROFILE="/path"          # Persistent browser profile

# Display
AGENT_BROWSER_HEADED=1                 # Run headed (show browser window)

# Cloud providers (for CI/remote execution)
AGENT_BROWSER_PROVIDER="browserbase"   # Options: browserbase | kernel | browseruse
```

---

## Cloud Provider Configuration

### BrowserBase

```bash
# Set provider
export AGENT_BROWSER_PROVIDER="browserbase"
export BROWSERBASE_API_KEY="your-api-key"
export BROWSERBASE_PROJECT_ID="your-project-id"

# Use normally - agent-browser handles cloud routing
agent-browser open https://example.com
```

### Docker/CI Setup

```dockerfile
FROM node:20-slim
RUN npm install -g agent-browser
RUN agent-browser install --with-deps
```

```yaml
# GitHub Actions
- name: Install agent-browser
  run: |
    npm install -g agent-browser
    agent-browser install --with-deps
```

---

## Playwright Direct Setup (Alternative)

When you need the full Playwright API instead of agent-browser CLI:

### Installation

```bash
npm init playwright@latest
# Installs: @playwright/test, browsers, config file

# Or add to existing project
npm install -D @playwright/test
npx playwright install
```

### Configuration

```typescript
// playwright.config.ts
import { defineConfig } from '@playwright/test';

export default defineConfig({
  use: {
    headless: true,
    viewport: { width: 1280, height: 720 },
    actionTimeout: 10000,
    navigationTimeout: 30000,
  },
  projects: [
    { name: 'chromium', use: { browserName: 'chromium' } },
  ],
});
```

---

## Session Isolation

Use named sessions for parallel browser instances:

```bash
# Isolated sessions for different tasks
agent-browser --session scrape1 open https://site1.com
agent-browser --session scrape2 open https://site2.com

# Each session has its own cookies, storage, and state
agent-browser --session scrape1 state save /tmp/session1.json
agent-browser --session scrape2 state save /tmp/session2.json
```

---

## OrchestKit Integration

**Safety hook** -- `agent-browser-safety.ts` blocks destructive patterns (credential exfil, recursive spawning) automatically via pretool hook.

**Upstream docs:** [github.com/vercel-labs/agent-browser](https://github.com/vercel-labs/agent-browser)

Run `agent-browser --help` for the full 60+ command reference.


</Accordion>

<Accordion title="Scraping Strategies">

# Scraping Strategies

Multi-page crawling, pagination handling, and parallel scraping patterns using agent-browser.

## Overview

Multi-page scraping is needed when:
- Documentation spans multiple pages
- Content is paginated (search results, listings)
- Need to follow navigation links
- Building a comprehensive content index

---

## Basic Crawl Pattern

### Extract Links, Then Visit

```bash
# 1. Navigate to starting page
agent-browser open https://docs.example.com

# 2. Wait for page to load
agent-browser wait --load networkidle

# 3. Extract all navigation links
LINKS=$(agent-browser eval "
JSON.stringify(
    Array.from(document.querySelectorAll('nav a, .sidebar a'))
        .map(a => a.href)
        .filter(href => href.startsWith('https://docs.example.com'))
)
")

# 4. Visit each link and extract
for link in $(echo "$LINKS" | jq -r '.[]'); do
    echo "Extracting: $link"
    agent-browser open "$link"
    agent-browser wait --load networkidle
    agent-browser get text body > "/tmp/$(basename "$link").txt"
done

# 5. Close browser
agent-browser close
```

---

## Structured Crawl with Metadata

```bash
#!/bin/bash
# Crawl with metadata extraction

OUTPUT_DIR="/tmp/docs-crawl"
mkdir -p "$OUTPUT_DIR"

agent-browser open https://docs.example.com
agent-browser wait --load networkidle

# Get links with titles
PAGES=$(agent-browser eval "
JSON.stringify(
    Array.from(document.querySelectorAll('nav a'))
        .map(a => ({
            url: a.href,
            title: a.innerText.trim()
        }))
        .filter(p => p.url.startsWith(window.location.origin))
)
")

# Process each page
echo "$PAGES" | jq -c '.[]' | while read -r page; do
    URL=$(echo "$page" | jq -r '.url')
    TITLE=$(echo "$page" | jq -r '.title')
    FILENAME=$(echo "$TITLE" | tr ' ' '-' | tr '[:upper:]' '[:lower:]')

    echo "Crawling: $TITLE"
    agent-browser open "$URL"
    agent-browser wait --load networkidle

    # Save content with metadata
    {
        echo "---"
        echo "title: $TITLE"
        echo "url: $URL"
        echo "crawled_at: $(date -Iseconds)"
        echo "---"
        echo ""
        agent-browser get text body
    } > "$OUTPUT_DIR/$FILENAME.md"
done

agent-browser close
echo "Crawl complete: $(ls "$OUTPUT_DIR" | wc -l) pages"
```

---

## Pagination Handling

### Click-Based Pagination

```bash
#!/bin/bash
# Handle "Next" button pagination

PAGE=1
while true; do
    echo "Extracting page $PAGE..."

    # Extract current page content
    agent-browser get text body > "/tmp/page-$PAGE.txt"

    # Check for next button
    agent-browser snapshot -i
    NEXT_BUTTON=$(agent-browser eval "
        const next = document.querySelector('.next, [rel=\"next\"], a:has-text(\"Next\")');
        next ? 'found' : 'none';
    ")

    if [[ "$NEXT_BUTTON" == "none" ]]; then
        echo "No more pages"
        break
    fi

    # Click next
    agent-browser click @e1  # Next button ref from snapshot
    agent-browser wait --load networkidle
    ((PAGE++))
done
```

### URL-Based Pagination

```bash
#!/bin/bash
# Handle URL parameter pagination

BASE_URL="https://api.example.com/docs"
PAGE=1

while true; do
    URL="${BASE_URL}?page=${PAGE}"
    echo "Fetching: $URL"

    agent-browser open "$URL"
    agent-browser wait --load networkidle

    # Check if page has content
    HAS_CONTENT=$(agent-browser eval "
        document.querySelector('.content').children.length > 0
    ")

    if [[ "$HAS_CONTENT" != "true" ]]; then
        echo "No more content at page $PAGE"
        break
    fi

    agent-browser get text body > "/tmp/page-$PAGE.txt"
    ((PAGE++))
done
```

---

## Recursive Crawl

### Follow All Links (Depth-Limited)

```bash
#!/bin/bash
# Recursive crawl with depth limit

MAX_DEPTH=3
VISITED_FILE="/tmp/visited-urls.txt"
touch "$VISITED_FILE"

crawl_page() {
    local url="$1"
    local depth="$2"

    # Skip if already visited
    grep -qF "$url" "$VISITED_FILE" && return

    # Skip if too deep
    [[ $depth -gt $MAX_DEPTH ]] && return

    echo "[$depth] Crawling: $url"
    echo "$url" >> "$VISITED_FILE"

    agent-browser open "$url"
    agent-browser wait --load networkidle

    # Save content
    local filename
    filename=$(echo "$url" | md5sum | cut -d' ' -f1)
    agent-browser get text body > "/tmp/crawl/$filename.txt"

    # Get child links
    local links
    links=$(agent-browser eval "
        JSON.stringify(
            Array.from(document.querySelectorAll('a'))
                .map(a => a.href)
                .filter(h => h.startsWith('$BASE_URL'))
        )
    ")

    # Recursively crawl children
    for link in $(echo "$links" | jq -r '.[]' | head -20); do
        crawl_page "$link" $((depth + 1))
    done
}

BASE_URL="https://docs.example.com"
mkdir -p /tmp/crawl
crawl_page "$BASE_URL" 0
agent-browser close
```

---

## Parallel Crawling with Sessions

```bash
#!/bin/bash
# Use multiple sessions for parallel crawling

URLS=(
    "https://docs.example.com/page1"
    "https://docs.example.com/page2"
    "https://docs.example.com/page3"
    "https://docs.example.com/page4"
)

# Start parallel sessions
for i in "${!URLS[@]}"; do
    SESSION="crawler-$i"
    URL="${URLS[$i]}"

    (
        agent-browser --session "$SESSION" open "$URL"
        agent-browser --session "$SESSION" wait --load networkidle
        agent-browser --session "$SESSION" get text body > "/tmp/page-$i.txt"
        agent-browser --session "$SESSION" close
    ) &
done

# Wait for all to complete
wait
echo "All pages crawled"
```

---

## Best Practices

1. **Extract links first, then visit** -- Avoid discovering links incrementally during crawl
2. **Depth-limit recursive crawls** -- Prevent unbounded crawling with MAX_DEPTH
3. **Track visited URLs** -- Prevent duplicate visits with a visited file/set
4. **Use parallel sessions** -- Speed up crawling with isolated browser instances
5. **Limit batch size** -- Process links in batches of 20-50 to manage memory


</Accordion>

<Accordion title="Spa Extraction">

# SPA Content Extraction

Patterns for extracting content from JavaScript-rendered Single Page Applications using agent-browser.

## Why SPAs Are Different

Traditional scrapers fail on SPAs because:

1. **Initial HTML is empty** -- Content loads via JavaScript
2. **Hydration timing** -- React/Vue must "hydrate" before content is interactive
3. **Client-side routing** -- URLs change without page reloads
4. **Lazy loading** -- Content loads as user scrolls
5. **API-driven** -- Data fetched from backend after page load

**Solution:** Use agent-browser to wait for JavaScript execution.

---

## Detection Patterns

### Identify SPA Framework

```bash
# Check for React
agent-browser eval "window.__REACT_DEVTOOLS_GLOBAL_HOOK__ !== undefined"

# Check for Vue
agent-browser eval "window.__VUE__ !== undefined"

# Check for Angular
agent-browser eval "window.ng !== undefined"

# Check for Next.js
agent-browser eval "document.querySelector('#__next') !== null"

# Check for Nuxt
agent-browser eval "document.querySelector('#__nuxt') !== null"
```

---

## React Extraction

### Wait for React Hydration

```bash
# Navigate
agent-browser open https://react-docs.example.com

# Wait for React to render content
agent-browser wait --load networkidle

# Or wait for specific hydration marker
agent-browser wait --fn "document.querySelector('[data-hydrated]') !== null"

# Get snapshot to find content
agent-browser snapshot -i

# Extract content
agent-browser get text @e5
```

### Next.js Specific

```bash
# Wait for Next.js
agent-browser open https://nextjs-site.com
agent-browser wait --fn "document.querySelector('#__next').children.length > 0"
agent-browser snapshot -i
agent-browser get text @e3
```

### Docusaurus Sites

```bash
agent-browser open https://docusaurus-docs.com
agent-browser wait --load networkidle
agent-browser eval "document.querySelector('.theme-doc-markdown').innerText"
```

---

## Vue Extraction

### Wait for Vue Mount

```bash
# Navigate
agent-browser open https://vue-app.example.com

# Wait for Vue to mount
agent-browser wait --fn "document.querySelector('#app').children.length > 0"

# Or wait for Vue data attributes
agent-browser wait --fn "document.querySelector('[data-v-]') !== null"

# Extract
agent-browser snapshot -i
agent-browser get text @e4
```

### Nuxt Specific

```bash
agent-browser open https://nuxt-site.com
agent-browser wait --fn "document.querySelector('#__nuxt').children.length > 0"
agent-browser snapshot -i
agent-browser get text @e2
```

### VitePress/VuePress

```bash
# VitePress
agent-browser open https://vitepress-docs.com
agent-browser wait --fn "document.querySelector('.vp-doc') !== null"
agent-browser eval "document.querySelector('.vp-doc').innerText"
```

---

## Angular Extraction

### Wait for Angular Bootstrap

```bash
# Navigate
agent-browser open https://angular-app.example.com

# Wait for Angular
agent-browser wait --fn "document.querySelector('app-root').children.length > 0"

# Or check ng-version attribute
agent-browser wait --fn "document.querySelector('[ng-version]') !== null"

# Extract
agent-browser snapshot -i
agent-browser get text @e3
```

---

## Generic SPA Patterns

### Wait for Content, Not Framework

When framework is unknown, wait for visible content:

```bash
# Wait for meaningful content (page has substantial text)
agent-browser wait --fn "document.body.innerText.trim().length > 500"

# Or wait for specific text
agent-browser wait --text "Welcome"
```

### Handle Infinite Scroll

```bash
# Scroll to load all content
agent-browser eval "
async function scrollToBottom() {
    let lastHeight = document.body.scrollHeight;
    while (true) {
        window.scrollTo(0, document.body.scrollHeight);
        await new Promise(r => setTimeout(r, 1000));
        if (document.body.scrollHeight === lastHeight) break;
        lastHeight = document.body.scrollHeight;
    }
    return document.body.innerText;
}
scrollToBottom();
"
```

### Handle Lazy Images

```bash
# Trigger lazy image loading
agent-browser eval "
document.querySelectorAll('img[data-src]').forEach(img => img.src = img.dataset.src);
document.querySelectorAll('img[loading=\"lazy\"]').forEach(img => img.loading = 'eager');
"
agent-browser wait 2000
```

### Extract Clean Content

```bash
# Remove noise elements before extraction
agent-browser eval "
['nav', 'header', 'footer', '.sidebar', '.ads', '.cookie-banner']
    .forEach(sel => document.querySelectorAll(sel).forEach(el => el.remove()));
const main = document.querySelector('main, article, .content, #content');
main ? main.innerText : document.body.innerText;
"
```

---

## Troubleshooting

| Issue | Cause | Solution |
|-------|-------|----------|
| Empty content | JS not executed | Add `wait --load networkidle` |
| Partial content | Hydration incomplete | Use `wait --fn` with specific check |
| Stale content | Client-side cache | Add cache-busting param to URL |
| Loading spinner | Slow API | Increase timeout, use `wait --text` |
| 404 after nav | Client routing issue | Use full page reload |


</Accordion>

<Accordion title="Structured Output">

# Structured Output

Patterns for converting scraped web content into clean, structured formats for downstream processing.

## Output Formats

| Format | Use Case | Tool |
|--------|----------|------|
| Markdown | Documentation, notes, LLM context | Text extraction + formatting |
| JSON | APIs, data pipelines, structured data | JavaScript eval + JSON.stringify |
| CSV | Tabular data, spreadsheets | Table extraction + conversion |
| Plain text | Search indexing, simple storage | get text @e# |

---

## Markdown Conversion

### Page to Clean Markdown

```bash
# Extract content as clean markdown
agent-browser eval "
function toMarkdown(el) {
    let md = '';
    el.childNodes.forEach(node => {
        if (node.nodeType === 3) {
            md += node.textContent;
        } else if (node.nodeType === 1) {
            const tag = node.tagName.toLowerCase();
            switch(tag) {
                case 'h1': md += '# ' + node.innerText + '\\n\\n'; break;
                case 'h2': md += '## ' + node.innerText + '\\n\\n'; break;
                case 'h3': md += '### ' + node.innerText + '\\n\\n'; break;
                case 'p': md += node.innerText + '\\n\\n'; break;
                case 'li': md += '- ' + node.innerText + '\\n'; break;
                case 'code': md += '\`' + node.innerText + '\`'; break;
                case 'pre': md += '\\n\`\`\`\\n' + node.innerText + '\\n\`\`\`\\n\\n'; break;
                case 'a': md += '[' + node.innerText + '](' + node.href + ')'; break;
                case 'strong': case 'b': md += '**' + node.innerText + '**'; break;
                case 'em': case 'i': md += '*' + node.innerText + '*'; break;
                default: md += toMarkdown(node);
            }
        }
    });
    return md;
}
const main = document.querySelector('article, main, .content');
toMarkdown(main || document.body);
"
```

### With Frontmatter Metadata

```bash
# Save page with YAML frontmatter
URL=$(agent-browser eval "window.location.href")
TITLE=$(agent-browser eval "document.title")
CONTENT=$(agent-browser eval "document.querySelector('article')?.innerText || document.body.innerText")

{
    echo "---"
    echo "title: $TITLE"
    echo "url: $URL"
    echo "crawled_at: $(date -Iseconds)"
    echo "---"
    echo ""
    echo "$CONTENT"
} > "/tmp/page-output.md"
```

---

## JSON Extraction

### Structured Data from Page

```bash
# Extract structured data
agent-browser eval "
const article = document.querySelector('article');
JSON.stringify({
    title: document.title,
    url: window.location.href,
    author: document.querySelector('[rel=\"author\"], .author')?.innerText || '',
    publishDate: document.querySelector('time')?.getAttribute('datetime') || '',
    content: article?.innerText || '',
    wordCount: (article?.innerText || '').split(/\s+/).length,
    links: Array.from(article?.querySelectorAll('a') || []).map(a => ({
        text: a.innerText.trim(),
        href: a.href
    })),
    images: Array.from(article?.querySelectorAll('img') || []).map(img => ({
        alt: img.alt,
        src: img.src
    }))
}, null, 2);
"
```

### JSON-LD / Schema.org Data

```bash
# Extract structured data from JSON-LD
agent-browser eval "
const scripts = document.querySelectorAll('script[type=\"application/ld+json\"]');
const data = Array.from(scripts).map(s => JSON.parse(s.textContent));
JSON.stringify(data, null, 2);
"
```

### Product Data Extraction

```bash
# Extract product information
agent-browser eval "
const products = Array.from(document.querySelectorAll('.product-card, [data-product]'));
JSON.stringify(products.map(p => ({
    name: p.querySelector('.product-name, h3, h2')?.innerText.trim(),
    price: p.querySelector('.price, [data-price]')?.innerText.trim(),
    rating: p.querySelector('.rating, [data-rating]')?.innerText.trim(),
    image: p.querySelector('img')?.src,
    url: p.querySelector('a')?.href
})), null, 2);
"
```

---

## Table Extraction

### HTML Table to JSON

```bash
# Convert table to structured JSON
agent-browser eval "
const rows = document.querySelectorAll('table tr');
if (rows.length === 0) { '[]'; }
else {
    const headers = Array.from(rows[0].querySelectorAll('th, td')).map(th => th.innerText.trim());
    const data = Array.from(rows).slice(1).map(row =>
        Object.fromEntries(
            Array.from(row.querySelectorAll('td')).map((td, i) => [headers[i] || 'col_' + i, td.innerText.trim()])
        )
    );
    JSON.stringify(data, null, 2);
}
"
```

### Table to CSV

```bash
# Convert table to CSV
agent-browser eval "
const rows = document.querySelectorAll('table tr');
Array.from(rows).map(row =>
    Array.from(row.querySelectorAll('th, td'))
        .map(cell => '\"' + cell.innerText.trim().replace(/\"/g, '\"\"') + '\"')
        .join(',')
).join('\\n');
" > /tmp/table-output.csv
```

---

## Content Cleaning

### Remove Noise Elements

```bash
# Clean page before extraction
agent-browser eval "
// Remove non-content elements
const selectors = [
    'nav', 'header', 'footer',
    '.sidebar', '.toc', '.breadcrumbs',
    '.ads', '.advertisement', '.sponsored',
    '.cookie-banner', '.popup', '.modal',
    '.comments', '.related-posts',
    'script', 'style', 'noscript'
];
selectors.forEach(sel =>
    document.querySelectorAll(sel).forEach(el => el.remove())
);

// Return cleaned content
const main = document.querySelector('main, article, .content, #content, .post-body');
main ? main.innerText : document.body.innerText;
"
```

### Normalize Whitespace

```bash
# Clean up extracted text
agent-browser eval "
const content = document.querySelector('article')?.innerText || '';
content
    .replace(/\\t/g, ' ')           // Replace tabs with spaces
    .replace(/ {2,}/g, ' ')         // Collapse multiple spaces
    .replace(/\\n{3,}/g, '\\n\\n')  // Max two newlines
    .trim();
"
```

---

## Batch Output

### Multi-Page to Single File

```bash
#!/bin/bash
# Combine crawled pages into single document

OUTPUT="/tmp/combined-docs.md"
echo "# Combined Documentation" > "$OUTPUT"
echo "" >> "$OUTPUT"
echo "Crawled: $(date -Iseconds)" >> "$OUTPUT"
echo "" >> "$OUTPUT"

for file in /tmp/docs-crawl/*.md; do
    echo "---" >> "$OUTPUT"
    echo "" >> "$OUTPUT"
    cat "$file" >> "$OUTPUT"
    echo "" >> "$OUTPUT"
done

echo "Combined output: $(wc -l < "$OUTPUT") lines"
```

### Index File Generation

```bash
#!/bin/bash
# Generate index of crawled pages

OUTPUT="/tmp/crawl-index.json"

agent-browser eval "
JSON.stringify(
    Array.from(document.querySelectorAll('nav a')).map(a => ({
        title: a.innerText.trim(),
        url: a.href,
        section: a.closest('section')?.querySelector('h2')?.innerText || 'uncategorized'
    })),
    null, 2
);
" > "$OUTPUT"
```

---

## Validation

### Verify Extraction Quality

```bash
# Check that extraction returned meaningful content
CONTENT=$(agent-browser get text @e5)
WORD_COUNT=$(echo "$CONTENT" | wc -w)

if [[ $WORD_COUNT -lt 50 ]]; then
    echo "Warning: Extraction returned only $WORD_COUNT words"
    echo "Consider: wait --load networkidle, or different selector"
fi
```

### Validate JSON Output

```bash
# Ensure valid JSON
OUTPUT=$(agent-browser eval "JSON.stringify({data: 'test'})")

if echo "$OUTPUT" | jq . > /dev/null 2>&1; then
    echo "Valid JSON"
    echo "$OUTPUT" | jq . > /tmp/validated-output.json
else
    echo "Invalid JSON output, raw: $OUTPUT"
fi
```

---

## Best Practices

1. **Preserve metadata** -- Always include title, URL, timestamp in output
2. **Clean before extracting** -- Remove noise elements for better signal-to-noise
3. **Validate output** -- Check word count, JSON validity before saving
4. **Use targeted selectors** -- Extract specific content areas, not full page
5. **Normalize whitespace** -- Clean up tabs, multiple spaces, excessive newlines
6. **Check for JSON-LD** -- Structured data may already exist on the page


</Accordion>

</Accordions>