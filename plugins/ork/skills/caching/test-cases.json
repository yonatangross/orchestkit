{
  "skill": "caching",
  "version": "2.0.0",
  "testCases": [
    {
      "id": "backend-cache-aside",
      "rule": "backend-cache-aside",
      "query": "Implement a lazy loading cache with Redis",
      "expectedBehavior": [
        "Uses cache-aside pattern with get_or_set",
        "Checks cache first, fetches on miss",
        "Stores result in cache with TTL",
        "Supports custom serialization/deserialization"
      ]
    },
    {
      "id": "backend-write-through",
      "rule": "backend-write-through",
      "query": "Keep cache consistent with database on writes",
      "expectedBehavior": [
        "Writes to database first for consistency",
        "Updates cache synchronously after DB write",
        "Shows write-behind alternative for high throughput",
        "Implements background flush with batch processing"
      ]
    },
    {
      "id": "backend-invalidation",
      "rule": "backend-invalidation",
      "query": "How do I invalidate cache and prevent stampede?",
      "expectedBehavior": [
        "Shows TTL-based invalidation with jitter",
        "Implements event-based and version-based invalidation",
        "Uses distributed lock for stampede prevention",
        "Double-checks cache after acquiring lock"
      ]
    },
    {
      "id": "prompt-claude",
      "rule": "prompt-claude",
      "query": "Use Claude prompt caching to reduce costs",
      "expectedBehavior": [
        "Uses cache_control with ephemeral type",
        "Supports 5m default and 1h extended TTL",
        "Places stable content before dynamic content",
        "Shows 90% cost reduction on cache reads"
      ]
    },
    {
      "id": "prompt-openai",
      "rule": "prompt-openai",
      "query": "How does OpenAI prompt caching work?",
      "expectedBehavior": [
        "Explains automatic prefix caching",
        "No cache_control markers needed",
        "Shows how to check cache usage in response",
        "Lists supported models (gpt-5.2, o3)"
      ]
    },
    {
      "id": "prompt-breakpoints",
      "rule": "prompt-breakpoints",
      "query": "Structure prompts for optimal cache hits",
      "expectedBehavior": [
        "Explains processing order: tools, system, messages",
        "Recommends stable prefix first, variable content last",
        "Shows cache effectiveness monitoring code",
        "Lists common mistakes with prompt ordering"
      ]
    },
    {
      "id": "semantic-vector",
      "rule": "semantic-vector",
      "query": "Cache LLM responses using vector similarity",
      "expectedBehavior": [
        "Uses Redis vector search with redisvl",
        "Configures similarity threshold (default 0.92)",
        "Filters by agent_type metadata",
        "Shows threshold tuning recommendations"
      ]
    },
    {
      "id": "semantic-multi-level",
      "rule": "semantic-multi-level",
      "query": "Build a multi-level cache for LLM responses",
      "expectedBehavior": [
        "Implements L1 in-memory, L2 semantic, L3 prompt, L4 LLM",
        "Shows latency and cost savings per level",
        "Promotes lower-level hits to L1",
        "Provides complete lookup implementation"
      ]
    },
    {
      "id": "semantic-redis",
      "rule": "semantic-redis",
      "query": "Use Redis 8.4 hybrid search for caching",
      "expectedBehavior": [
        "Uses FT.HYBRID command for combined search",
        "Combines keyword filtering with vector similarity",
        "Uses RRF for score combination",
        "Explains when to use hybrid vs sequential filtering"
      ]
    },
    {
      "id": "cost-langfuse",
      "rule": "cost-langfuse",
      "query": "Track LLM costs with Langfuse",
      "expectedBehavior": [
        "Uses @observe decorator for automatic tracking",
        "Records cache layer and hit status in metadata",
        "Links to parent trace with session_id",
        "Tracks prompt cache savings separately"
      ]
    },
    {
      "id": "cost-attribution",
      "rule": "cost-attribution",
      "query": "Attribute costs to individual agents in a multi-agent system",
      "expectedBehavior": [
        "Uses hierarchical trace structure",
        "Child generation costs roll up to parent",
        "Queries costs by agent_type metadata",
        "Shows per-agent cost breakdown code"
      ]
    },
    {
      "id": "cost-effectiveness",
      "rule": "cost-effectiveness",
      "query": "Calculate cache hit rate and cost savings",
      "expectedBehavior": [
        "Calculates hit rate from hits and misses",
        "Estimates cost saved per cache hit",
        "Tracks metrics over configurable time windows",
        "Shows ROI calculation for caching investment"
      ]
    }
  ]
}
