{
  "skill": "async-jobs",
  "version": "2.0.0",
  "testCases": [
    {
      "id": "celery-config",
      "rule": "jobs-task-queue",
      "query": "Set up a production Celery app with Redis broker",
      "expectedBehavior": [
        "Uses JSON serializer (never pickle)",
        "Sets task_acks_late=True to prevent task loss",
        "Configures both task_time_limit and task_soft_time_limit",
        "Sets worker_prefetch_multiplier=1 for fair distribution"
      ]
    },
    {
      "id": "task-routing",
      "rule": null,
      "query": "Route tasks to priority queues based on user tier",
      "expectedBehavior": [
        "Defines critical/high/default/low queues with kombu",
        "Configures broker_transport_options with priority_steps",
        "Implements TaskRouter class for dynamic routing",
        "Assigns dedicated workers per queue with tuned concurrency"
      ]
    },
    {
      "id": "canvas-workflows",
      "rule": "celery-canvas",
      "query": "Build an ETL pipeline with parallel processing and aggregation",
      "expectedBehavior": [
        "Uses chain for sequential extract-transform-load steps",
        "Uses chord for parallel chunk processing with callback",
        "Implements link_error for workflow-level error handling",
        "Uses si() for immutable signatures in appropriate steps"
      ]
    },
    {
      "id": "retry-strategies",
      "rule": "jobs-task-queue",
      "query": "Implement a task with retry and idempotency protection",
      "expectedBehavior": [
        "Uses autoretry_for with retry_backoff=True and retry_jitter=True",
        "Implements idempotency key check in Redis before processing",
        "Sets max_retries with retry_backoff_max ceiling",
        "Moves permanently failed tasks to dead letter queue"
      ]
    },
    {
      "id": "scheduled-tasks",
      "rule": "jobs-scheduling",
      "query": "Configure periodic tasks with overlap prevention",
      "expectedBehavior": [
        "Uses celery.schedules.crontab for time-based scheduling",
        "Implements Redis-based schedule lock to prevent overlap",
        "Configures beat_schedule with UTC timezone",
        "Separates beat process from workers"
      ]
    },
    {
      "id": "monitoring-health",
      "rule": null,
      "query": "Add monitoring and health checks to Celery workers",
      "expectedBehavior": [
        "Connects to celery signals for task_prerun and task_failure",
        "Implements health check verifying broker and active workers",
        "Monitors queue depths for autoscaling decisions",
        "Uses Flower with persistent storage for dashboard"
      ]
    },
    {
      "id": "result-backends",
      "rule": "jobs-monitoring",
      "query": "Track task progress with custom states",
      "expectedBehavior": [
        "Defines custom states (VALIDATING, PROCESSING, UPLOADING)",
        "Uses update_state() for progress reporting",
        "Queries progress via AsyncResult with state checks",
        "Stores large results in S3/DB, not Redis"
      ]
    },
    {
      "id": "arq-patterns",
      "rule": "jobs-task-queue",
      "query": "Set up async background tasks with ARQ and FastAPI",
      "expectedBehavior": [
        "Creates ARQ worker with startup/shutdown lifecycle hooks",
        "Enqueues jobs from FastAPI routes with enqueue_job()",
        "Implements job status endpoint with Job.status()",
        "Configures max_jobs and job_timeout in WorkerSettings"
      ]
    }
  ]
}
